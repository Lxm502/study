{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f981a54-c60e-4e7c-8f92-6ee9c9560236",
   "metadata": {},
   "source": [
    "#  `tf.GradientTape` 自动求导的核心工具的基本用法\n",
    "## 核心概念：tf.GradientTape 是什么？\n",
    "你可以把 `tf.GradientTape` 想象成一个“磁带录音机”。\n",
    "\n",
    "录制 (Recording)：当你开启一个 `GradientTape` 的上下文环境时，它会像录音机一样，自动“录制”所有在 TensorFlow 中执行的、涉及可训练变量 (`tf.Variable`) 的计算步骤。\n",
    "\n",
    " - 回放 (Rewinding)：计算完成后，你可以调用 `.gradient()` 方法，`GradientTape` 会“回放”这个磁带（在数学上是利用反向传播和链式法则），计算出某个结果（通常是损失函数）相对于一个或多个变量的梯度。\n",
    "\n",
    " - 销毁 (Erasing)：默认情况下，一旦 `.gradient()` 方法被调用，这个“磁带”上的内容就会被立即释放，以节省内存。\n",
    "\n",
    "这是实现自动微分（Automatic Differentiation）的关键，也是训练神经网络的基础。\n",
    "\n",
    "---\n",
    "## 基本用法三步曲\n",
    "使用 `tf.GradientTape` 通常遵循以下三个步骤：\n",
    "\n",
    "### 1. 创建 `GradientTape` 上下文\n",
    "使用 Python 的 `with` 语句来创建一个录制环境。所有在这个 `with` 代码块中进行的计算都会被追踪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182df2e-b3e0-4f64-a374-a562e40e87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 准备一些变量\n",
    "# tf.Variable 是可训练的，会被 GradientTape 自动追踪\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # 在 with 代码块内定义需要求导的计算\n",
    "  y = x * x # 等价于 x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107cf04-cdd1-41ed-96b5-caa3c81fdaa6",
   "metadata": {},
   "source": [
    "### 2. 定义计算过程\n",
    "在 `with` 代码块内部，定义你的`前向传播计算`。这可以是一个简单的数学公式，也可以是一个复杂的神经网络模型。`GradientTape` 会自动监控所有 `tf.Variable` 并记录相关的操作。\n",
    "\n",
    "在上面的例子中，y = x * x 就是我们关心的计算过程。\n",
    "\n",
    "---\n",
    "### 3. 计算梯度\n",
    "使用 `tape.gradient(target, sources)` 方法来计算梯度。\n",
    "\n",
    " - `target`: 你想要对其求导的目标，通常是模型的损失函数（`loss`），必须是一个标量（`scalar`）。\n",
    "\n",
    " - `sources`: 你想要计算梯度的来源，通常是模型的权重（`tf.Variable`），可以是一个变量，也可以是一个变量列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa29be6-b698-410e-8103-2d09453ae0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 y 相对于 x 的梯度\n",
    "# target 是 y, source 是 x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(dy_dx)\n",
    "\n",
    "输出结果：\n",
    "tf.Tensor(6.0, shape=(), dtype=float32)\n",
    "\n",
    "这个结果是完全正确的，因为函数 y=x*x 的导数是 2x。当 x=3.0 时，导数就是 $2 * 3.0 = 6.0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8166a78-d305-4622-914a-8845e8af14b0",
   "metadata": {},
   "source": [
    "---\n",
    "## 关键参数和进阶用法\n",
    "### 1. 追踪 `tf.Tensor`：tape.watch()\n",
    "默认情况下，`GradientTape` 只会自动追踪 `tf.Variable`。如果你想对一个普通的 `tf.Tensor` 求梯度，你需要手动告诉 \"录音机\" 开始监视这个张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7a6ad-8171-4763-ab57-6f27b6e0798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0) # 这是一个常量 Tensor，不是 Variable\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # 必须手动 watch 一个非 Variable 的 Tensor\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx) # tf.Tensor(6.0, shape=(), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5df33-c878-494d-a052-1c9b41e1bde6",
   "metadata": {},
   "source": [
    "注意：在构建模型时，模型的`权重`通常都定义为 `tf.Variable`，所以你很少需要手动 `watch`。\n",
    "\n",
    "---\n",
    "### 2. 计算多个梯度：persistent=True\n",
    "前面提到，磁带在调用 `.gradient()` 后会立即被擦除。如果你需要对同一个计算过程调用多次` .gradient()`（例如，计算一个复杂的损失函数对不同层权重的梯度），你需要让磁带“持久化”。使用完持久化的 `tape` 后，手动删除它以释放资源 `del tape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c48f6-918f-4910-9ed1-329b889afd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(4.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  z1 = x * x\n",
    "  z2 = y * y\n",
    "  z = z1 + z2\n",
    "\n",
    "# 计算 z 相对于 x 的梯度 (∂z/∂x = 2x = 8.0)\n",
    "dz_dx = tape.gradient(z, x)\n",
    "print(dz_dx)\n",
    "\n",
    "# 计算 z 相对于 y 的梯度 (∂z/∂y = 2y = 6.0)\n",
    "# 如果没有 persistent=True, 这一步会报错！\n",
    "dz_dy = tape.gradient(z, y)\n",
    "print(dz_dy)\n",
    "\n",
    "# 使用完持久化的 tape 后，手动删除它以释放资源\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1e7f1-d8ee-4ecb-8767-bb5084210eca",
   "metadata": {},
   "source": [
    "### 3. 梯度为 None 的常见情况\n",
    "有时 `tape.gradient()` 会返回 `None`，这通常意味着：\n",
    "\n",
    " - 目标和来源之间没有计算路径：你求导的变量没有参与目标的计算。\n",
    "\n",
    " - 变量是整数类型：梯度只能对浮点数类型（如 `float32`, `float64`）的变量计算。\n",
    "\n",
    " - 计算在 `TensorFlow` 之外进行：例如，你使用了 NumPy 的操作，`GradientTape` 无法追踪到它。\n",
    "\n",
    " - 变量被放置在` with tape `代码块之外创建或计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582386f-0302-4ba8-bf2e-d8b9289fe93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y1 = x * x # y1 的计算在 tape 外部，未被录制\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y2 = x + 1 # y2 的计算在 tape 内部，被录制\n",
    "\n",
    "print(tape.gradient(y1, x)) # 输出 None，因为 y1 的计算未被追踪\n",
    "print(tape.gradient(y2, x)) # 输出 tf.Tensor(1.0, ...)，因为 y2 的计算被追踪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510005d5-64a0-4a08-afdc-7291709bdd7f",
   "metadata": {},
   "source": [
    "## 综合实例：一个简单的线性回归模型\n",
    "下面是一个完整的例子，展示了如何使用 `tf.GradientTape` 来训练一个简单的模型 y=Wx+b。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94563d31-a78c-496f-a112-64d91dc94f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 准备数据\n",
    "X_raw = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n",
    "              7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1], dtype=np.float32)\n",
    "y_raw = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221,\n",
    "              2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3], dtype=np.float32)\n",
    "\n",
    "X = tf.constant(X_raw)\n",
    "y_true = tf.constant(y_raw)\n",
    "\n",
    "# 2. 初始化模型参数 (必须是 tf.Variable)\n",
    "W = tf.Variable(tf.random.normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.zeros([1]), name=\"bias\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# 3. 训练循环\n",
    "for epoch in range(epochs):\n",
    "  # 开启 GradientTape\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 前向传播：计算预测值\n",
    "    y_pred = W * X + b\n",
    "    # 计算损失 (均方误差)\n",
    "    loss = tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "  # 计算损失函数对 W 和 b 的梯度\n",
    "  # tape.gradient(目标, [来源列表])\n",
    "  gradients = tape.gradient(loss, [W, b])\n",
    "  dW, db = gradients[0], gradients[1]\n",
    "\n",
    "  # 反向传播：更新权重和偏置\n",
    "  # W = W - learning_rate * dW\n",
    "  W.assign_sub(learning_rate * dW)\n",
    "  b.assign_sub(learning_rate * db)\n",
    "\n",
    "  if (epoch + 1) % 10 == 0:\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.numpy():.4f}, W: {W.numpy()[0]:.4f}, b: {b.numpy()[0]:.4f}')\n",
    "\n",
    "print(\"\\n训练完成!\")\n",
    "print(f\"最终模型: y = {W.numpy()[0]:.4f}x + {b.numpy()[0]:.4f}\")\n",
    "\n",
    "# 可视化结果\n",
    "plt.scatter(X, y_true, label='Original data')\n",
    "plt.plot(X, W * X + b, 'r-', label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4384d94-91e2-4c9e-971f-c4145f5e3edc",
   "metadata": {},
   "source": [
    "在这个例子中，`tf.GradientTape` 录制了从 `W `和` b `到 `loss` 的整个计算图，然后我们用 `tape.gradient()` 得到了损失对参数的梯度，并用这些梯度来更新参数，从而使模型的预测越来越准。这就是机器学习训练的核心循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859809bb-7249-4709-aeec-b03d017688f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
