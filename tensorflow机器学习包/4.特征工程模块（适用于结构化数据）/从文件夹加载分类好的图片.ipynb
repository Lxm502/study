{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9acc2460-fc8d-4fd5-a91f-9ec29ce2af61",
   "metadata": {},
   "source": [
    "# tf.keras.preprocessing.image_dataset_from_directory\n",
    "## 1. 基本功能\n",
    "\n",
    "它会从 目录结构 中自动创建 `tf.data.Dataset`，同时会根据 `子文件夹的名字` 自动生成类别标签。\n",
    "\n",
    "📂 目录结构示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073b243-3057-492e-9bf1-5558f2319a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset/\n",
    "├── train/\n",
    "│   ├── cats/\n",
    "│   │   ├── cat1.jpg\n",
    "│   │   ├── cat2.jpg\n",
    "│   └── dogs/\n",
    "│       ├── dog1.jpg\n",
    "│       ├── dog2.jpg\n",
    "└── val/\n",
    "    ├── cats/\n",
    "    └── dogs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f5b9a-0409-4c7f-941f-68b5511f1efd",
   "metadata": {},
   "source": [
    "## 2. 为什么需要使用它？\n",
    "- 极简代码：用一行代码替代了繁琐的图像读取、解码、调整大小、标注和批处理流程。\n",
    "\n",
    "- 高性能：生成的是 `tf.data.Dataset` 对象，支持 `TensorFlow` 的所有高效数据管道功能（预取、缓存、并行化等）。\n",
    "\n",
    "- 内存高效：不会一次性将所有图像加载到内存中，而是动态地从磁盘读取，适合处理大型数据集。\n",
    "\n",
    "- 自动标签：根据目录结构自动推断标签，无需手动创建标签文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50882e-8f92-4a62-bd25-5456b622b7a3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 核心参数详解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc58032-7eea-46b5-ac7a-8680586972eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory,                   # 必需：数据所在目录的路径\n",
    "    labels='inferred',           # 标签来源：'inferred'（从目录结构），或 None（无标签）\n",
    "    label_mode='int',            # 标签格式：'int'（整数标签），'categorical'（one-hot），'binary'（二分类）\n",
    "    class_names=None,            # 显式指定类别名称列表，用于控制顺序\n",
    "    color_mode='rgb',            # 图像通道：'rgb'（3通道），'grayscale'（1通道），'rgba'（4通道）\n",
    "    batch_size=32,               # 批次大小\n",
    "    image_size=(256, 256),       # 将所有图像调整为此尺寸（高度，宽度）\n",
    "    shuffle=True,                # 是否打乱数据\n",
    "    seed=None,                   # 随机种子，用于可重现的 shuffling\n",
    "    validation_split=None,       # 分割部分数据作为验证集（0-1之间的浮点数）\n",
    "    subset=None,                 # 如果使用 validation_split，指定是 'training' 还是 'validation'\n",
    "    interpolation='bilinear',    # 图像调整大小的插值方法：'bilinear', 'nearest', 等\n",
    "    follow_links=False,          # 是否跟随子目录中的符号链接\n",
    "    crop_to_aspect_ratio=False,  # 是否裁剪图像以保持原始宽高比\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849a3e7-a9a0-4779-b21d-48b48362e3d9",
   "metadata": {},
   "source": [
    "### 最重要的参数：\n",
    "- 1.`directory`：包含按类别分组的子目录的目录路径。\n",
    "\n",
    "- 2.`image_size`：必须指定。所有图像将被调整为此尺寸。例如 (224, 224)。\n",
    "\n",
    "- 3.`batch_size`：每个批次中包含的样本数。\n",
    "\n",
    "- 4.`label_mode`：\n",
    "\n",
    "    - `int`：标签为整数（如 0, 1, 2）。适用于 `sparse_categorical_crossentropy` 损失。\n",
    "    \n",
    "    - `categorical`：标签为 `one-hot` 编码向量（如 [1, 0, 0], [0, 1, 0]）。适用于 `categorical_crossentropy` 损失。\n",
    "    \n",
    "    - `binary`：标签为二分类的 0 或 1。适用于 `binary_crossentropy` 损失。\n",
    "\n",
    "- 5.`validation_split` 和 `subset`：用于从同一目录创建训练/验证集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262812d-b972-48a5-bea7-bed0c9edb26b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 基本使用示例\n",
    "示例 1：创建训练集和验证集（从不同目录）\\\n",
    "这是最推荐的方式，训练集和验证集分别放在不同的文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a75874-ee8a-47f2-bf5d-93c45d6c0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义图像大小和批次大小\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 从训练目录创建训练集\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='data/train',      # 训练集目录\n",
    "    labels='inferred',           # 从目录结构推断标签\n",
    "    label_mode='categorical',    # 使用 one-hot 编码标签\n",
    "    color_mode='rgb',            # 彩色图像\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,         # 调整图像大小\n",
    "    shuffle=True,                # 打乱数据\n",
    "    seed=123,                    # 随机种子\n",
    ")\n",
    "\n",
    "# 从验证目录创建验证集\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='data/validation', # 验证集目录\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=False,               # 验证集通常不需要打乱\n",
    ")\n",
    "\n",
    "# 查看数据集信息\n",
    "print(f\"训练集类别: {train_ds.class_names}\")\n",
    "print(f\"训练集批次形状: {next(iter(train_ds))[0].shape}\") # (32, 224, 224, 3)\n",
    "print(f\"标签形状: {next(iter(train_ds))[1].shape}\")       # (32, 2) - 假设有2个类别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b77ad-a981-4e4c-ae03-f0256c7ee0e0",
   "metadata": {},
   "source": [
    "示例 2：从同一目录分割训练/验证集 \\\n",
    "当你的所有数据都在一个目录中时，可以使用 `validation_split` 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51bff0-069f-4fd6-85dc-31aa6298cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练集（80%的数据）\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='data/all_images',\n",
    "    validation_split=0.2,        # 使用20%作为验证集\n",
    "    subset='training',           # 这是训练部分\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# 创建验证集（20%的数据）\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='data/all_images',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',         # 这是验证部分\n",
    "    seed=123,                    # 必须使用相同的seed！\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83255c6a-0df3-4278-8e94-c7053bfa5db7",
   "metadata": {},
   "source": [
    "## 5 遍历数据集\n",
    "使用 数据集 `.take(n)` 从数据集中取出前 `n` 个批次的数据（n是你传入的数字）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52673c51-0488-4966-a2be-ce217ff468b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)  # (32, 224, 224, 3)\n",
    "    print(labels.numpy())  # e.g. [0, 1, 0, ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d8740-a829-4d28-b72e-ec5e3575bbc1",
   "metadata": {},
   "source": [
    "## 6. 数据预处理和增强\n",
    "创建数据集后，你通常需要添加预处理和数据增强。\n",
    "\n",
    "标准预处理（归一化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e7d3f-80d2-47ce-9c75-5fe0af2961c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预处理函数：将像素值从 [0, 255] 归一化到 [0, 1] 或 [-1, 1]\n",
    "def preprocess(image, label):\n",
    "    # 方法1: 归一化到 [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    # 方法2: 归一化到 [-1,  ]（某些预训练模型需要）\n",
    "    # image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# 应用预处理\n",
    "train_ds = train_ds.map(preprocess)\n",
    "val_ds = val_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95b73613-3ae5-425c-ae45-5e27cebff714",
   "metadata": {},
   "source": [
    "数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35abcc-4bd0-4a61-b9b2-a881b1e4820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据增强层\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    # 会随机对输入图像进行水平翻转、垂直翻转，或者同时进行水平和垂直翻转（即对角线翻转）\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2), # 用于对输入图像进行随机旋转\n",
    "    tf.keras.layers.RandomZoom(0.1),     # 用于对输入图像进行随机缩放（放大或缩小）\n",
    "    tf.keras.layers.RandomContrast(0.1), # 用于对输入图像进行随机对比度调整\n",
    "])\n",
    "\n",
    "# 应用数据增强（只对训练集）\n",
    "def augment(image, label):\n",
    "    image = data_augmentation(image, training=True)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc92e07-27ba-400c-8627-e012224104eb",
   "metadata": {},
   "source": [
    "## 6. 性能优化\n",
    "使用 `tf.data` 的性能优化技巧来加速训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dafaa-db19-48d6-b435-aab8ccd5540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置数据集以获得最佳性能\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000) # 将数据缓存到内存中（如果数据集较小）.shuffle(1000) → 打乱\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)  # 在训练时预取后续批次\n",
    "\n",
    "val_ds = val_ds.cache()\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da74ac6-4348-4730-b1a3-8e163cb5d066",
   "metadata": {},
   "source": [
    "## 7. 完整的工作流程示例 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a6087-9bc8-44d0-b4b6-79befefd2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 创建数据集\n",
    "IMG_SIZE = (180, 180)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'data/train',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'data/train',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# 查看类别信息\n",
    "class_names = train_ds.class_names\n",
    "print(f\"类别: {class_names}\")\n",
    "print(f\"类别数量: {len(class_names)}\")\n",
    "\n",
    "# 2. 数据预处理和增强\n",
    "# 归一化\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "# 数据增强\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "])\n",
    "\n",
    "def augment(image, label):\n",
    "    image = data_augmentation(image, training=True)\n",
    "    return image, label\n",
    "\n",
    "# 应用预处理和增强\n",
    "train_ds = train_ds.map(augment).map(preprocess)\n",
    "val_ds = val_ds.map(preprocess)\n",
    "\n",
    "# 3. 性能优化\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 4. 创建和训练模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 5. 训练模型\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608e018-3015-433e-b60a-16c46c12ec29",
   "metadata": {},
   "source": [
    "## 8.与 ImageDataGenerator 的区别\n",
    "| `image_dataset_from_directory`  | `ImageDataGenerator` |\n",
    "| ------------------------------- | -------------------- |\n",
    "| 返回 `tf.data.Dataset`（适合 TF 2.x） | 返回 Python 生成器        |\n",
    "| 更快、更优化                          | 功能较旧                 |\n",
    "| 支持 GPU pipeline                 | 更多传统数据增强             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d0509-5114-4ca7-b61c-10a026f91219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
