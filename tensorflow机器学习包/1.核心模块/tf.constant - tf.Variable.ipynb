{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47cff0b-7aae-42af-81cf-bf169dc8c79d",
   "metadata": {},
   "source": [
    "# tf.constant 和 tf.Variable \n",
    "是 TensorFlow 中用于创建 张量（tensor）对象 的两种基础方式，它们都可以用来表示数据，但功能和用途有所不同。\n",
    "\n",
    "### 🧱 1. tf.constant\n",
    "✅ 定义：\\\n",
    "表示不可变（immutable）的张量，创建后值不能修改。\n",
    "\n",
    "📌 用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3f30ce-e1a6-4d8c-bdbc-2c231d4650d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([1.0, 2.0, 3.0])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8db90b-6a40-41bf-aa65-2708ac84741b",
   "metadata": {},
   "source": [
    "### 🧠 特点：\n",
    "| 特性              | 说明        |\n",
    "| --------------- | --------- |\n",
    "| 不可变             | 创建后数值无法更改 |\n",
    "| 用于模型参数中不需要更新的部分 | 如超参数、常量项  |\n",
    "| 可用于图构建或计算图输入    |           |\n",
    "\n",
    "💡 示例：\\\n",
    "c = tf.constant([[1, 2], [3, 4]]) \\\n",
    "c[0][0] = 100  # ❌ 错误：不能修改 constant 的值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219bed8-9699-4f4c-9d06-3c23296581f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13914c2-c77a-4ce4-ae7c-5a3113e526fb",
   "metadata": {},
   "source": [
    "### 🧱 2. tf.Variable\n",
    "✅ 定义：\\\n",
    "表示**可变（mutable）**的张量，通常用于表示模型的可训练参数，如神经网络的权重。\n",
    "\n",
    "📌 用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27aea97-a66b-421d-a77f-23a81ae76429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "v = tf.Variable([1.0, 2.0, 3.0])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f9277-acab-4863-ae0d-240f134e2bd3",
   "metadata": {},
   "source": [
    "### 🧠 特点：\n",
    "| 特性                                   | 说明                     |\n",
    "| ------------------------------------ | ---------------------- |\n",
    "| 可变                                   | 值可以被更新                 |\n",
    "| 常用于机器学习模型的权重                         | 用于 `GradientTape` 自动微分 |\n",
    "| 支持 `.assign()` 和 `.assign_add()` 等操作 |                        |\n",
    "\n",
    "💡 示例：\\\n",
    "v = tf.Variable([1, 2, 3]) \\\n",
    "v.assign([4, 5, 6])          # 替换值  \\\n",
    "v.assign_add([1, 1, 1])      # 加法更新  \n",
    "### ✅ 对比总结表：\n",
    "| 特征                     | `tf.constant` | `tf.Variable` |\n",
    "| ---------------------- | ------------- | ------------- |\n",
    "| 是否可变                   | ❌ 不可变         | ✅ 可变          |\n",
    "| 是否可训练                  | ❌ 否           | ✅ 是           |\n",
    "| 是否用于模型参数               | 🚫 一般不用       | ✅ 常用于权重、偏置    |\n",
    "| 是否支持 `.assign()` 等更新方法 | ❌ 否           | ✅ 是           |\n",
    "| 自动微分支持                 | ✅ 支持          | ✅ 支持          |\n",
    "\n",
    "### 📘 举个典型场景："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882c6e3-1cc3-4813-adb5-bea33c32191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常量：学习率或标签\n",
    "learning_rate = tf.constant(0.01)\n",
    "\n",
    "# 可训练变量：模型权重\n",
    "W = tf.Variable(tf.random.normal([3, 2]), name='weight')\n",
    "b = tf.Variable(tf.zeros([2]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14312faf-308a-4ad5-8d82-53eea8b7a255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aa6ce3a-ed2d-4470-bc5c-1f6e578f4d62",
   "metadata": {},
   "source": [
    "# .assign()、.assign_add() 和相关操作方法。\n",
    "## 🔧 一、.assign() —— 直接赋新值\n",
    "✅ 功能：将变量的值替换为一个新的值\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "v = tf.Variable([1.0, 2.0, 3.0])\\\n",
    "v.assign([4.0, 5.0, 6.0])\\\n",
    "print(v.numpy())  # 输出: [4. 5. 6.]   ---> 查看张量的值使用：tensor.numpy() \\\n",
    "📌 要求新值的形状和原始变量一致，否则报错。\n",
    "\n",
    "---\n",
    "## ➕ 二、.assign_add() —— 原值累加\n",
    "✅ 功能：将变量的值增加一个值（原地加法）\n",
    "\n",
    "v = tf.Variable([1.0, 2.0, 3.0])\\\n",
    "v.assign_add([10.0, 10.0, 10.0])\\\n",
    "print(v.numpy())  # 输出: [11. 12. 13.]\n",
    "\n",
    "---\n",
    "## ➖ 三、.assign_sub() —— 原值累减\n",
    "✅ 功能：将变量的值减去一个值（原地减法）\\\n",
    "\n",
    "v = tf.Variable([10.0, 10.0, 10.0])\\\n",
    "v.assign_sub([1.0, 2.0, 3.0])\\\n",
    "print(v.numpy())  # 输出: [9. 8. 7.]\n",
    "\n",
    "---\n",
    "## 🚨 注意事项\n",
    "| 操作名             | 含义    | 是否原地操作 | 支持梯度传播 |\n",
    "| --------------- | ----- | ------ | ------ |\n",
    "| `.assign()`     | 赋新值   | ✅ 是    | ✅ 支持   |\n",
    "| `.assign_add()` | 原值加新值 | ✅ 是    | ✅ 支持   |\n",
    "| `.assign_sub()` | 原值减新值 | ✅ 是    | ✅ 支持   |\n",
    "\n",
    "-  这些操作必须用于 tf.Variable 类型。\n",
    "-  如果你对一个 tf.constant 使用 .assign() 会报错。\n",
    "-  支持在 tf.GradientTape() 环境中使用，依然可正常反向传播。\n",
    "\n",
    "### 💡 在训练中常见的用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1323dbb-a366-46b3-ae50-c93eac584b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟梯度下降过程\n",
    "W = tf.Variable(5.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = (W - 3.0) ** 2  # 假设损失函数\n",
    "\n",
    "grad = tape.gradient(loss, W)\n",
    "W.assign_sub(0.1 * grad)  # 梯度下降一步\n",
    "\n",
    "print(W.numpy())  # 权重被更新\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35fed9-a9f1-44bb-87d7-3fac3ff01e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ebf1610-e72d-4687-8653-a04e304635e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5824df-d525-4614-859e-3197484e4394",
   "metadata": {},
   "source": [
    "# 📖 tf.GradientTape() 讲解\n",
    "## 1. 基本概念\n",
    "\n",
    "- 作用：记录张量的运算过程，从而能够自动计算梯度。\n",
    "\n",
    "- 用途：常用于神经网络训练，帮我们算出 损失函数对模型参数的梯度。\n",
    "\n",
    "- 换句话说：\\\n",
    "它相当于“录制机 🎥”，会把你在它上下文中执行的运算步骤都记下来，然后可以“反向播放”，帮你求梯度。\n",
    "\n",
    "### 👉什么时候使用？：\n",
    "需要`自定义训练循环`时\t使用 tf.GradientTape\n",
    "\n",
    "## 2. 基本用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a31b0-e1a5-4e0a-b761-f8527134888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义变量\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# 在 GradientTape 的上下文内进行要记录的操作\n",
    "with tf.GradientTape() as tape:\n",
    "    # tape.watch(x)  对于由 tf.constant 定义的变量，tape.watch() 可确保它被追踪\n",
    "    # tf.Variable 定义的变量 无需手动调用 tape.watch(variable)\n",
    "    y = x**2  # y = x^2   前向传播计算 y\n",
    "\n",
    "# 计算 y 关于 x 的梯度 .求 dy/dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())  # 输出： tf.Tensor(6.0, shape=(), dtype=float32)\n",
    "# 因为 dy/dx = 2x，当 x=3.0 时，结果为 6.0\n",
    "\n",
    "👉 这里的原理是：\n",
    "tape 记录了 y = x^2 的计算\n",
    "tape.gradient(y, x) 自动帮我们算了导数 dy/dx = 2x = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d48f7-c634-4685-ac16-75a2aacb8a19",
   "metadata": {},
   "source": [
    "## 3. 常见参数\n",
    "`persistent`=True\n",
    "\n",
    "- `默认`情况下，`GradientTape` 只能计算`一次`梯度，算完就释放资源。\n",
    "\n",
    "- 如果想多次计算，需要设置 `persistent` = `True`。切记用完后手动删除磁带：del tape -->  # 重要！手动释放资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cccd63-6618-4763-b33a-5140a81bc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x**2\n",
    "    z = y * 2\n",
    "\n",
    "dy_dx = tape.gradient(y, x)  # 6.0\n",
    "dz_dx = tape.gradient(z, x)  # 12.0\n",
    "del tape # 重要！手动释放资源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81f7f5-67e0-4b1e-ab80-f3ec0a659747",
   "metadata": {},
   "source": [
    "`watch()`\n",
    "\n",
    "- 变量（`tf.Variable`）会被自动监视（tape 自动追踪它的梯度）。\n",
    "\n",
    "- 普通张量（`tf.constant`）不会自动追踪，需要手动 tape.watch()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63551254-7a3d-48d1-b4c0-7a55e9ea4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)  # 手动监视\n",
    "    y = x**2\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())  # 6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1745fe-8a96-4b80-8128-cbf51c0dbff4",
   "metadata": {},
   "source": [
    "## 4. 在训练中的应用\n",
    "通常我们会在训练循环里用 `GradientTape` 来求梯度，再用优化器更新参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4e72c-69b3-4c73-ab3f-e76a93cc661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的线性回归：y = w*x + b\n",
    "w = tf.Variable(2.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "x = tf.constant([1.0, 2.0, 3.0])\n",
    "y_true = tf.constant([3.0, 5.0, 7.0])  # 真实值\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "for step in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = w * x + b\n",
    "        loss = tf.reduce_mean(tf.square(y_true - y_pred))  # MSE\n",
    "    \n",
    "    # 计算梯度\n",
    "    grads = tape.gradient(loss, [w, b])\n",
    "    # 应用梯度\n",
    "    optimizer.apply_gradients(zip(grads, [w, b]))\n",
    "\n",
    "print(w.numpy(), b.numpy())  # 接近真实值 w=2, b=1\n",
    "\n",
    "👉 训练流程：\n",
    "\n",
    "1.前向传播：计算预测值 & 损失\n",
    "\n",
    "2.反向传播：用 tape.gradient 求梯度\n",
    "\n",
    "3.参数更新：优化器根据梯度更新 w 和 b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa0765-0c9b-43f5-a0df-bb00189a46ed",
   "metadata": {},
   "source": [
    "## 5. 多阶导数（高阶导数）\n",
    "\n",
    "tf.GradientTape 还能嵌套使用，计算二阶导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f8270-e1fa-4f78-ba82-be84189c7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as t1:\n",
    "    with tf.GradientTape() as t0:\n",
    "        y = x**3  # y = x^3\n",
    "    dy_dx = t0.gradient(y, x)  # dy/dx = 3x^2 = 27\n",
    "\n",
    "d2y_dx2 = t1.gradient(dy_dx, x)  # d²y/dx² = 6x = 18\n",
    "print(dy_dx.numpy(), d2y_dx2.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1f383-3c9f-4dec-bd10-759cb74d13c4",
   "metadata": {},
   "source": [
    "## 6.现代训练循环的模板：\n",
    "在实际项目中，我们通常会使用 Keras Model 和 Optimizer，让代码更简洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8131f-0f16-444b-9ed1-633890114ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 model 是一个 tf.keras.Model, optimizer 是 tf.keras.optimizers\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_dataset: # 使用 tf.data.Dataset\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(batch_x, training=True) # 前向传播\n",
    "            loss = loss_function(batch_y, predictions)  # 计算损失\n",
    "\n",
    "        # 计算梯度\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # 使用优化器更新权重（代替手动 W.assign_sub）\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7bdabe-5c7e-4da9-b8c7-6d3cf52930a8",
   "metadata": {},
   "source": [
    "📊 总结速查表\n",
    "| 功能   | 方法                                 |\n",
    "| ---- | ---------------------------------- |\n",
    "| 开始录制 | `with tf.GradientTape() as tape:`  |\n",
    "| 计算梯度 | `tape.gradient(loss, var_list)`    |\n",
    "| 监视张量 | `tape.watch(tensor)`               |\n",
    "| 多次使用 | `tf.GradientTape(persistent=True)` |\n",
    "| 高阶导数 | 嵌套 `GradientTape`                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc6eec-88cd-42ba-863e-1c6e85450fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
