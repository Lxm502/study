{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d833576-38ef-4521-a17a-7b33681b62a5",
   "metadata": {},
   "source": [
    "# 📌 tf.keras.optimizers 优化器 速查表\n",
    "| 优化器                                                | 主要特点                               | 常用参数                                                                        | 适用场景               | 注意事项                        |\n",
    "| -------------------------------------------------- | ---------------------------------- | --------------------------------------------------------------------------- | ------------------ | --------------------------- |\n",
    "| **SGD** \n",
    "(`tf.keras.optimizers.SGD`)                | 最基本的随机梯度下降，可配合 Momentum / Nesterov | `learning_rate`, `momentum`, `nesterov`                                     | 适合简单任务或理论研究        | 对学习率敏感，收敛慢                  |\n",
    "| **Momentum**\n",
    "（SGD+Momentum）                         | 在 SGD 基础上增加动量，加快收敛                 | 同上                                                                          | 深度网络训练，加速梯度下降      | 需调 `momentum`（常用 0.9）       |\n",
    "| **Nesterov**                                       | Momentum 改进版本，提前看梯度方向              | 同上（`nesterov=True`）                                                         | 深度网络，避免震荡          | 通常收敛速度更稳定                   |\n",
    "| **Adagrad** \n",
    "(`tf.keras.optimizers.Adagrad`)        | 自适应学习率，稀疏特征表现好                     | `learning_rate`, `initial_accumulator_value`                                | NLP、稀疏特征训练         | 学习率会不断衰减，后期可能收敛过慢           |\n",
    "| **RMSprop** \n",
    "(`tf.keras.optimizers.RMSprop`)        | 平滑梯度，自适应学习率，常用于 RNN                | `learning_rate`, `rho`, `momentum`, `centered`                              | RNN、序列数据           | 比 Adam 更适合某些循环任务            |\n",
    "| **Adadelta** \n",
    "(`tf.keras.optimizers.Adadelta`)      | Adagrad 改进版，避免过快衰减                 | `learning_rate`, `rho`                                                      | 无需手动调大学习率          | 适合中等规模任务                    |\n",
    "| **Adam** \n",
    "(`tf.keras.optimizers.Adam`)              | 结合 Momentum + RMSprop，自适应学习率       | `learning_rate`, `beta_1`, `beta_2`, `epsilon`, `amsgrad`                   | 绝大多数深度学习任务         | 默认参数通常好用（0.001, 0.9, 0.999） |\n",
    "| **Adamax** \n",
    "(`tf.keras.optimizers.Adamax`)          | Adam 的无界版本，基于无穷范数                  | 同上（少 `amsgrad`）                                                             | 稳定性较好，适合稀疏梯度       | 收敛可能比 Adam 慢                |\n",
    "| **Nadam** \n",
    "(`tf.keras.optimizers.Nadam`)            | Adam + Nesterov 动量                 | 同 Adam + `schedule_decay`                                                   | 可能比 Adam 收敛快       | 对序列任务效果好                    |\n",
    "| **Ftrl** \n",
    "(`tf.keras.optimizers.Ftrl`)              | 适合大规模线性模型训练（广告 CTR 预测）             | `learning_rate`, `l1_regularization_strength`, `l2_regularization_strength` | 线上学习、稀疏特征          | 常用于推荐系统和广告业务                |\n",
    "| **Lion** \n",
    "(`tf.keras.optimizers.experimental.Lion`) | Google 2023 新优化器，低内存，高性能           | `learning_rate`, `beta_1`, `beta_2`                                         | 大模型（如 Transformer） | 需要实验性 API                   |\n",
    "\n",
    "# 📍 常用参数解释\n",
    "| 参数                 | 类型                               | 作用                      |\n",
    "| ------------------ | -------------------------------- | ----------------------- |\n",
    "| **learning\\_rate** | `float` / `LearningRateSchedule` | 学习率                     |\n",
    "| **momentum**       | `float`                          | 动量因子（SGD、RMSprop 等）     |\n",
    "| **nesterov**       | `bool`                           | 是否使用 Nesterov 动量        |\n",
    "| **beta\\_1**        | `float`                          | Adam/Nadam 一阶矩估计衰减率     |\n",
    "| **beta\\_2**        | `float`                          | Adam/Nadam 二阶矩估计衰减率     |\n",
    "| **epsilon**        | `float`                          | 防止除零的小数值                |\n",
    "| **rho**            | `float`                          | 平滑系数（RMSprop、Adadelta）  |\n",
    "| **amsgrad**        | `bool`                           | 是否启用 AMSGrad 稳定版本（Adam） |\n",
    "# 📌 使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fb312-aff7-49a1-9ae7-72075cb7fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 使用 Adam\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# 2. 使用带动量的 SGD\n",
    "optimizer2 = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "# 3. 在 compile 中使用\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b62bc2-f1d8-470a-bc87-9b002165d510",
   "metadata": {},
   "source": [
    "# 📚 选择建议\n",
    "入门 / 通用 → Adam（大多数任务）\n",
    "\n",
    "简单任务 / 理论分析 → SGD（可加 Momentum）\n",
    "\n",
    "稀疏特征 → Adagrad / Ftrl\n",
    "\n",
    "循环神经网络（RNN） → RMSprop / Nadam\n",
    "\n",
    "大规模数据 → Adamax / Lion / Ftrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea84587-5f1c-43e5-bb84-13ce95016604",
   "metadata": {},
   "source": [
    "# 1️⃣ 什么是优化器（Optimizer）？\n",
    "在 TensorFlow 中，优化器（tf.keras.optimizers）是用来根据损失函数的梯度更新模型参数的算法。\n",
    "简单来说：\\\n",
    "1、损失函数告诉我们模型预测的好坏\\\n",
    "2、梯度告诉我们如何调整参数才能变好\\\n",
    "3、优化器负责根据梯度来调整权重（权重更新规则）\n",
    "\n",
    "# 2️⃣ 优化器模块结构\n",
    "tf.keras.optimizers 提供的主要优化器分类：\n",
    "| 分类           | 优化器                | 核心思想                              | 代表                  |\n",
    "| ------------ | ------------------ | --------------------------------- | ------------------- |\n",
    "| **基础型**      | SGD                | 固定学习率梯度下降                         | `SGD`               |\n",
    "| **动量型**      | Momentum, Nesterov | 使用动量加速收敛                          | `SGD(momentum=0.9)` |\n",
    "| **自适应型**     | 学习率随梯度变化           | `Adagrad`, `Adadelta`, `RMSprop`  |                     |\n",
    "| **自适应动量型**   | 结合动量 + 自适应学习率      | `Adam`, `Adamax`, `Nadam`, `Lion` |                     |\n",
    "| **大规模稀疏数据型** | 稀疏更新 + 正则化         | `Ftrl`                            |                     |\n",
    "# 3️⃣ 常用优化器详细说明\n",
    "| 优化器          | 原理简述               | 常用参数                                                      | 适用场景             | 注意事项                  |\n",
    "| ------------ | ------------------ | --------------------------------------------------------- | ---------------- | --------------------- |\n",
    "| **SGD**      | 固定学习率更新权重          | `learning_rate`, `momentum`, `nesterov`                   | 理论研究、简单模型        | 对学习率敏感                |\n",
    "| **Momentum** | SGD + 动量，加速下降      | 同上                                                        | 深度网络，减少震荡        | 需调 `momentum`（0.9 常用） |\n",
    "| **Nesterov** | 先看一步再更新            | 同上（`nesterov=True`）                                       | 比 Momentum 更稳定   |                       |\n",
    "| **Adagrad**  | 学习率随梯度累积自动缩小       | `learning_rate`                                           | NLP 稀疏特征         | 学习率会衰减到极小             |\n",
    "| **Adadelta** | Adagrad 改进版，避免衰减过快 | `learning_rate`, `rho`                                    | 无需手动大幅调学习率       |                       |\n",
    "| **RMSprop**  | 平滑梯度，适合 RNN        | `learning_rate`, `rho`, `momentum`                        | 序列数据训练           |                       |\n",
    "| **Adam**     | Momentum + RMSprop | `learning_rate`, `beta_1`, `beta_2`, `epsilon`, `amsgrad` | 绝大多数任务           | 默认参数通常可直接用            |\n",
    "| **Adamax**   | Adam 改进版，无穷范数稳定    | 同 Adam                                                    | 稀疏梯度             |                       |\n",
    "| **Nadam**    | Adam + Nesterov    | 同 Adam + `schedule_decay`                                 | RNN, LSTM        |                       |\n",
    "| **Ftrl**     | 稀疏特征优化 + 正则        | `learning_rate`, `l1_regularization_strength`             | CTR 预测、推荐系统      |                       |\n",
    "| **Lion**     | 2023 Google 新优化器   | `learning_rate`, `beta_1`, `beta_2`                       | 大模型（Transformer） | 实验性 API               |\n",
    "## 4️⃣使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500a330-3ce2-41c9-866e-99f037fd75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 创建优化器\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,  # 学习率\n",
    "    beta_1=0.9,           # 一阶动量系数\n",
    "    beta_2=0.999,         # 二阶动量系数\n",
    "    epsilon=1e-07         # 防止除零\n",
    ")\n",
    "\n",
    "# 2. 用在 compile\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 3. 也可以用 SGD + Momentum\n",
    "optimizer2 = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d78598-3948-4bf0-83c6-0132ec1f2966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45fefb-5705-49b8-9c07-a0e4c0da9fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepseek_env)",
   "language": "python",
   "name": "deepseek_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
