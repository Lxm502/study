{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fdbef09-108f-4e6c-b7c4-5b887596b4f9",
   "metadata": {},
   "source": [
    "# Keras\n",
    "## 尤其是 tensorflow.keras.layers 提供了丰富的神经网络层\n",
    "### 涵盖了从基础到高级各种常用组件。下面给你整理一份常用层的分类和简要说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac8b57-b10d-434b-a01c-6e8ca1fd0cc8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## 1. 基础层（Core Layers）\n",
    "1.1、Dense: \\\n",
    "  全连接层，最常用的层，适合处理固定形状的张量。\n",
    "\n",
    "1.2、Activation: \\\n",
    "  激活层，应用激活函数（如 ReLU、Sigmoid）。\n",
    "\n",
    "1.3、Dropout:\\\n",
    "  随机失活，用于防止过拟合。\n",
    "  \n",
    "1.4、Flatten:\\\n",
    "  多维张量拉平成一维。\n",
    "\n",
    "1.5、Reshape:\\\n",
    "  改变张量形状。\n",
    "\n",
    "1.6、BatchNormalization:\\\n",
    "  批标准化，加快训练速度并稳定训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c029137f-fd5a-4944-a7cb-350223f78908",
   "metadata": {},
   "source": [
    "### 1.1、tf.keras.layers.Dense（全连接层）\n",
    "是 TensorFlow / Keras 中最基本的全连接层（Fully Connected Layer），广泛用于几乎所有类型的神经网络中，包括：\n",
    "图像分类 \\\n",
    "自然语言处理  \\\n",
    "回归预测  \\\n",
    "时间序列建模\n",
    "\n",
    "#### 🧠 它做了什么？\n",
    "把输入的向量乘上一个权重矩阵并加上偏置向量，然后再通过一个激活函数输出结果。  \\\n",
    "数学形式： \\\n",
    "output = activation(𝑋𝑊+𝑏)\n",
    "\n",
    "tf.keras.layers.Dense(\n",
    "\n",
    "    units, --> 类型：int、含义：输出空间的维度，即神经元数量。（必须参数）\n",
    "    \n",
    "    activation=None, --> 应用于输出的激活函数。如 `'relu'`、`'sigmoid'`、`'softmax'\n",
    "    \n",
    "    use_bias=True, --> 是否使用偏置项\n",
    "    \n",
    "    kernel_initializer='glorot_uniform', --> 权重矩阵 W 的初始化方法\n",
    "    \n",
    "    bias_initializer='zeros', --> 偏置项 `b` 的初始化方法 \n",
    "    ...\n",
    ")\n",
    "| 参数                   | 说明                                      |\n",
    "| -------------------- | --------------------------------------- |\n",
    "| `units`              | 输出神经元个数（也是输出维度）  （必须参数）          |\n",
    "| `activation`         | 激活函数，如 `'relu'`、`'sigmoid'`、`'softmax'` |\n",
    "| `use_bias`           | 是否使用偏置项 `b`                             |\n",
    "| `kernel_initializer` | 权重矩阵 `W` 的初始化方法                         |\n",
    "| `bias_initializer`   | 偏置项 `b` 的初始化方法                          |\n",
    "\n",
    "#### ✅ 输入输出形状\n",
    "输入形状：任意形状，最后一维为特征数 input_dim \\\n",
    "输出形状：最后一维变为 units\n",
    "\n",
    "#### 🧮 参数计算方式\n",
    "如果你有：\\\n",
    "输入维度 = input_dim  \\\n",
    "输出维度 = units  \\\n",
    "那么参数个数为：\\\n",
    "params = input_dim × units + units\n",
    "\n",
    "例如：\n",
    "\n",
    "Dense(64) with input (None, 100)\n",
    "---> 参数 = 100 × 64 + 64 = 6464\n",
    "\n",
    "#### 🎯 使用场景总结\n",
    "| 场景          | 是否适合 Dense              |\n",
    "| ----------- | ----------------------- |\n",
    "| 分类任务输出层     | ✅ 使用 `softmax` 激活       |\n",
    "| 回归任务输出层     | ✅ 使用 `linear`（默认）       |\n",
    "| 图像、文本中的全连接层 | ✅ 通常用于 Flatten 后接 Dense |\n",
    "| 特征提取、嵌入     | ✅ 作为特征变换层               |\n",
    "| 卷积层替代       | ❌ 通常不替代 CNN 层           |\n",
    "\n",
    "#### 📌 常见组合方式\n",
    "| 模块      | 示例                                    |\n",
    "| ------- | ------------------------------------- |\n",
    "| 激活函数分离  | `Dense(128)` + `Activation('relu')`   |\n",
    "| 批归一化    | `Dense(...)` + `BatchNormalization()` |\n",
    "| Dropout | `Dense(...)` + `Dropout(0.5)`         |\n",
    "\n",
    "#### ⚠️ 注意事项\n",
    "若用于分类输出层，activation='softmax' 且 loss='sparse_categorical_crossentropy' 是常见搭配 \\\n",
    "输入必须是向量形式：使用 Flatten 或 GlobalAveragePooling2D 等层进行转换\n",
    "\n",
    "#### 🧮 参数详解\n",
    "2. activation \\\n",
    "类型：str 或函数、含义：应用于输出的激活函数\\\n",
    "常见值： \\\n",
    "'relu'：推荐默认，快速收敛  \\\n",
    "'sigmoid'：用于二分类输出   \\\n",
    "'softmax'：用于多分类输出  \\\n",
    "'tanh'：输出范围为 -1 到 1   \\\n",
    "None：不使用激活函数，线性输出\n",
    "\n",
    "示例：activation='relu'\n",
    "\n",
    "3. use_bias \\\n",
    "类型：bool，默认 True、含义：是否添加偏置项（bias）。\n",
    "\n",
    "4. kernel_initializer  \\\n",
    "类型：字符串或初始化器对象、含义：权重矩阵的初始化方法。 \\\n",
    "常用值：\n",
    "'glorot_uniform'（默认，适用于 ReLU） \\\n",
    "'he_normal'（适合 ReLU 家族） \\\n",
    "'random_normal'\n",
    "\n",
    "示例：kernel_initializer='he_normal'\n",
    "\n",
    "5. bias_initializer \\\n",
    "类型：字符串或初始化器、含义：偏置项的初始化方式。 \\\n",
    "默认值：'zeros'\n",
    "\n",
    "6. kernel_regularizer / bias_regularizer \\\n",
    "用于添加 L1 / L2 正则化防止过拟合。\n",
    "\n",
    "7. activity_regularizer  \\\n",
    "对层的输出应用正则化。\n",
    "\n",
    "8. name  \\\n",
    "给这层取一个名字，方便调试或模型可视化。\n",
    "\n",
    "#### 📘 小结\n",
    "\n",
    "| 参数名                    | 作用      | 常见取值 / 示例                          |\n",
    "| ---------------------- | ------- | ---------------------------------- |\n",
    "| `units`                | 输出神经元个数 | `units=128`                        |\n",
    "| `activation`           | 激活函数    | `'relu'`, `'sigmoid'`, `'softmax'` |\n",
    "| `use_bias`             | 是否加偏置   | `True`                             |\n",
    "| `kernel_initializer`   | 权重初始化方式 | `'he_normal'`                      |\n",
    "| `bias_initializer`     | 偏置初始化方式 | `'zeros'`                          |\n",
    "| `kernel_regularizer`   | 权重正则化   | `l2(0.01)`                         |\n",
    "| `activity_regularizer` | 输出正则化   | `l1(0.01)`                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9ae95-e7bd-4ece-b4c2-f6c664b1bd7c",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2、tf.keras.layers.Activation 是 TensorFlow Keras 中的激活函数层\n",
    "\n",
    "用于为前一层的输出添加`非线性变换`。这是构建神经网络的关键步骤之一，可以让网络学习更复杂的模式。\n",
    "\n",
    "激活函数是神经网络中每一层的`“非线性门`”，它的作用是：\n",
    "\n",
    " - 引入非线性：没有激活函数，神经网络只相当于线性变换的叠加，无法拟合复杂的函数。\n",
    "\n",
    " - 控制输出范围：比如 `sigmoid` 将输出限制在 [0, 1]，`tanh` 在 [-1, 1]，有利于梯度传播。加速收敛、减少过拟合（部分激活函数具备此特性）。\n",
    "\n",
    "---\n",
    "### 一、使用方法：\n",
    "\n",
    "方法 1、直接使用 `Activation` 层显式指定\n",
    "\n",
    "model = tf.keras.Sequential([  \\\n",
    "    layers.Dense(64), \\\n",
    "    layers.Activation('relu'),  # 显式添加 ReLU 激活函数  \\\n",
    "])\n",
    "\n",
    "方法 2：在 `Dense` 层中直接指定 `activation` 参数（更常用）\n",
    "\n",
    "model = tf.keras.Sequential([ \\\n",
    "    layers.Dense(64, activation='relu'),  # 推荐写法，更简洁\n",
    "])\n",
    "\n",
    "---\n",
    "### 二、常用激活函数说明\n",
    "| 名称          | 作用范围        | 特点/使用场景               |\n",
    "| ----------- | ----------- | --------------------- |\n",
    "| `'relu'`    | \\[0, ∞)     | 默认首选，快速收敛，适用于多数隐藏层    |\n",
    "| `'sigmoid'` | (0, 1)      | 用于二分类输出层，可能造成梯度消失     |\n",
    "| `'tanh'`    | (-1, 1)     | 比 sigmoid 更对称，适合某些隐藏层 |\n",
    "| `'softmax'` | \\[0, 1]，和为1 | 用于多分类输出层（如10类）        |\n",
    "| `'linear'`  | 原样输出        | 通常用于回归任务的输出层          |\n",
    "\n",
    "---\n",
    "### 三、激活函数选用场景图（推荐）\n",
    "| 层类型          | 推荐激活函数       | 原因/适用场景                                 |\n",
    "| ------------ | ------------ | --------------------------------------- |\n",
    "| **输入层**      | 无            | 通常不加激活，直接传递输入特征                         |\n",
    "| **隐藏层**      | `relu`       | 默认首选，简单高效，能有效避免梯度消失问题                   |\n",
    "|              | `leaky_relu` | 当 `relu` 出现“神经元死亡”问题时（输出恒为0）            |\n",
    "|              | `tanh`       | 若数据是负对称的；适用于 RNN（例如 LSTM）               |\n",
    "|              | `gelu`       | Transformer 等大模型中使用，性能较好                |\n",
    "| **输出层**（回归）  | `linear`     | 直接输出数值，不加限制                             |\n",
    "| **输出层**（二分类） | `sigmoid`    | 输出为概率值，配合 `BinaryCrossentropy` 使用       |\n",
    "| **输出层**（多分类） | `softmax`    | 输出类别概率和为 1，配合 `CategoricalCrossentropy` |\n",
    "\n",
    "---\n",
    "### 四、使用建议\n",
    "\n",
    " - 隐藏层推荐：`relu`, `leaky_relu`, `gelu`。\n",
    "\n",
    " - 输出层分类用：\n",
    "\n",
    "    - 二分类：`sigmoid` + `BinaryCrossentropy`\n",
    "\n",
    "    - 多分类：`softmax` + `CategoricalCrossentropy`\n",
    "\n",
    " - 输出层回归用：`linear`（即不加激活）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ea297-fba4-446f-8ee0-7232cdc85fdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 1.3、tf.keras.layers.Dropout（防止神经网络过拟合，提高模型的泛化能力）\n",
    "\n",
    "Dropout 层的主要作用是在训练过程中，以一定的概率将神经元的输出“随机置为 0”，从而使模型不会过度依赖某些特定神经元，提高模型的泛化能力。\n",
    "\n",
    "在训练时生效（每个 batch 都会随机丢弃部分神经元）。\n",
    "\n",
    "在预测（测试）时不启用 Dropout，即保留所有神经元，并将输出乘以一个缩放因子（自动处理，无需手动指定）。\n",
    "\n",
    "tf.keras.layers.Dropout(\n",
    "\n",
    "    rate,              # 必选参数，丢弃率。0~1之间,一般取0.2~0.5，例如 rate=0.5 表示每次训练随机将 50% 的神经元输出设为 0\n",
    "    \n",
    "    noise_shape=None,  # 控制哪些维度进行 dropout，默认为 None，表示在所有维度上应用 Dropout。常用于 时间序列 或 RNN 中\n",
    "    \n",
    "    seed=None,         # 随机种子，保证可重复性\n",
    "    \n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd338c-e141-4b1e-a3b7-fa56118a950d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 1.4、tf.keras.layers.Flatten  是 Keras 中的一个实用层，用于将多维张量展平为一维向量。\n",
    "它通常出现在卷积层（如 Conv2D）之后，用于连接到全连接层（Dense）。\n",
    "\n",
    "1、为什么要使用 Flatten？\n",
    "\n",
    "神经网络中的卷积层或池化层会输出形如 (batch_size, height, width, channels) 的 4D 张量。\n",
    "而全连接层（Dense）要求输入为 2D 的 (batch_size, features)，这时就需要 Flatten 来变形。\n",
    "\n",
    "2、Flatten 的参数说明\n",
    "\n",
    "tf.keras.layers.Flatten(\n",
    "    data_format=None,  # 可选：\"channels_last\"（默认）或 \"channels_first\"\n",
    "    **kwargs\n",
    ")\n",
    "| 参数名           | 类型      | 说明                                                                   |\n",
    "| ------------- | ------- | -------------------------------------------------------------------- |\n",
    "| `data_format` | 字符串     | 默认为 `\"channels_last\"`（即 NHWC 格式），也可以设为 `\"channels_first\"`（即 NCHW 格式） |\n",
    "| `kwargs`      | 其他关键字参数 | 一般不需要使用，可用于自定义子类时传递                                                  |\n",
    "\n",
    "3、Flatten vs Reshape 对比\n",
    "| 操作          | 用途                 | 灵活性    |\n",
    "| ----------- | ------------------ | ------ |\n",
    "| `Flatten()` | 自动将除 batch 维度外全部展开 | 最常用，简单 |\n",
    "| `Reshape()` | 明确指定输出形状           | 更灵活、可控 |\n",
    "\n",
    "4、适用场景\n",
    "\n",
    "卷积网络 → 全连接层之间的连接桥梁\n",
    "\n",
    "也可用于任何需要展平结构、准备进入全连接层的地方\n",
    "\n",
    "✅ Flatten 是 CNN 中的桥梁层，把张量展平成向量。\n",
    "\n",
    "✅ 通常放在卷积/池化之后、全连接层之前。\n",
    "\n",
    "✅ 自动适配输入形状，只展平，不影响 batch size。\n",
    "\n",
    "✅ 无需参数、非常轻量，极常用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3339dc-b8da-41b5-aa1d-fc0078e8e357",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 1.5、tf.keras.layers.Reshape 是 Keras 中用于改变张量形状的层（Layer），功能类似于 NumPy 中的 reshape()\n",
    "\n",
    "1、它的主要作用是：\n",
    "\n",
    "✅ 将输入张量的形状（除了 batch 大小）转换为指定的新形状，而不改变总元素个数。\n",
    "\n",
    "2、参数说明\n",
    "\n",
    "tf.keras.layers.Reshape(\n",
    "    target_shape,     # 指定新形状（不包含 batch 维度）\n",
    "    **kwargs\n",
    ")\n",
    "| 参数名            | 类型      | 说明                   |\n",
    "| -------------- | ------- | -------------------- |\n",
    "| `target_shape` | 元组或列表   | 新形状（不包括 batch\\_size） |\n",
    "| `kwargs`       | 其他关键字参数 | 用于兼容 Layer API       |\n",
    "\n",
    "3、输入输出形状示例\n",
    "\n",
    "假设输入是 shape: (batch_size, 4, 4, 1)，即：batch_size x 高 x 宽 x 通道数\n",
    "\n",
    "我们可以 reshape 成：\n",
    "\n",
    "layers.Reshape((16, 1))     # 输出: (batch_size, 16, 1)\n",
    "\n",
    "layers.Reshape((2, 2, 4))   # 输出: (batch_size, 2, 2, 4)\n",
    "\n",
    "4、与 Flatten 的区别\n",
    "| 层类型         | 作用                | 适用场景                  |\n",
    "| ----------- | ----------------- | --------------------- |\n",
    "| `Flatten()` | 展平为一维向量（除了 batch） | 卷积层后接全连接层             |\n",
    "| `Reshape()` | 按指定形状重构张量         | 更灵活，适用于任意变换（前提是元素数一致） |\n",
    "\n",
    "💬 小结\n",
    "| 优点              | 注意事项                |\n",
    "| --------------- | ------------------- |\n",
    "| ✅ 灵活重构数据结构      | ❗ 重构前后元素数量必须一致      |\n",
    "| ✅ 适用于图像、序列数据    | ❗ 不包含 batch 维度      |\n",
    "| ✅ 可配合 `-1` 自动推断 | ❗ 不改变数据内容，只改变 shape |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296b84b-3a80-4bec-aba2-ea3886181e7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 1.6、tf.keras.layers.BatchNormalization 是 深度学习中常用的一种加速训练和提高模型稳定性的技术。\n",
    "\n",
    "1、✅ 它的主要作用是：\n",
    "\n",
    "对每个 mini-batch（小批量） 的特征维度进行标准化处理，使其均值为 0，方差为 1。\n",
    "\n",
    "Mini-batch 是指在训练神经网络等机器学习模型时，每次迭代（iteration）所使用的训练数据的一个小子集。 它是介于“全批量梯度下降”和“随机梯度下降”之间的一种折中策略\n",
    "\n",
    "Mini-batch 方法就是为了克服上述两种方法的缺点而设计的\n",
    "\n",
    "做法：\n",
    "\n",
    "将整个训练数据集打乱。\n",
    "\n",
    "分割成多个较小的、大小固定（通常是2的幂，如32, 64, 128, 256）的组，每个组就是一个 mini-batch。\n",
    "\n",
    "在每个 迭代中：\n",
    "\n",
    "模型处理一个 mini-batch（比如64张图片）。\n",
    "\n",
    "计算这个 mini-batch 的平均损失。\n",
    "\n",
    "计算这个 mini-batch 的平均梯度。\n",
    "\n",
    "使用这个平均梯度来更新一次模型参数。\n",
    "\n",
    "处理完所有 mini-batch（即遍历完一次整个训练数据集）称为完成了一个 epoch。训练通常需要多个 epoch。\n",
    "\n",
    "2、作用和好处\n",
    "| 好处            | 解释                       |\n",
    "| ------------- | ------------------------ |\n",
    "| ✅ 加速收敛速度      | 减少梯度消失/爆炸，提高训练效率         |\n",
    "| ✅ 减少对权重初始化的依赖 | 不需要那么精细的初始化方法            |\n",
    "| ✅ 降低过拟合       | 有轻微正则化作用，降低对 Dropout 的依赖 |\n",
    "| ✅ 提高模型精度和稳定性  | 网络更容易训练                  |\n",
    "\n",
    "3、使用方法\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(128),\n",
    "    layers.BatchNormalization(),  # 加在激活函数之前或之后（推荐在之前）\n",
    "    layers.Activation('relu')\n",
    "])\n",
    "\n",
    "4、常用参数说明\n",
    "\n",
    "tf.keras.layers.BatchNormalization(\n",
    "    axis=-1,\n",
    "    momentum=0.99,\n",
    "    epsilon=1e-3,\n",
    "    center=True,\n",
    "    scale=True,\n",
    "    beta_initializer='zeros',\n",
    "    gamma_initializer='ones',\n",
    "    moving_mean_initializer='zeros',\n",
    "    moving_variance_initializer='ones',\n",
    "    trainable=True,\n",
    "    **kwargs\n",
    ")\n",
    "| 参数名         | 含义                                                                                         | 默认值    |\n",
    "| ----------- | ------------------------------------------------------------------------------------------ | ------ |\n",
    "| `axis`      | 指定要规范化的轴（通常是特征轴）。对于 `Dense` 是 `-1`，对于 `Conv2D` 是 `3`（channels\\_last）或 `1`（channels\\_first） | `-1`   |\n",
    "| `momentum`  | 用于更新移动平均的动量（用于推理）                                                                          | `0.99` |\n",
    "| `epsilon`   | 避免除以 0 的小常数（用于数值稳定性）                                                                       | `1e-3` |\n",
    "| `center`    | 是否添加可训练偏移参数 β（beta）                                                                        | `True` |\n",
    "| `scale`     | 是否添加可训练缩放参数 γ（gamma）                                                                       | `True` |\n",
    "| `trainable` | 是否可训练                                                                                      | `True` |\n",
    "\n",
    "5、训练 vs 推理（inference）\n",
    "| 阶段       | 均值与方差来源                |\n",
    "| -------- | ---------------------- |\n",
    "| **训练阶段** | 使用当前 mini-batch 的均值和方差 |\n",
    "| **推理阶段** | 使用训练过程中累计的移动平均值        |\n",
    "\n",
    "6、在不同模型中的使用位置\n",
    "| 模型类型                 | 推荐使用方式                                       |\n",
    "| -------------------- | -------------------------------------------- |\n",
    "| **全连接网络（MLP）**       | 放在 `Dense` 后，`Activation` 前或后都可以（一般先 BN 后激活） |\n",
    "| **卷积神经网络（CNN）**      | 放在 `Conv2D` 后面，`ReLU` 之前                     |\n",
    "| **循环神经网络（RNN/LSTM）** | 不常用 BN（推荐 LayerNormalization）                |\n",
    "\n",
    "7、BatchNormalization vs LayerNormalization\n",
    "| 特点                 | BatchNormalization | LayerNormalization |\n",
    "| ------------------ | ------------------ | ------------------ |\n",
    "| 按哪个维度归一化           | Batch 维度           | 每个样本内部维度           |\n",
    "| 是否受 batch\\_size 影响 | 是                  | 否                  |\n",
    "| 适用模型               | CNN、Dense 网络       | RNN、Transformer    |\n",
    "\n",
    "🧾 8、总结\n",
    "| 优点      | 使用场景             | 注意事项                                  |\n",
    "| ------- | ---------------- | ------------------------------------- |\n",
    "| ✅ 加快训练  | Dense 层、Conv 层之后 | ❗推理时自动使用移动均值                          |\n",
    "| ✅ 减少过拟合 | 搭配 ReLU 效果佳      | ❗不能直接用于 LSTM，推荐用 `LayerNormalization` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887142c-7e7f-476a-aba2-ddb0ca706c4a",
   "metadata": {},
   "source": [
    "## 2. 卷积层（Convolutional Layers）\n",
    "2.1、Conv1D:\n",
    "一维卷积，适合时间序列、文本。\n",
    "\n",
    "2.2、Conv2D:\n",
    "二维卷积，最常用的图像卷积层。\n",
    "\n",
    "2.3、Conv3D:\n",
    "三维卷积，用于视频、3D数据。\n",
    "\n",
    "2.4、SeparableConv2D:\n",
    "深度可分离卷积，参数更少，计算更快。\n",
    "\n",
    "2.5、Conv2DTranspose:\n",
    "转置卷积（反卷积），常用于生成模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2f702-815a-4dbd-b197-a1422f6b4c14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 2.1、tf.keras.layers.Conv1D \n",
    "是 TensorFlow 中用于处理一维序列数据（如时间序列、音频、文本）的卷积层。\n",
    "\n",
    "🧠 作用理解\n",
    "\n",
    "Conv1D = 滑动一个小窗口（卷积核）在时间轴上提取局部特征\n",
    "\n",
    "比如：你用过去 10 个时间步的温度数据预测未来的变化趋势，Conv1D 就像一个移动的“模式检测器”。\n",
    "\n",
    "tf.keras.layers.Conv1D(\n",
    "\n",
    "    filters, --> 卷积核个数（输出通道数 / 特征图数）\n",
    "    \n",
    "    kernel_size, --> 卷积核的大小（时间步的跨度）\n",
    "    \n",
    "    strides=1, --> 步长，卷积窗口移动的距离 \n",
    "    \n",
    "    padding='valid', --> `'valid'`：不补零； `'same'`：补零后保持输出长度不变，输出长度 = 输入长度\n",
    "    \n",
    "    activation=None, --> 激活函数（如 `'relu'`、`'tanh'`、`None`）\n",
    "    \n",
    "    use_bias=True,  --> 是否使用偏置项 \n",
    "    \n",
    "    kernel_initializer='glorot_uniform', --> 卷积核的初始化方式    \n",
    "    \n",
    "    bias_initializer='zeros', --> 偏置的初始化方式   \n",
    "    ...\n",
    ")\n",
    "\n",
    "📑 参数详解\n",
    "| 参数                   | 说明                                  |\n",
    "| -------------------- | ----------------------------------- |\n",
    "| `filters`            | 卷积核个数（输出通道数 / 特征图数）                 |\n",
    "| `kernel_size`        | 卷积核的大小（时间步的跨度）                      |\n",
    "| `strides`            | 步长，卷积窗口移动的距离                        |\n",
    "| `padding`            | `'valid'`：不补零； `'same'`：补零后保持输出长度不变，输出长度 = 输入长度 |\n",
    "| `activation`         | 激活函数（如 `'relu'`、`'tanh'`、`None`）    |\n",
    "| `use_bias`           | 是否使用偏置项                             |\n",
    "| `kernel_initializer` | 卷积核的初始化方式                           |\n",
    "| `bias_initializer`   | 偏置的初始化方式                            |\n",
    "\n",
    "✅ 输入输出形状\n",
    "\n",
    "输入 shape: (batch_size, time_steps, input_dim)\n",
    "\n",
    "输出 shape: (batch_size, new_time_steps, filters)\n",
    "\n",
    "其中 new_time_steps 由输入长度、kernel_size、stride、padding 决定。\n",
    "\n",
    "🔁 与 LSTM 的对比\n",
    "| 特性     | Conv1D   | LSTM / GRU |\n",
    "| ------ | -------- | ---------- |\n",
    "| 并行计算   | ✅ 快速     | ❌ 顺序执行慢    |\n",
    "| 捕捉局部模式 | ✅ 很强     | ✅ 可记忆全局依赖  |\n",
    "| 长期依赖建模 | ❌ 一般     | ✅ 强        |\n",
    "| 用途     | 快速提取局部特征 | 学习时间上下文信息  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e75e9-b51d-4341-8517-724f3ba49813",
   "metadata": {},
   "source": [
    "### 2.2、tf.keras.layers.Conv2D \n",
    "是 TensorFlow / Keras 中的二维卷积层，广泛用于图像处理和计算机视觉任务\n",
    "\n",
    "🧠 作用理解\n",
    "\n",
    "Conv2D 的功能就是在 2D 空间上（如图像的宽 × 高）滑动卷积核，提取局部空间特征，例如边缘、纹理、形状等。\n",
    "\n",
    "它是卷积神经网络（CNN）的基础构建块。\n",
    "\n",
    "tf.keras.layers.Conv2D(\n",
    "\n",
    "    filters,  --> 卷积核的数量（决定输出通道数） \n",
    "    \n",
    "    kernel_size, --> 卷积核尺寸，如 `(3,3)`，可简写为 `3`\n",
    "    \n",
    "    strides=(1, 1), --> 步长，决定卷积核移动的步伐，默认为 `(1, 1)`  \n",
    "    \n",
    "    padding='valid', --> `'valid'`：无填充，输出尺寸变小；`'same'`：自动填充，保持尺寸不变\n",
    "    \n",
    "    activation=None, --> 激活函数，如 `'relu'`, `'sigmoid'`  \n",
    "    \n",
    "    use_bias=True, --> 是否使用偏置项      \n",
    "    \n",
    "    kernel_initializer='glorot_uniform', --> 卷积核权重初始化方法 \n",
    "    \n",
    "    bias_initializer='zeros', --> 偏置初始化方法   \n",
    "    ...\n",
    ")\n",
    "\n",
    "📑 参数详解\n",
    "| 参数                   | 说明                                        |\n",
    "| -------------------- | ----------------------------------------- |\n",
    "| `filters`            | 卷积核的数量（决定输出通道数）                           |\n",
    "| `kernel_size`        | 卷积核尺寸，如 `(3,3)`，可简写为 `3`                  |\n",
    "| `strides`            | 步长，决定卷积核移动的步伐，默认为 `(1, 1)`                |\n",
    "| `padding`            | `'valid'`：无填充，输出尺寸变小；`'same'`：自动填充，保持尺寸不变 |\n",
    "| `activation`         | 激活函数，如 `'relu'`, `'sigmoid'`              |\n",
    "| `use_bias`           | 是否使用偏置项                                   |\n",
    "| `kernel_initializer` | 卷积核权重初始化方法                                |\n",
    "| `bias_initializer`   | 偏置初始化方法                                   |\n",
    "\n",
    "✅ 输入/输出形状\n",
    "\n",
    "输入 shape: (batch_size, height, width, channels)\n",
    "\n",
    "输出 shape: (batch_size, new_height, new_width, filters)\n",
    "\n",
    "🔁 常见用法组合（典型 CNN 架构）\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(64, 64, 3)),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # 多分类输出\n",
    "])\n",
    "\n",
    "🎯 应用场景\n",
    "| 任务         | 是否适用 Conv2D   |\n",
    "| ---------- | ------------- |\n",
    "| 图像分类       | ✅             |\n",
    "| 目标检测       | ✅             |\n",
    "| 图像分割       | ✅             |\n",
    "| 医疗图像分析     | ✅             |\n",
    "| 视频数据（每帧图像） | ✅（也可以用 3D 卷积） |\n",
    "\n",
    "🔍 对比：Conv1D vs Conv2D\n",
    "| 特性   | Conv1D                     | Conv2D                             |\n",
    "| ---- | -------------------------- | ---------------------------------- |\n",
    "| 用于   | 时间序列、一维信号                  | 图像、二维数据                            |\n",
    "| 输入维度 | `(batch, steps, features)` | `(batch, height, width, channels)` |\n",
    "| 卷积核  | 1D（只沿时间/空间一维滑动）            | 2D（沿两个维度滑动）                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c58f7-b57d-408e-a177-1eba1dab40f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 2.3、tf.keras.layers.Conv3D\n",
    "是 TensorFlow 中用于三维数据卷积操作的层，适用于如下类型的数据：\n",
    "✅ 常见应用场景：\n",
    "| 应用          | 举例                           |\n",
    "| ----------- | ---------------------------- |\n",
    "| 医学影像分析      | 3D CT、MRI 扫描图像（体积数据）         |\n",
    "| 视频处理        | 视频 = 时间 + 高度 + 宽度（可看作 3D 图像） |\n",
    "| 3D 数据建模     | 点云体素化、3D 建模等                 |\n",
    "| 超分辨率 / 动作识别 | 基于视频帧序列建模                    |\n",
    "📑 参数说明\n",
    "| 参数            | 含义                                        |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| `filters`     | 卷积核数量（即输出通道数）                             |\n",
    "| `kernel_size` | 卷积核的三维尺寸，如 `(3,3,3)`（可简写为单个数字）            |\n",
    "| `strides`     | 卷积滑动步长，默认是 1                              |\n",
    "| `padding`     | `'valid'`（不补零）或 `'same'`（自动补零，保持输出尺寸）     |\n",
    "| `activation`  | 激活函数，如 `'relu'`、`'sigmoid'`               |\n",
    "| `use_bias`    | 是否添加偏置项                                   |\n",
    "| `data_format` | `'channels_last'`（默认）或 `'channels_first'` |\n",
    "\n",
    "✅ 输入输出 shape\n",
    "\n",
    "输入形状（默认 channels_last）：\n",
    "(batch_size, depth, height, width, channels)\n",
    "\n",
    "输出形状：\n",
    "(batch_size, new_depth, new_height, new_width, filters)\n",
    "\n",
    "📌 注意事项\n",
    "| 问题                                             | 说明                                        |\n",
    "| ---------------------------------------------- | ----------------------------------------- |\n",
    "| 输入必须是 5D 张量                                    | `[batch, depth, height, width, channels]` |\n",
    "| 内存开销大                                          | 3D 卷积运算和参数远大于 Conv2D                      |\n",
    "| 可与 `MaxPooling3D`、`UpSampling3D` 等配合使用         | ✅                                         |\n",
    "| 对于视频可选两种方式建模：Conv3D vs TimeDistributed(Conv2D) | 前者学习空间+时间联合特征；后者是时间上独立卷积                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a641b-7f0a-4b9f-a96a-1e4a52103549",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 2.4、tf.keras.layers.SeparableConv2D\n",
    " 是 TensorFlow / Keras 中的一种轻量级二维卷积层，其核心思想是将标准卷积操作拆分为两个步骤：\n",
    "\n",
    "深度可分离卷积（Depthwise Separable Convolution）\n",
    "\n",
    "是一种更高效的卷积方式，能够在不明显损失性能的前提下，大幅减少模型的参数量和计算量。\n",
    "\n",
    "🧠 与 Conv2D 的区别\n",
    "\n",
    "Conv2D（标准卷积）：\n",
    "\n",
    "同时在空间和通道维度上进行卷积\n",
    "\n",
    "计算量大，参数多\n",
    "\n",
    "SeparableConv2D：\n",
    "\n",
    "分成两步：\n",
    "\n",
    "Depthwise 卷积（每个输入通道单独卷积，不混合通道）\n",
    "\n",
    "Pointwise 卷积（使用 1×1 卷积混合通道）\n",
    "\n",
    "这种方法参数量和计算量大幅下降，特别适用于：\n",
    "\n",
    "移动设备\n",
    "\n",
    "实时推理\n",
    "\n",
    "轻量模型（如 MobileNet）\n",
    "\n",
    "📑 参数说明\n",
    "| 参数                      | 说明                       |\n",
    "| ----------------------- | ------------------------ |\n",
    "| `filters`               | 最终输出通道数（Pointwise 卷积后输出） |\n",
    "| `kernel_size`           | 卷积核大小，如 `(3,3)`          |\n",
    "| `strides`               | 步长，默认为 `(1,1)`           |\n",
    "| `padding`               | `'same'` 或 `'valid'`     |\n",
    "| `depth_multiplier`      | 每个输入通道卷积核个数（默认1）         |\n",
    "| `activation`            | 激活函数，如 `'relu'`          |\n",
    "| `use_bias`              | 是否使用偏置项                  |\n",
    "| `depthwise_initializer` | Depthwise 卷积核初始化方式       |\n",
    "| `pointwise_initializer` | Pointwise 卷积核初始化方式       |\n",
    "\n",
    "✅ 输入输出形状\n",
    "\n",
    "输入：(batch, height, width, channels)\n",
    "\n",
    "输出：(batch, new_height, new_width, filters)\n",
    "\n",
    "✅ 实用场景\n",
    "| 场景                  | 是否推荐           |\n",
    "| ------------------- | -------------- |\n",
    "| 构建轻量模型（如 MobileNet） | ✅              |\n",
    "| 图像分类任务中快速卷积层        | ✅              |\n",
    "| 有效减少模型大小            | ✅              |\n",
    "| 想要最高性能而不在乎计算量       | ❌ 建议用 `Conv2D` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add696b1-1970-4070-afc7-de3289c4e264",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 2.5、tf.keras.layers.Conv2DTranspose \n",
    "是 TensorFlow 中用于 反卷积（转置卷积） 的层，常用于 生成模型（如 GAN） 或 上采样操作（如 U-Net） 中，将特征图的空间尺寸放大。\n",
    "\n",
    "✅ 作用\n",
    "\n",
    "它是 Conv2D 的逆操作：\n",
    "\n",
    "Conv2D：对输入图像下采样或提取特征\n",
    "\n",
    "Conv2DTranspose：将特征图上采样，增大空间尺寸（高和宽）\n",
    "\n",
    "🔧 常用参数说明\n",
    "\n",
    "tf.keras.layers.Conv2DTranspose(\n",
    "\n",
    "    filters, -->  输出空间的通道数（即输出图像的深度）\n",
    "    \n",
    "    kernel_size, --> 卷积核尺寸，如 `(3, 3)`\n",
    "    \n",
    "    strides=(1, 1), --> 步幅，控制上采样的放大倍数   \n",
    "    \n",
    "    padding='valid', --> `'same'输入=输出` or `'valid'` 输出变小\n",
    "    \n",
    "    output_padding=None, --> 控制输出大小，帮助确定边界情况，通常用于特定的上采样需求\n",
    "    \n",
    "    data_format=None, -->\n",
    "    \n",
    "    dilation_rate=(1, 1), -->\n",
    "    \n",
    "    activation=None, --> 激活函数，如 `'relu'`, `'sigmoid'` 等\n",
    "    \n",
    "    use_bias=True, --> 是否加偏置项    \n",
    "    \n",
    "    kernel_initializer='glorot_uniform', --> 卷积核初始化方式      \n",
    "    \n",
    "    bias_initializer='zeros', --> 偏置项初始化方式      \n",
    "    \n",
    "    kernel_regularizer=None, -->\n",
    "    \n",
    "    bias_regularizer=None, -->\n",
    "    \n",
    "    activity_regularizer=None, -->\n",
    "    \n",
    "    kernel_constraint=None, -->\n",
    "    \n",
    "    bias_constraint=None, -->\n",
    "    \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "| 参数                   | 说明                             |\n",
    "| -------------------- | ------------------------------ |\n",
    "| `filters`            | 输出空间的通道数（即输出图像的深度）             |\n",
    "| `kernel_size`        | 卷积核尺寸，如 `(3, 3)`               |\n",
    "| `strides`            | 步幅，控制上采样的放大倍数                  |\n",
    "| `padding`            | `'same'` or `'valid'`，控制输出尺寸   |\n",
    "| `output_padding`     | 控制输出大小，帮助确定边界情况，通常用于特定的上采样需求   |\n",
    "| `activation`         | 激活函数，如 `'relu'`, `'sigmoid'` 等 |\n",
    "| `use_bias`           | 是否加偏置项                         |\n",
    "| `kernel_initializer` | 卷积核初始化方式                       |\n",
    "| `bias_initializer`   | 偏置项初始化方式                       |\n",
    "\n",
    "🧠 工作原理简化理解\n",
    "\n",
    "以 stride=2 为例：\n",
    "\n",
    "Conv2D：把一个大图压缩为小图（如 32×32 → 16×16）\n",
    "\n",
    "Conv2DTranspose：把小图还原为大图（如 16×16 → 32×32）\n",
    "\n",
    "它不是直接“插值”，而是通过可学习的“反卷积核”实现的空间恢复。\n",
    "\n",
    "📌 小技巧\n",
    "\n",
    "若你用它来恢复图像尺寸，一般选择 strides=2。\n",
    "\n",
    "与 UpSampling2D + Conv2D 不同，Conv2DTranspose 的权重是可学习的，学习如何“上采样”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc7af7-0a8e-4365-abe6-bbb4f359446e",
   "metadata": {},
   "source": [
    "## 3. 池化层（Pooling Layers）\n",
    "3.1、MaxPooling1D / 2D / 3D:\n",
    "最大池化，取区域最大值。\n",
    "\n",
    "3.2、AveragePooling1D / 2D / 3D:\n",
    "平均池化。\n",
    "\n",
    "3.3、GlobalMaxPooling1D / 2D / 3D:\n",
    "对整个空间维度取最大值。\n",
    "\n",
    "3.4、GlobalAveragePooling1D / 2D / 3D:\n",
    "对整个空间维度取平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132543f-05cc-4461-ac52-754b9925e5da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 3.1、tf.keras.layers.MaxPooling1D/2D/3D\n",
    "分别用于对 1D、2D、3D 的输入数据进行 最大池化（Max Pooling）操作，它是一种 下采样方法，用于减少特征图的空间维度，同时保留最重要的特征。\n",
    "\n",
    "🧠 什么是 MaxPooling？\n",
    "\n",
    "MaxPooling 的核心思想是：\n",
    "\n",
    "在一个局部区域内，取最大值来代表该区域的特征。\n",
    "\n",
    "它通常跟卷积层一起使用，用于压缩数据、降低参数量、加快训练速度、减少过拟合。\n",
    "\n",
    "✅ 通用参数说明（1D/2D/3D 通用）\n",
    "\n",
    "tf.keras.layers.MaxPooling2D(\n",
    "\n",
    "    pool_size=(2, 2),\n",
    "    \n",
    "    strides=None,\n",
    "    \n",
    "    padding='valid',\n",
    "    \n",
    "    data_format=None,\n",
    "    \n",
    "    **kwargs\n",
    ")\n",
    "| 参数            | 说明                                        |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| `pool_size`   | 池化窗口的大小（如 2 表示在每个维度上取 2）                  |\n",
    "| `strides`     | 步幅（默认为 `pool_size`）                       |\n",
    "| `padding`     | `'valid'`（不填充）或 `'same'`（边缘填充）            |\n",
    "| `data_format` | `'channels_last'`（默认）或 `'channels_first'` |\n",
    "\n",
    "📦 各版本对比\n",
    "| 层              | 输入形状                                      | 操作维度         | 使用场景        |\n",
    "| -------------- | ----------------------------------------- | ------------ | ----------- |\n",
    "| `MaxPooling1D` | `(batch, steps, channels)`                | 时间序列 / 1D 数据 | NLP / 信号处理  |\n",
    "| `MaxPooling2D` | `(batch, height, width, channels)`        | 图像 / 视频帧     | 图像处理        |\n",
    "| `MaxPooling3D` | `(batch, depth, height, width, channels)` | 视频 / 医学影像    | 体积数据、CT 扫描等 |\n",
    "\n",
    "🆚 MaxPooling vs AveragePooling\n",
    "| 对比项 | MaxPooling | AveragePooling |\n",
    "| --- | ---------- | -------------- |\n",
    "| 方法  | 取最大值       | 取平均值           |\n",
    "| 优点  | 更突出强特征     | 更平滑、保留更多信息     |\n",
    "| 适用  | 图像边缘、物体识别等 | 图像压缩、背景特征保留等   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f482c-3fb4-45d6-abfe-038bb3449bc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 3.2、tf.keras.layers.AveragePooling\n",
    "是 Keras 中用于对特征图进行 平均池化（Average Pooling） 的层，通常用于下采样，与 MaxPooling 类似，但方式不同。\n",
    "\n",
    "🔍 什么是 Average Pooling？\n",
    "\n",
    "Average Pooling 是指：\n",
    "\n",
    "在每个池化窗口区域内，计算像素/特征的 平均值，并作为输出。\n",
    "\n",
    "和 MaxPooling 不同，AveragePooling 更加平滑，保留了所有特征的整体趋势，而不是只提取最大值。\n",
    "\n",
    "✅ 三个版本介绍\n",
    "| 层名                 | 适用数据           | 输入维度                                      |\n",
    "| ------------------ | -------------- | ----------------------------------------- |\n",
    "| `AveragePooling1D` | 时间序列、文本向量      | `(batch, steps, channels)`                |\n",
    "| `AveragePooling2D` | 图像             | `(batch, height, width, channels)`        |\n",
    "| `AveragePooling3D` | 视频 / 医学图像等体积数据 | `(batch, depth, height, width, channels)` |\n",
    "\n",
    "🧷 通用参数说明\n",
    "\n",
    "tf.keras.layers.AveragePooling2D(\n",
    "\n",
    "    pool_size=(2, 2),\n",
    "    \n",
    "    strides=None,\n",
    "    \n",
    "    padding='valid',\n",
    "    \n",
    "    data_format=None\n",
    ")\n",
    "| 参数            | 说明                                        |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| `pool_size`   | 池化窗口大小，如 `(2, 2)`                         |\n",
    "| `strides`     | 步长，默认为 `pool_size`                        |\n",
    "| `padding`     | `'valid'`（不补零）或 `'same'`（补零保证输出尺寸）        |\n",
    "| `data_format` | `'channels_last'`（默认）或 `'channels_first'` |\n",
    "\n",
    "📌 典型使用场景\n",
    "\n",
    "用于模型后期下采样平滑处理\n",
    "\n",
    "图像特征全局平均池化（如 GlobalAveragePooling2D）\n",
    "\n",
    "替代 MaxPooling 的更平滑版本，用于防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531fb45-6da3-48bd-8a7d-e53da1877acc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 3.3、tf.keras.layers.GlobalMaxPooling1D / 2D / 3D\n",
    "是 TensorFlow Keras 中的全局最大池化层，用于将整个特征图中的每个通道（channel）压缩为一个最大值。\n",
    "\n",
    "🧠 什么是 Global Max Pooling？\n",
    "\n",
    "Global Max Pooling（全局最大池化） 是一种特殊的最大池化，它不像普通池化那样使用固定大小的窗口滑动操作，而是：\n",
    "\n",
    "对每个通道，在所有空间位置上取一个最大值。\n",
    "\n",
    "它将每张特征图压缩为一个标量，从而大大降低特征维度。\n",
    "\n",
    "📦 各版本简介\n",
    "| 层                    | 输入维度                                      | 用途        | 输出形状                |\n",
    "| -------------------- | ----------------------------------------- | --------- | ------------------- |\n",
    "| `GlobalMaxPooling1D` | `(batch, steps, channels)`                | 时间序列      | `(batch, channels)` |\n",
    "| `GlobalMaxPooling2D` | `(batch, height, width, channels)`        | 图像        | `(batch, channels)` |\n",
    "| `GlobalMaxPooling3D` | `(batch, depth, height, width, channels)` | 体积图像 / 视频 | `(batch, channels)` |\n",
    "\n",
    "✅ 参数\n",
    "三个版本的接口都非常简洁：\n",
    "\n",
    "tf.keras.layers.GlobalMaxPooling2D(data_format=None)\n",
    "| 参数            | 说明                                        |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| `data_format` | `'channels_last'`（默认）或 `'channels_first'` |\n",
    "\n",
    "📌 常见用途\n",
    "\n",
    "用于卷积层后的特征压缩：\n",
    "\n",
    "将 4D/5D 的卷积输出转为 2D，接全连接层\n",
    "\n",
    "替代 Flatten（更少参数，更抗过拟合）\n",
    "\n",
    "适合时间序列、图像分类任务中的特征提取\n",
    "\n",
    "🆚 与 GlobalAveragePooling\n",
    "| 特征   | `GlobalMaxPooling` | `GlobalAveragePooling` |\n",
    "| ---- | ------------------ | ---------------------- |\n",
    "| 取值方式 | 每通道最大值             | 每通道平均值                 |\n",
    "| 强调   | 最强响应区域             | 整体趋势                   |\n",
    "| 适用场景 | 局部激活明显的任务（如目标检测）   | 平滑趋势建模（如情感分析）          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51f14c-0e9d-4d7f-aa33-5ec4547de32b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 3.4、tf.keras.layers.GlobalAveragePooling1D / 2D / 3D\n",
    "是 TensorFlow 中用于对输入数据进行 全局平均池化（Global Average Pooling） 的层，广泛用于特征图降维、减少参数量，并提升模型泛化能力。\n",
    "\n",
    "🧠 什么是 Global Average Pooling？\n",
    "\n",
    "Global Average Pooling 是指：\n",
    "\n",
    "对每个通道，取其所有空间（时间、空间或体积）位置的平均值。\n",
    "\n",
    "它比 Flatten 更高效且更不容易过拟合。\n",
    "\n",
    "🧾 三种版本说明\n",
    "| 层名                       | 输入维度                                      | 用途         | 输出形状                |\n",
    "| ------------------------ | ----------------------------------------- | ---------- | ------------------- |\n",
    "| `GlobalAveragePooling1D` | `(batch, steps, channels)`                | 时间序列 / NLP | `(batch, channels)` |\n",
    "| `GlobalAveragePooling2D` | `(batch, height, width, channels)`        | 图像处理       | `(batch, channels)` |\n",
    "| `GlobalAveragePooling3D` | `(batch, depth, height, width, channels)` | 医学图像 / 视频  | `(batch, channels)` |\n",
    "\n",
    "🔧 常用参数（几乎无需配置）\n",
    "\n",
    "tf.keras.layers.GlobalAveragePooling2D(data_format=None)\n",
    "| 参数            | 说明                                        |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| `data_format` | `'channels_last'`（默认）或 `'channels_first'` |\n",
    "\n",
    "✅ 优势与用途\n",
    "| 优点    | 说明                                 |\n",
    "| ----- | ---------------------------------- |\n",
    "| 减少参数量 | 不像 `Flatten` + `Dense`，不引入额外参数     |\n",
    "| 降维    | 把高维特征图转为 `(batch, channels)`       |\n",
    "| 抗过拟合  | 不容易学习到无关特征                         |\n",
    "| 应用广泛  | 常用于 CNN 分类模型末尾（如 MobileNet、ResNet） |\n",
    "\n",
    "🆚 与其他层对比\n",
    "| 层                      | 工作方式   | 特点         |\n",
    "| ---------------------- | ------ | ---------- |\n",
    "| `Flatten`              | 拉平成一维  | 参数多、信息量大   |\n",
    "| `GlobalMaxPooling`     | 每通道最大值 | 强调“最活跃”区域  |\n",
    "| `GlobalAveragePooling` | 每通道平均值 | 强调整体趋势，更稳定 |\n",
    "\n",
    "📌 小结\n",
    "\n",
    "GlobalAveragePooling 是替代 Flatten 的优秀方案。\n",
    "\n",
    "可直接作为 CNN 输出前的降维手段。\n",
    "\n",
    "广泛应用于轻量化网络和注意力机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6b37d-8165-4842-93b0-4fa467f4ee21",
   "metadata": {},
   "source": [
    "#### 4. 循环层（Recurrent Layers）\n",
    "4.1、SimpleRNN:\n",
    "基本循环神经网络。\n",
    "\n",
    "4.2、STM:\n",
    "长短时记忆网络，常用循环层。\n",
    "\n",
    "4.3、GRU:\n",
    "门控循环单元，LSTM的简化版"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b036169-92a0-461e-8fd4-16ef089042a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 4.2、tf.keras.layers.LSTM 是 Keras 提供的 长短期记忆网络（Long Short-Term Memory） 层，是 RNN 的一种改进版本。\n",
    "专门用于处理和学习时间序列数据或序列输入（如文本、语音、传感器数据等）。\n",
    "\n",
    "它能记住长期依赖关系，解决传统 RNN 容易遗忘旧信息的问题。\n",
    "\n",
    "tf.keras.layers.LSTM(\n",
    "\n",
    "    units,  --> 整数（必选），隐藏状态的维度大小，也就是 LSTM 输出向量的长度\n",
    "    \n",
    "    activation='tanh', --> 激活函数，默认（tanh）(-1,1)\n",
    "    \n",
    "    recurrent_activation='sigmoid', --> 循环门（遗忘门、输入门、输出门）用的激活函数，默认'sigmoid'（0，1）\n",
    "    \n",
    "    use_bias=True, --> 是否使用偏置项，默认 True\n",
    "\n",
    "    kernel_initializer='glorot_uniform', --> 输入权重的初始化方式\n",
    "    \n",
    "    recurrent_initializer='orthogonal', --> 循环权重的初始化方式。\n",
    "    \n",
    "    bias_initializer='zeros',--> 偏置的初始化方式\n",
    "    \n",
    "    unit_forget_bias=True, --> 是否给遗忘门偏置初始化为 1（默认为 True，有助于训练更快收敛）。\n",
    "    \n",
    "    dropout=0.0, --> 输入门的 dropout 比例，防止过拟合\n",
    "    \n",
    "    recurrent_dropout=0.0, --> 循环状态的 dropout 比例\n",
    "    \n",
    "    return_sequences=False, --> （默认）：只返回序列最后一个时间步的输出，形状为 (batch_size, units)，True：返回整个时间序列的输出，形状为 (batch_size, timesteps, units)\n",
    "    \n",
    "    return_state=False, -->  （默认）：只返回输出。True：返回输出以及隐藏状态和细胞状态三个张量，常用于构建编码器-解码器模型。\n",
    "    \n",
    "    go_backwards=False, --> 是否反向遍历输入序列，默认为 False。\n",
    "    \n",
    "    stateful=False, --> 是否保持状态，在批次间保持隐藏层状态，默认为 False，适合状态依赖的时间序列。\n",
    "    \n",
    "    unroll=False, --> 是否展开循环，默认 False，在序列很短且计算性能有要求时可设为 True。\n",
    "    \n",
    "    time_major=False, --> 输入数据格式是否是时间步为主轴，默认为 False（batch 轴为主）\n",
    "    \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "参数详解\n",
    "| 参数名                    | 说明                | 示例 / 默认值        |\n",
    "| ---------------------- | ----------------- | --------------- |\n",
    "| `units`                | 输出空间维度（隐藏状态大小）    | `128`           |\n",
    "| `activation`           | 主体激活函数            | `'tanh'`（默认）    |\n",
    "| `recurrent_activation` | 门控激活函数            | `'sigmoid'`（默认） |\n",
    "| `return_sequences`     | 是否返回所有时间步的输出      | `False`（默认）     |\n",
    "| `return_state`         | 是否返回最终状态 `(h, c)` | `False`（默认）     |\n",
    "| `dropout`              | 输入的 dropout 比例    | `0.0`           |\n",
    "| `recurrent_dropout`    | 循环状态的 dropout 比例  | `0.0`           |\n",
    "| `go_backwards`         | 是否反向处理序列          | `False`         |\n",
    "| `stateful`             | 是否在 batch 之间保留状态  | `False`         |\n",
    "| `unroll`               | 是否展开循环（提升速度，耗内存）  | `False`         |\n",
    "\n",
    "📤 输入 & 输出形状\n",
    "\n",
    "📥 输入形状：\n",
    "\n",
    "(batch_size, timesteps, features)\n",
    "\n",
    "--batch_size：每次送入多少个样本\n",
    "\n",
    "--timesteps：每个样本中时间步数（序列长度）\n",
    "\n",
    "--features：每个时间步的特征数\n",
    "\n",
    "📤 输出形状取决于 return_sequences：\n",
    "\n",
    "--return_sequences=False（默认）：\n",
    "\n",
    "(batch_size, units) -→ 仅返回最后一个时间步的输出（常用于分类、预测）\n",
    "\n",
    "--return_sequences=True：\n",
    "\n",
    "(batch_size, timesteps, units) -→ 返回每一个时间步的输出（适用于序列到序列）\n",
    "\n",
    "📤 若设置 return_state=True：\n",
    "\n",
    "返回：output, state_h, state_c\n",
    "\n",
    "--output：LSTM 输出（和 return_sequences 设置一致）\n",
    "\n",
    "--state_h：最后一个时间步的隐藏状态\n",
    "\n",
    "--state_c：最后一个时间步的细胞状态（cell state）\n",
    "\n",
    "📘 LSTM 内部结构简要（高级理解）\n",
    "\n",
    "LSTM 有三个门：\n",
    "\n",
    "1、遗忘门（Forget Gate）：决定当前时间步要遗忘多少之前的信息。\n",
    "\n",
    "2、输入门（Input Gate）：决定当前输入保留多少信息。\n",
    "\n",
    "3、输出门（Output Gate）：决定当前隐藏状态输出多少信息。\n",
    "\n",
    "每一时刻，LSTM 都会维护一个：\n",
    "\n",
    "隐藏状态 hₜ\n",
    "\n",
    "细胞状态 cₜ（信息记忆的关键）\n",
    "\n",
    "✅ 总结重点\n",
    "| 功能        | 参数                      | 说明              |\n",
    "| --------- | ----------------------- | --------------- |\n",
    "| 输出维度      | `units=128`             | 隐藏状态大小          |\n",
    "| 是否输出所有时间步 | `return_sequences=True` | 多对多输出（如翻译）      |\n",
    "| 是否输出状态    | `return_state=True`     | 获得 (h, c)       |\n",
    "| 过拟合控制     | `dropout=0.2`           | 输入 dropout      |\n",
    "| 状态保留      | `stateful=True`         | 训练中跨 batch 记忆状态 |\n",
    "\n",
    "❓什么时候用 LSTM\n",
    "\n",
    "文本情感分析（句子是序列）\n",
    "\n",
    "股票趋势预测（时间序列）\n",
    "\n",
    "语音识别（连续语音）\n",
    "\n",
    "翻译模型（序列到序列）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184c656-514a-44b0-8b8d-c6c3306a8acb",
   "metadata": {},
   "source": [
    "## 5. 嵌入层（Embedding Layer）\n",
    "Embedding:\n",
    "将离散整数映射为稠密向量，常用于自然语言处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6f187-6f45-48f5-9602-09ea813172dc",
   "metadata": {},
   "source": [
    "## 6. 归一化层（Normalization Layers）\n",
    "LayerNormalization:\n",
    "对单个样本的特征维度进行归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb0309-1ac4-4565-8ad6-4c77b0e37b5f",
   "metadata": {},
   "source": [
    "### 6.1、tf.keras.layers.LayerNormalization是一种**对每个样本独立进行归一化（标准化）**的层。\n",
    "\n",
    "1、✅ 它的主要作用是：\n",
    "\n",
    "对每一个样本的特征维度进行归一化，使其均值为 0，方差为 1，从而提高模型的训练稳定性。\n",
    "\n",
    "2、✅ 适用于：\n",
    "\n",
    "批量大小很小或变化不定的情况（例如 RNN、在线推理）\n",
    "\n",
    "Transformer、BERT、GPT 等结构中默认使用它\n",
    "\n",
    "序列建模任务：如 NLP、语音、时序预测等\n",
    "\n",
    "3、使用说明\n",
    "\n",
    "import tensorflow as tf  、\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    layers.Dense(64),\n",
    "    \n",
    "    layers.LayerNormalization(),  # 在每个样本的特征维度上做归一化\n",
    "    \n",
    "    layers.Activation('relu')\n",
    "])\n",
    "\n",
    "4、归一化维度说明\n",
    "\n",
    "假设输入张量的形状是 (batch_size, features)：\n",
    "\n",
    "BatchNormalization：对 batch 中某一特征维度进行归一化（跨样本）\n",
    "\n",
    "LayerNormalization：对每个样本内部的所有特征归一化（不跨 batch）\n",
    "\n",
    "5、参数详解\n",
    "\n",
    "tf.keras.layers.LayerNormalization(\n",
    "\n",
    "    axis=-1,\n",
    "    \n",
    "    epsilon=1e-5,\n",
    "    \n",
    "    center=True,\n",
    "    \n",
    "    scale=True,\n",
    "    \n",
    "    beta_initializer='zeros',\n",
    "    \n",
    "    gamma_initializer='ones',\n",
    "    \n",
    "    trainable=True,\n",
    "    \n",
    "    **kwargs\n",
    ")\n",
    "| 参数名                 | 含义                 |\n",
    "| ------------------- | ------------------ |\n",
    "| `axis`              | 指定归一化的维度（默认是最后一个轴） |\n",
    "| `epsilon`           | 防止除以 0 的小常数        |\n",
    "| `center`            | 是否添加可训练偏置 β        |\n",
    "| `scale`             | 是否添加可训练缩放 γ        |\n",
    "| `beta_initializer`  | β 初始化方法（默认为 0）     |\n",
    "| `gamma_initializer` | γ 初始化方法（默认为 1）     |\n",
    "\n",
    "6、和 BatchNormalization 的区别\n",
    "| 特性               | `BatchNormalization` | `LayerNormalization` |\n",
    "| ---------------- | -------------------- | -------------------- |\n",
    "| 归一化维度            | 跨样本（batch 维）         | 跨特征（样本自身）            |\n",
    "| 受 batch\\_size 影响 | 是                    | 否                    |\n",
    "| 适合任务             | CNN、Dense            | RNN、Transformer      |\n",
    "| 推理行为             | 使用移动均值/方差            | 训练和推理行为一致            |\n",
    "| 实现位置             | 多用于 ReLU 之前          | 多用于残差连接之后            |\n",
    "\n",
    "✅ 7、小结\n",
    "| 优点                  | 使用建议                   | 注意            |\n",
    "| ------------------- | ---------------------- | ------------- |\n",
    "| ✅ 不受 batch\\_size 影响 | 推荐用于 Transformer / RNN | ❗需注意轴方向       |\n",
    "| ✅ 稳定收敛、提升深层模型表现     | 可在残差连接后使用              | ❗相比 BN 少了动量机制 |\n",
    "| ✅ 推理训练一致性强          | 替代 BN 可用于更动态的任务        |               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4317230-a3e5-4037-aa7c-e7876b08373a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 6.2、tf.keras.layers.BatchNormalization 是Keras 中的批量归一化层，用于加速模型训练、提高稳定性、抑制过拟合。\n",
    "🧠 什么是 Batch Normalization？\n",
    "\n",
    "简单来说，它会在每一层的输出上做如下操作：\n",
    "\n",
    "对一个 mini-batch 中的所有样本的特征做 标准化（均值为 0，方差为 1），然后再引入可学习的缩放（γ）和平移（β）参数。\n",
    "\n",
    "这个过程可以缓解训练中的 梯度消失/爆炸问题，提高深层神经网络的训练效率和效果。\n",
    "\n",
    "📌 使用方式\n",
    "tf.keras.layers.BatchNormalization(\n",
    "\n",
    "    axis=-1,\n",
    "    \n",
    "    momentum=0.99,\n",
    "    \n",
    "    epsilon=1e-3,\n",
    "    \n",
    "    center=True,\n",
    "    \n",
    "    scale=True,\n",
    "    ...\n",
    ")\n",
    "📑 参数解释\n",
    "| 参数          | 说明                              |\n",
    "| ----------- | ------------------------------- |\n",
    "| `axis`      | 被归一化的维度。对图像而言，通常为最后一个通道维度（`-1`） |\n",
    "| `momentum`  | 用于计算移动平均（训练 vs 推理时的均值/方差）       |\n",
    "| `epsilon`   | 为避免除以 0 而加的小常数                  |\n",
    "| `center`    | 是否允许学习偏置项 β（平移）                 |\n",
    "| `scale`     | 是否允许学习缩放参数 γ                    |\n",
    "| `trainable` | 是否在训练时更新该层的统计量和参数               |\n",
    "\n",
    "📈 优点总结\n",
    "| 优点            | 描述              |\n",
    "| ------------- | --------------- |\n",
    "| ✅ 加速训练        | 梯度传播更稳定，允许更大学习率 |\n",
    "| ✅ 更深网络可训练     | 缓解梯度消失问题        |\n",
    "| ✅ 减少对权重初始化的依赖 | 更容易收敛           |\n",
    "| ✅ 有轻微正则化效果    | 降低过拟合风险         |\n",
    "| ✅ 训练效果更稳健     | 模型表现更鲁棒         |\n",
    "\n",
    "⚠️ 注意事项\n",
    "| 注意点                        | 说明                                                                   |\n",
    "| -------------------------- | -------------------------------------------------------------------- |\n",
    "| 推理时使用的是“滑动平均”而不是 batch 统计量 | 所以训练完要调用 `model.evaluate()` 或 `model.predict()` 时保持 `training=False` |\n",
    "| 不建议与 `Dropout` 同时使用在同一位置   | BatchNorm 本身已有轻度正则化作用                                                |\n",
    "| 使用 GPU 时更有效                | TF 在 GPU 上优化了 BN 层                                         |\n",
    "\n",
    "📘 扩展：与 LayerNormalization 区别\n",
    "\n",
    "| 项目    | BatchNormalization | LayerNormalization   |\n",
    "| ----- | ------------------ | -------------------- |\n",
    "| 归一化维度 | batch 维度           | 每个样本的特征维度            |\n",
    "| 适用场景  | CNN、图像任务           | RNN、Transformer、文本任务 |\n",
    "| 批大小敏感 | 是                  | 否                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bb07b-f214-4ef2-9018-c091ff561aca",
   "metadata": {},
   "source": [
    "## 7. 其他常用层\n",
    "7.1、InputLayer:\n",
    "输入层，定义输入形状。\n",
    "\n",
    "7.2、Concatenate:\n",
    "张量拼接层。\n",
    "\n",
    "7.3、Add / Multiply:\n",
    "张量加法/乘法层。\n",
    "\n",
    "7.4、RepeatVector:\n",
    "将向量复制多次，形成序列。\n",
    "\n",
    "7.5、TimeDistributed:\n",
    "将层应用到序列的每个时间步。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2f164-50fa-453a-acf0-7f8979087e3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 7.1、tf.keras.layers.InputLayer \n",
    "是 TensorFlow Keras 中的一个特殊层，用于显式定义模型的 输入张量形状，通常作为模型的第一层。\n",
    "\n",
    "🧠 简单理解：\n",
    "\n",
    "InputLayer 是一个“入口”，告诉模型输入数据的形状。\n",
    "\n",
    "通常你不会直接用它，而是使用 tf.keras.Input() 或 Sequential 的 input_shape 自动添加。\n",
    "\n",
    "但在 函数式 API 或自定义模型中，InputLayer 有助于明确定义结构。\n",
    "\n",
    "✅ 基本用法\n",
    "\n",
    "tf.keras.layers.InputLayer(\n",
    "\n",
    "    input_shape=None,\n",
    "    \n",
    "    batch_size=None,\n",
    "    \n",
    "    dtype=None,\n",
    "    \n",
    "    input_tensor=None,\n",
    "    \n",
    "    name=None,\n",
    "    \n",
    "    sparse=False,\n",
    "    \n",
    "    ragged=False\n",
    ")\n",
    "| 参数             | 说明                                    |\n",
    "| -------------- | ------------------------------------- |\n",
    "| `input_shape`  | 不包含 batch size 的输入形状，例如 `(28, 28, 1)` |\n",
    "| `batch_size`   | 可选，指定输入的 batch 大小                     |\n",
    "| `dtype`        | 数据类型（默认 float32）                      |\n",
    "| `input_tensor` | 可传入已有的张量                              |\n",
    "| `sparse`       | 是否为稀疏张量                               |\n",
    "| `ragged`       | 是否为 ragged 张量（不规则长度）                  |\n",
    "\n",
    "📌 小结\n",
    "| 特点   | 说明                                                   |\n",
    "| ---- | ---------------------------------------------------- |\n",
    "| 作用   | 显式指定模型输入的形状                                          |\n",
    "| 常用场景 | 自定义模型、函数式 API、自动转换                                   |\n",
    "| 自动添加 | 在 `Sequential` 中指定 `input_shape` 就会隐式创建 `InputLayer` |\n",
    "| 不可训练 | 它不包含权重和参数                                            |\n",
    "\n",
    "🆚 与 Input()\n",
    "| 比较项  | `InputLayer`     | `Input()`    |\n",
    "| ---- | ---------------- | ------------ |\n",
    "| 类型   | Keras 层（Layer）   | 返回张量（Tensor） |\n",
    "| 用途   | 明确结构定义           | 创建输入张量，接后续层  |\n",
    "| 使用位置 | Sequential / 自定义 | 函数式 API 中更常见 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b19034-cf5f-4b8f-bc95-d9f96a3d95a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 7.2、tf.keras.layers.Concatenate\n",
    "是 TensorFlow Keras 中用于将多个张量 按指定轴连接（拼接） 的层，常用于模型中多个分支的合并（如 U-Net、ResNet 等结构）。\n",
    "\n",
    "#### 🧠 简单理解：\n",
    "Concatenate 不是叠加、也不是加法，而是 沿着某个轴把多个张量拼接在一起。 \\\n",
    "它的作用类似于 NumPy 中的 np.concatenate()。\n",
    "\n",
    "#### ✅ 常用构造方式：\n",
    "tf.keras.layers.Concatenate(axis=-1)\n",
    "| 参数         | 说明                            |\n",
    "| ---------- | ----------------------------- |\n",
    "| `axis`     | 指定拼接的轴，默认是最后一个维度（`-1`，通常是通道维） |\n",
    "| `**kwargs` | 其他层参数，如 `name` 等              |\n",
    "\n",
    "#### 🧱 与其他合并层对比\n",
    "| 层                               | 功能        | 是否学习参数 | 示例                |\n",
    "| ------------------------------- | --------- | ------ | ----------------- |\n",
    "| `Concatenate`                   | 拼接张量      | ❌      | `[a, b] → [a, b]` |\n",
    "| `Add` / `Subtract` / `Multiply` | 元素级相加/减/乘 | ❌      | `a + b`           |\n",
    "| `Dot`                           | 点积/相似度    | ❌      | `Dot(axes=-1)`    |\n",
    "| `Average`                       | 取平均值      | ❌      | `Mean([a, b])`    |\n",
    "\n",
    "#### ❗ 注意事项：\n",
    "拼接维度必须一致，除了拼接轴以外的维度必须相同  \\\n",
    "可以用于 Model(inputs=[...], outputs=...) 组合多个输入分支  \\\n",
    "用于构建跳跃连接、融合多层信息非常常见（如 U-Net, Inception）\n",
    "#### 🧩 小结：\n",
    "| 属性    | 说明                             |\n",
    "| ----- | ------------------------------ |\n",
    "| 类别    | 层（Layer）                       |\n",
    "| 功能    | 沿某一维拼接多个张量                     |\n",
    "| 常用场景  | 多输入模型、跳跃连接、Inception/U-Net 等结构 |\n",
    "| 是否可训练 | 否，无可学习参数                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09a5be-14a0-42d9-b19c-9dba95ebbf58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 7.3、tf.keras.layers.Add / Multiply\n",
    " 是 TensorFlow Keras 中用于将多个张量按元素级相加或相乘的操作层，属于“合并层（Merge Layers）”。\n",
    "\n",
    "🧠 简单理解\n",
    "\n",
    "Add：逐元素加法（output[i] = input1[i] + input2[i]）\n",
    "\n",
    "Multiply：逐元素乘法（output[i] = input1[i] * input2[i]）\n",
    "\n",
    "注意：它们不是拼接（Concatenate），也不是矩阵乘法（Dot），而是 按位运算（element-wise）。\n",
    "\n",
    "✅ 常用形式\n",
    "\n",
    "tf.keras.layers.Add()([tensor1, tensor2, ...])\n",
    "\n",
    "tf.keras.layers.Multiply()([tensor1, tensor2, ...])\n",
    "\n",
    "所有输入张量必须形状完全相同。\n",
    "\n",
    "🔧 应用场景\n",
    "| 场景           | Add | Multiply      |\n",
    "| ------------ | --- | ------------- |\n",
    "| 残差连接（ResNet） | ✅   | -             |\n",
    "| 注意力机制        | -   | ✅（注意力权重 × 特征） |\n",
    "| 多分支信息融合      | ✅   | ✅             |\n",
    "| 模型融合（相加输出）   | ✅   | ✅             |\n",
    "\n",
    "🧱 与其他合并层对比\n",
    "| 层类型             | 操作       | 是否需要形状一致   | 是否改变维度 | 可学习参数 |\n",
    "| --------------- | -------- | ---------- | ------ | ----- |\n",
    "| `Add()`         | 元素加法     | ✅ 完全一致     | ❌      | ❌     |\n",
    "| `Multiply()`    | 元素乘法     | ✅ 完全一致     | ❌      | ❌     |\n",
    "| `Concatenate()` | 沿轴拼接     | ❌ 只要求拼接轴不同 | ✅      | ❌     |\n",
    "| `Dot()`         | 张量乘积（点积） | ✅ 有匹配轴     | ✅      | ❌     |\n",
    "\n",
    "📌 小结\n",
    "| 层          | 功能   | 输出形状  | 是否可训练 |\n",
    "| ---------- | ---- | ----- | ----- |\n",
    "| `Add`      | 元素加法 | 与输入相同 | 否     |\n",
    "| `Multiply` | 元素乘法 | 与输入相同 | 否     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006a0f1-b023-4d76-ab26-5d2c464fc379",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 7.4、tf.keras.layers.RepeatVector\n",
    "是 TensorFlow Keras 中用于将一个二维张量（(batch_size, features)）重复复制 N 次，扩展为三维张量（(batch_size, n, features)）的层。\n",
    "\n",
    "🧠 简单理解\n",
    "\n",
    "把一个向量按时间步（序列长度）方向复制多次，常用于从全连接层输出扩展为 RNN 输入。\n",
    "\n",
    "✅ 常用语法：\n",
    "tf.keras.layers.RepeatVector(n)\n",
    "| 参数  | 说明                      |\n",
    "| --- | ----------------------- |\n",
    "| `n` | 要重复的次数，通常表示“时间步”或“序列长度” |\n",
    "\n",
    "📌 应用场景\n",
    "| 应用          | 是否常见 | 说明               |\n",
    "| ----------- | ---- | ---------------- |\n",
    "| 编码器-解码器结构   | ✅    | 把编码向量变成序列输入给解码器  |\n",
    "| 条件语言模型      | ✅    | 把上下文向量重复提供给每个时间步 |\n",
    "| 多标签分类（变形用法） | ⚠️   | 不常见但可用于广播标签对比    |\n",
    "\n",
    "📌 小结\n",
    "| 属性    | 值                           |\n",
    "| ----- | --------------------------- |\n",
    "| 类型    | 层（Layer）                    |\n",
    "| 功能    | 沿时间步方向复制向量                  |\n",
    "| 输入形状  | `(batch_size, features)`    |\n",
    "| 输出形状  | `(batch_size, n, features)` |\n",
    "| 是否可训练 | ❌（无可学习参数）                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89521f-0bc1-4ec4-85df-a1a5256310c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 7.5、tf.keras.layers.TimeDistributed\n",
    "是 Keras 中非常实用的一种“包装层”，用于将某个层（如 Dense, Conv2D, 等）应用在每个时间步（或帧）上，逐时间步独立处理输入序列的每一帧。\n",
    "\n",
    "🧠 核心作用\n",
    "\n",
    "TimeDistributed(layer) = 将 layer 重复作用在每个时间步上，保持每个时间步独立计算。\n",
    "\n",
    "适用于 输入是 3D 或 4D（序列）张量，例如 RNN、视频帧、时间序列处理等场景。\n",
    "\n",
    "✅ 使用语法\n",
    "\n",
    "tf.keras.layers.TimeDistributed(\n",
    "\n",
    "    layer,      # 被包装的层\n",
    "    \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "📐 输入输出形状说明\n",
    "\n",
    "假设你有一个输入：\n",
    "\n",
    "(batch_size, time_steps, input_dim)\n",
    "\n",
    "你将 Dense(64) 用 TimeDistributed 包装后，每个时间步会独立地过一遍 Dense 层，输出为：\n",
    "\n",
    "(batch_size, time_steps, 64)\n",
    "\n",
    "🆚 与不使用 TimeDistributed 的区别\n",
    "\n",
    "若你不加 TimeDistributed，例如直接写：\n",
    "\n",
    "Dense(64)(inputs)\n",
    "\n",
    "此时 Keras 会认为你要对 整个序列（整个张量）做一次 Dense 运算，即把 (batch_size, time_steps, input_dim) 当成一个整体输入，不再逐时间步处理。\n",
    "\n",
    "但加上：\n",
    "\n",
    "TimeDistributed(Dense(64))(inputs)\n",
    "\n",
    "Keras 会按 time_steps 维度 逐帧应用 Dense，处理效果为：对每个时间步 单独地、重复地 使用同一个 Dense 层。\n",
    "\n",
    "📌 应用场景\n",
    "| 场景          | 说明                   |\n",
    "| ----------- | -------------------- |\n",
    "| RNN 输出后接全连接 | 对每个时间步输出做独立分类        |\n",
    "| 视频帧处理       | 对每一帧图像用 CNN 处理后再合成序列 |\n",
    "| 多帧时间序列预测    | 对每一步输出做逐帧预测          |\n",
    "\n",
    "📦 小结\n",
    "| 特性     | 说明                                        |\n",
    "| ------ | ----------------------------------------- |\n",
    "| 是否可训练  | ✅（取决于被包装层）                                |\n",
    "| 是否共享权重 | ✅（每个时间步共享同一个子层）                           |\n",
    "| 输入维度   | ≥3D（最常见是 `(batch, time, ...)`）            |\n",
    "| 典型包装层  | `Dense`, `Conv2D`, `Flatten`, `Dropout` 等 |\n",
    "\n",
    "✅ 类似的还有：\n",
    "\n",
    "RepeatVector(n)：复制成 n 个时间步的序列\n",
    "\n",
    "RNN、LSTM：自身处理时间序列，但 TimeDistributed 用于辅助非序列层处理序列结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71156d56-226d-411e-bca6-5fa196e123df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398f691-c165-4fdd-b5a2-e2aa765a350a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73878339-63be-4671-aec6-0883b9568364",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6dfe0-add0-4968-af62-34ea169a4bf0",
   "metadata": {},
   "source": [
    "# tf.keras.layers.StringLookup \n",
    "核心作用是将一组字符串（或分类特征）映射为整数索引。它是构建处理文本或分类数据的神经网络模型时，一个非常基础和重要的预处理层。\n",
    "\n",
    "你可以把它想象成一个高效的“字典”或“查找表”：\n",
    "\n",
    "- 输入：一个字符串（例如，一个单词、一个类别名）。\n",
    "\n",
    "- 输出：该字符串在预定义“词汇表”中对应的唯一整数值（索引）。\n",
    "\n",
    "\n",
    "## 为什么需要它？\n",
    "神经网络（包括深度学习模型）的底层数学运算处理的是数值张量（Tensor），它们无法直接处理字符串。因此，我们必须将字符串转换为数字形式，这个过程称为编码。\n",
    "\n",
    "`StringLookup` 就是一种编码层，它提供了一种标准化的、可嵌入到 Keras 模型中的方式来完成这个转换。\n",
    "\n",
    "---\n",
    "### 主要功能特性\n",
    "- ✅词汇表构建：\n",
    "\n",
    "   - 你可以通过 `vocabulary` 参数直接传入一个字符串列表来指定词汇表。\n",
    "\n",
    "   - 你也可以让层自动从训练数据中学习词汇表（通过 `adapt()` 方法）。这是最常见的使用方式。\n",
    "\n",
    "例如，layer.adapt([“dog”, “cat”, “bird”, “dog”, “cat”]) 会学习词汇表 [“dog”, “cat”, “bird”]。\n",
    "\n",
    "- ✅处理未知词汇：\n",
    "\n",
    "   - 在现实数据中，验证集或测试集很可能出现训练时没见过的词汇（`Out-Of-Vocabulary`, `OOV`）。\n",
    "\n",
    "   - `oov_token` 参数至关重要。你可以指定一个字符串（如 “[UNK]”）来代表所有未知词汇。\n",
    "\n",
    "启用 `oov_token` 后，该 `token` 会成为词汇表的第一个元素（索引为 1 或 0，取决于 `mask_token`），所有未知字符串都会被映射到这个索引。\n",
    "\n",
    "- ✅掩码（Masking）支持：\n",
    "\n",
    "   - 在处理序列数据（如文本）时，经常需要处理不同长度的序列，通常会用 0 来填充（pad）短序列以达到统一长度。\n",
    "\n",
    "   - 通过设置 `mask_token`= 参数，你可以指定一个字符串（如 “” 空字符串）来代表这些填充值。\n",
    "\n",
    "该层会为这个 `mask_token` 分配索引 0。在模型后续的层（如 `Embedding` 层、`RNN`层）中，可以识别这个索引 0 并自动忽略（掩码）它，从而不影响计算。\n",
    "\n",
    "- ✅编码模式：\n",
    "\n",
    "   - 默认模式 (`output_mode=“int”`): 将每个字符串直接转换为其对应的整数索引。这是最常用的模式。\n",
    "\n",
    "   - 多热编码 (`output_mode=“multi_hot”`): 如果输入是一个字符串集合（例如 [“cat”, “dog”]），它可以输出一个多热编码向量（例如 [0, 1, 1, 0]），表示哪些类别存在。\n",
    "\n",
    "- ✅词频模式 (`output_mode=“count”`): 类似多热编码，但向量的每个元素是相应类别在输入中出现的次数。\n",
    "\n",
    "- ✅TF-IDF 模式 (`output_mode=“tf_idf”`): 输出 `TF-IDF` 加权向量，需要额外提供 `idf_weights` 参数。\n",
    "\n",
    "---\n",
    "## 工作流程示例\n",
    "一个典型的使用 `StringLookup` 层的工作流程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d951b3-13ee-49d7-adbe-e2831897741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 示例数据\n",
    "data = [[\"cat\", \"dog\"], [\"dog\", \"bird\"], [\"cat\", \"unknown_animal\"]]\n",
    "\n",
    "# 2. 创建层实例\n",
    "# 指定处理未知词汇和掩码词汇\n",
    "string_lookup = tf.keras.layers.StringLookup(\n",
    "    vocabulary=None,        # 我们让层自己从数据学习\n",
    "    oov_token=\"[UNK]\",      # 定义未知词汇的标记\n",
    "    mask_token=\"\",          # 定义填充用的掩码标记（空字符串）\n",
    "    output_mode=\"int\"       # 输出整数索引\n",
    ")\n",
    "\n",
    "# 3. 使层适应（学习）训练数据，构建词汇表\n",
    "# adapt 方法会统计所有数据，生成词汇表\n",
    "string_lookup.adapt(data)\n",
    "\n",
    "# 4. 查看词汇表\n",
    "print(\"Vocabulary:\", string_lookup.get_vocabulary())\n",
    "# 输出可能类似于：['', '[UNK]', 'cat', 'dog', 'bird']\n",
    "# 索引: 0 -> ‘’ (mask), 1 -> ‘[UNK]’ (oov), 2 -> ‘cat’, 3 -> ‘dog’, 4 -> ‘bird’\n",
    "\n",
    "# 5. 使用层进行转换\n",
    "test_data = [[\"cat\", \"dog\", \"bird\"], [\"lion\", \"cat\"]] # “lion”不在词汇表中\n",
    "encoded_data = string_lookup(test_data)\n",
    "print(\"Encoded data:\", encoded_data.numpy())\n",
    "# 输出: [[2 3 4]  [1 2]] \n",
    "# “lion”被映射到了索引 1 ([UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6ac60-f282-4dbe-adf6-2910b3c03487",
   "metadata": {},
   "source": [
    "## 常见应用场景\n",
    "1).文本处理：作为 NLP 模型的第一层，将单词转换为索引，然后接一个 Embedding 层将索引转换为密集向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bafcc-71c2-4247-888a-0493248c730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=\"\", output_mode=\"int\"),\n",
    "    tf.keras.layers.Embedding(input_dim=len(vocab)+2, output_dim=64), # +2 是因为有mask和oov\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    ...\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c71df-f644-4c17-bc5a-73c37eec6116",
   "metadata": {},
   "source": [
    "2).结构化数据中的分类特征：处理数据集中的字符串分类列（如“性别”：[\"男\", “女\"]），将其转换为整数后，可以直接输入给模型，或通过 Embedding 层处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd5234-8032-4c67-90d6-bd53f0a9c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设有一个名为 ‘gender’ 的特征列\n",
    "gender_lookup = tf.keras.layers.StringLookup()\n",
    "gender_lookup.adapt(train_df[‘gender’]) # 从训练集学习所有性别类别\n",
    "encoded_gender = gender_lookup(inputs[“gender”]) # 在模型中使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5306b78-8435-42a1-8d3d-81f97f51e196",
   "metadata": {},
   "source": [
    "# ✅总结\n",
    "| 特性 | 说明 |\n",
    "|------|------|\n",
    "| **核心功能** | 将字符串映射为整数索引，为神经网络准备数据 |\n",
    "| **关键参数** | `vocabulary`（词汇表），`oov_token`（处理未知词），`mask_token`（处理填充） |\n",
    "| **学习方式** | 可通过 `adapt()` 方法从数据中自动学习，或直接传入 |\n",
    "| **输出模式** | `\"int\"`（索引），`\"multi_hot\"`（多热编码），`\"count\"`（计数），`\"tf_idf\"` |\n",
    "| **下游应用** | 通常后面紧跟 Embedding 层或直接输入给全连接层 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2604145-0993-4fe1-a2e4-07d3a7a69144",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd03c2-f0c0-474b-81d2-88dbe0af13f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abb3f21-619f-45b5-b330-74f9dd79e794",
   "metadata": {},
   "source": [
    "# tf.keras.layers.Embedding\n",
    "它是处理离散数据（如文本、分类标签）的基石，理解了它就打开了深度学习 NLP 和推荐系统的大门。\n",
    "\n",
    "## 一、核心思想与作用：为什么要用 Embedding？\n",
    "在深入参数之前，必须理解它解决了什么问题。\n",
    "\n",
    "---\n",
    "### 1. 问题的起源：One-Hot 编码的缺陷\n",
    "在传统机器学习中，我们常用 One-Hot 编码来处理类别数据。\n",
    "\n",
    "  - \"猫\" -> [1, 0, 0, 0]\n",
    "\n",
    "  - \"狗\" -> [0, 1, 0, 0]\n",
    "\n",
    "  - \"鸟\" -> [0, 0, 1, 0]\n",
    "\n",
    "- 缺陷非常明显：\n",
    "\n",
    "    - 高维稀疏：词汇表很大时，向量维度会极高，且大部分元素为 0，计算和存储效率低下。\n",
    "\n",
    "    - 语义缺失：每个词向量彼此正交，内积为 0。这意味着模型无法从编码本身得知“猫”和“狗”都是动物，比“鸟”和“汽车”更相似。模型无法从数据中学习词语之间的关系。\n",
    " \n",
    "---\n",
    "\n",
    "### 2. Embedding 层的解决方案：密集词向量\n",
    "`Embedding` 层的核心思想是：\n",
    "- 学习一个低维、密集的向量表示，其中向量空间中的几何关系（如距离、方向）能够反映词语之间的语义关系。\n",
    "\n",
    "    - 输入：一个整数索引（比如代表“猫”的 2）。\n",
    "\n",
    "    - 输出：一个密集的浮点数向量（比如 [0.2, -0.5, 1.8, 0.4]，假设 embedding_dim=4）。\n",
    "\n",
    "- 关键优势：\n",
    "\n",
    "   - 降维：将千维、万维的 `One-Hot` 向量压缩到几十、几百维。\n",
    "\n",
    "   - 语义编码：语义相近的词（如“国王”和“王后”），它们的向量在空间中的距离会很近。甚至能捕捉到复杂的语义关系，如 vec(“国王”) - vec(“男人”) + vec(“女人”) ≈ vec(“王后”)。\n",
    "\n",
    "   - 可学习：这些向量是模型的参数，会在训练过程中通过反向传播不断更新，越来越能表征词语在特定任务下的含义。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36475ee0-d74f-4f95-bdcd-814a833c0a5f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4317b16-fce4-4ee9-bda6-39da32f931a7",
   "metadata": {},
   "source": [
    "## 二、工作原理：它到底是什么？\n",
    "`Embedding` 层本质上是一个可训练的查询表。\n",
    "\n",
    "- 初始化：在训练开始时，它会随机初始化一个大小为 (`vocabulary_size`, `embedding_dim`) 的矩阵。\n",
    "\n",
    "这个矩阵有 `vocabulary_size` 行，`embedding_dim` 列。\n",
    "\n",
    "每一行代表一个词汇的嵌入向量。\n",
    "\n",
    "- 前向传播（查询过程）：\n",
    "\n",
    "   - 输入：一个包含整数索引的张量，形状为 (`batch_size`, `sequence_length`)。\n",
    "\n",
    "   - 过程：对于输入中的每一个索引 `i`，层就去这个矩阵的第 `i` 行，把这一行的向量拿出来。\n",
    "\n",
    "   - 输出：一个新的张量，形状为 (batch_size, sequence_length, embedding_dim)。每个索引都被“替换”为了对应的 embedding_dim 维向量。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335990a1-dd9f-4417-965e-8f961d448637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设词汇表： [\"<pad>\", \"[UNK]\", \"猫\", \"狗\"] -> 索引: 0, 1, 2, 3\n",
    "# Embedding 矩阵 (假设 embedding_dim=4):\n",
    "# 行0 (索引0): [0.0, 0.0, 0.0, 0.0]  # 通常填充符的向量会趋于0\n",
    "# 行1 (索引1): [0.1, 0.2, 0.3, 0.4]  # 未知词向量\n",
    "# 行2 (索引2): [0.5, 0.6, -0.7, 0.8] # “猫”的向量\n",
    "# 行3 (索引3): [0.9, 1.0, -1.1, 1.2] # “狗”的向量\n",
    "\n",
    "# 输入: [[2, 3], [3, 1]]  # 批次大小=2，序列长度=2。内容是 [[\"猫\", \"狗\"], [\"狗\", \"未知词\"]]\n",
    "# 输出 (形状: 2, 2, 4):\n",
    "[\n",
    "  [[0.5, 0.6, -0.7, 0.8], [0.9, 1.0, -1.1, 1.2]],  # 第一个样本：猫 -> 第2行，狗 -> 第3行\n",
    "  [[0.9, 1.0, -1.1, 1.2], [0.1, 0.2, 0.3, 0.4]]   # 第二个样本：狗 -> 第3行，未知词 -> 第1行\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25230a-078a-493e-8b74-ba8e224af794",
   "metadata": {},
   "source": [
    "## 三、用法与参数详解\n",
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55795f7-3c90-4a64-9f20-8ef8d56fc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Embedding(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=False,\n",
    "    input_length=None,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2c39b-a84d-4991-b59a-bd03c77db13c",
   "metadata": {},
   "source": [
    "### 最关键的两个参数：\n",
    "\n",
    "- 1).`input_dim`：整数，词汇表的大小。即：最大整数索引 + 1。例如，如果你的 `StringLookup` 层输出的索引是 0, 1, 2, ..., 999，那么 input_dim 应该是 1000。\n",
    "\n",
    "- 2).`output_dim`：整数，密集嵌入的维度。即每个索引会被转换成多少维的向量。这是一个超参数，常见值在 50 到 300 之间，取决于任务和数据集大小。\n",
    "\n",
    "### 其他重要参数：\n",
    "\n",
    "- 3).`embeddings_initializer`：嵌入矩阵的初始化器（默认为 '`uniform`' 随机均匀分布）。你可以在这里使用预训练的词向量（如 `GloVe`、`Word2Vec`）来初始化，这通常能显著提升模型效果。\n",
    "\n",
    "- 4).`mask_zero`：布尔值。是否将索引 0 视为需要被“掩盖”的特殊值（通常是填充符 `<pad>`）。`StringLookup` 层通常将 `mask_token` 映射为索引 0。将其设置为 `True` 后，模型后续的层（如 `RNN`, `Transformer`）会自动跳过这些位置，不参与计算。非常重要！\n",
    "\n",
    "- 5).`input_length`：整数。输入序列的固定长度。当你后面接 `Flatten` 和 `Dense` 层时，需要指定这个参数以便层能计算出输出的固定大小。如果后面是 RNN 或全局池化层，则通常不需要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8884734-ecc6-434a-9926-2e2b7bbf34c9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507a270-86a2-49d1-b595-d09162b211d7",
   "metadata": {},
   "source": [
    "## 四、完整工作流程示例\n",
    "这是一个标准的文本分类流程，展示了 StringLookup 和 Embedding 如何协同工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed73b00-d120-42f3-bf0e-dc9387a07f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 示例文本数据\n",
    "texts = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"Do you think my dog is amazing?\",\n",
    "    \"My cat is very cute\"\n",
    "]\n",
    "labels = [1, 1, 1, 0] # 1 for positive, 0 for negative\n",
    "\n",
    "# 2. 文本标准化和分词\n",
    "# 将一个字符串或字符串批次（Batch）映射到整数 token 的序列或密集（Dense）数值表示（如 TF-IDF），从而使其可以被神经网络处理。\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=10,       # 只保留前10个最频繁的词\n",
    "    output_mode='int',   # 输出整数索引\n",
    "    output_sequence_length=5 # 统一序列长度为5（不足填充，过长截断）\n",
    ")\n",
    "vectorize_layer.adapt(texts) # 学习词汇表\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(\"Vocabulary:\", vocab)\n",
    "# 输出可能: ['', '[UNK]', 'my', 'love', 'dog', 'cat', 'i', 'you', ...]\n",
    "# 索引: 0->'', 1->'[UNK]', 2->'my', 3->'love', 4->'dog', 5->'cat'\n",
    "\n",
    "# 3. 构建模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string), # 输入原始字符串\n",
    "    vectorize_layer,                             # 文本 -> 整数索引\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(vocab),    # 词汇表大小\n",
    "        output_dim=64,           # 嵌入维度，超参数\n",
    "        mask_zero=True           # 启用掩码，忽略索引0（填充符）\n",
    "    ), # 输出形状: (None, 5, 64)\n",
    "    # 接下来可以接各种层，例如：\n",
    "    tf.keras.layers.LSTM(128),   # LSTM层会自动处理掩码\n",
    "    # tf.keras.layers.GlobalAveragePooling1D(), # 或者简单池化\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # 二分类输出层\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 4. 训练模型\n",
    "# 注意：输入是原始字符串数组，模型内部会自动完成索引化和嵌入\n",
    "history = model.fit(texts, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d41a3d-e904-49ab-b0e3-58125bb9be56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 五、总结与要点\n",
    "| 方面 | 说明 |\n",
    "|------|------|\n",
    "| **核心作用** | 将离散的整数索引映射为密集的、可学习的低维向量，捕获语义关系。 |\n",
    "| **输入** | 形状为 `(batch_size, sequence_length)` 的整数张量。 |\n",
    "| **输出** | 形状为 `(batch_size, sequence_length, output_dim)` 的浮点数张量。 |\n",
    "| **关键参数** | `input_dim`（词汇表大小），`output_dim`（嵌入维度），`mask_zero`（是否掩码填充）。 |\n",
    "| **训练** | 嵌入矩阵是模型参数，通过训练数据学习得到。 |\n",
    "| **进阶用法** | 使用预训练词向量初始化 (`embeddings_initializer`)，对嵌入向量施加正则化 (`embeddings_regularizer`)。 |\n",
    " \n",
    "### 一句话概括：\n",
    "`Embedding` 层是一个为模型学习“词语含义”的可训练字典**，它将简单的数字索引转换成了富含语义信息的数学表示，是现代深度学习处理离散数据的核心技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a8a35-7cf4-422e-a22e-c36994355f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
