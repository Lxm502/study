{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9487824f-3086-487e-a98c-95d8ede7289f",
   "metadata": {},
   "source": [
    "# 🧭torch.autograd\n",
    "它是 PyTorch 的 自动微分核心模块。理解了 autograd，基本就能掌握 PyTorch 的“自动梯度”原理。\n",
    "\n",
    "## 1. 概念\n",
    "\n",
    "autograd = 自动求导引擎\n",
    "\n",
    "本质：在 前向计算 时，PyTorch 会 动态构建一张计算图 (DAG)，记录张量之间的运算关系；在 反向传播 时，autograd 会从输出往输入回溯，利用 链式法则 自动计算梯度。\n",
    "\n",
    "适用于 标量 loss 对 模型参数 的梯度计算，是训练神经网络的关键。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. autograd 的核心属性\n",
    "### 👉(1) requires_grad\n",
    "\n",
    "- `控制一个张量是否需要梯度。`\n",
    "\n",
    "- `默认 False，需要手动打开：x = torch.ones(3, requires_grad=True)`\n",
    "\n",
    "- `只有 浮点 / 复数 张量能设置 requires_grad=True。`\n",
    "\n",
    "### 👉(2) .grad\n",
    "\n",
    "- `保存 梯度结果（即 ∂loss/∂x）。`\n",
    "\n",
    "- `只对 叶子节点（leaf tensors） 有意义：`\n",
    "\n",
    "- 1. `叶子节点 = 直接由用户创建的、requires_grad=True 的张量。`\n",
    "\n",
    "- 2. `中间节点的 .grad 默认不会存储（节省内存）。`\n",
    "\n",
    "### 👉(3) .grad_fn\n",
    "\n",
    "- `每个 非叶子张量 都有一个 .grad_fn 属性，记录它是由哪种函数（操作）创建的。`\n",
    "\n",
    "例如：\n",
    "x = torch.ones(2, requires_grad=True)  \\\n",
    "y = x + 2  \\\n",
    "print(y.grad_fn)   \n",
    "\n",
    "### 👉(4) 计算图 (Dynamic Computation Graph)\n",
    "\n",
    "- `动态图：每次前向传播时都会重新构建，不像 TensorFlow 1.x 那样要静态编图。`\n",
    "\n",
    "- `在 backward() 时，这个图会被释放（除非设置 retain_graph=True）。`\n",
    "\n",
    "---\n",
    "\n",
    "## 3. autograd 的核心接口\n",
    "| 函数                                                                                           | 作用                                 |\n",
    "| -------------------------------------------------------------------------------------------- | ---------------------------------- |\n",
    "| `torch.autograd.backward(tensors, grad_tensors, ...)`                                        | 核心反向传播接口（大部分时候通过 `.backward()` 调用） |\n",
    "| `tensor.backward(grad=None, ...)`                                                            | 常用的简化接口                            |\n",
    "| `torch.autograd.grad(outputs, inputs, grad_outputs, create_graph=False, retain_graph=False)` | 直接返回梯度，而不是累加到 `.grad`，常用于高阶梯度      |\n",
    "| `torch.autograd.set_detect_anomaly(True)`                                                    | 开启异常检测，定位梯度爆炸/NaN 的来源              |\n",
    "| `torch.no_grad()`                                                                            | 上下文管理器，临时禁用梯度计算（推理模式）              |\n",
    "| `torch.enable_grad()`                                                                        | 与 `no_grad` 对应，重新启用梯度              |\n",
    "| `torch.inference_mode()`                                                                     | 更高效的 **推理模式**，彻底禁用 autograd 记录     |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 运行机制（例子）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53e424-23ef-479e-b84c-1126dbdaf66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2 + 3 * x\n",
    "\n",
    "# 反向传播\n",
    "y.backward()\n",
    "print(x.grad)   # dy/dx = 2*x + 3 = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9102943c-f262-4980-90a1-e7b54dbbed55",
   "metadata": {},
   "source": [
    "### 流程：\n",
    "\n",
    "1、前向：构建图 y = x^2 + 3x，保存操作记录。\n",
    "\n",
    "2、调用 y.backward()：反向遍历计算图。\n",
    "\n",
    "3、根据链式法则求导：dy/dx = 2x + 3 = 7\n",
    "\n",
    "4、结果存到 x.grad。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8aaf35-25d4-47ab-a43b-c66cf8b28f62",
   "metadata": {},
   "source": [
    "## 5. 高阶用法\n",
    "### (1) 高阶梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbad7fe-dc22-4856-a2b5-6c8306e4872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "\n",
    "# 一阶导数\n",
    "grad1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "\n",
    "# 二阶导数\n",
    "grad2 = torch.autograd.grad(grad1, x)[0]\n",
    "print(grad1, grad2)   # 12, 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b4727-a5b9-42b0-8fbd-82f061f88f3a",
   "metadata": {},
   "source": [
    "### (2) 禁用梯度（推理/评估时）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06808066-825f-407d-9a68-0e28f8e82a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = model(x)  # 不会追踪梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827e11e-6954-4031-bcfe-cc03cac18720",
   "metadata": {},
   "source": [
    "### (3) 推理模式（PyTorch 1.9+）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea51e5-b02f-4a8d-83b0-dc76cce0459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    y = model(x)  # 比 no_grad 更快更省内存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e13fc-1e84-41f1-b07a-b51487305a1e",
   "metadata": {},
   "source": [
    "### (4) 只对部分输入求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098dfd8-7a6d-4a71-9056-838ec6413b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = a * b\n",
    "\n",
    "grad_a = torch.autograd.grad(c, a)[0]  # ∂c/∂a = b\n",
    "print(grad_a)  # 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cf328-60a4-488f-887b-0da46bc75541",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 常见坑 ⚠️\n",
    "1.梯度会累加\\\n",
    "每次 backward() 会把结果累加到 .grad。训练循环里要 optimizer.zero_grad()。\n",
    "\n",
    "2.非标量输出必须传梯度 \\\n",
    "y = model(x)   # y 不是标量 \\\n",
    "y.backward()   # ❌ 报错\\\n",
    "y.backward(torch.ones_like(y))   # ✅\n",
    "\n",
    "\n",
    "3.in-place 操作可能破坏计算图\\\n",
    "对 requires_grad=True 的张量做原地修改可能导致梯度错误。\\\n",
    "❌ x += 1\\\n",
    "✅ x = x + 1\n",
    "\n",
    "4.整数 / 布尔不能 requires_grad\\\n",
    "只有浮点 / 复数才行。\n",
    "\n",
    "5.中间结果默认不保存 .grad\\\n",
    "如果想要中间节点的梯度，要用 torch.autograd.grad()。\n",
    "\n",
    "---\n",
    "\n",
    "## 7. 学习速查表\n",
    "# torch.autograd 速查表\n",
    "---\n",
    "## 核心属性\n",
    "- `requires_grad` : 是否追踪梯度\n",
    "- `.grad`         : 存放梯度结果（仅叶子张量）\n",
    "- `.grad_fn`      : 创建该张量的 Function\n",
    "- 计算图 (DAG)   : 前向时动态构建，反向传播时释放\n",
    "---\n",
    "## 常用接口\n",
    "| 函数 | 功能 |\n",
    "|------|------|\n",
    "| `tensor.backward()` | 反向传播（标量输出可省略 grad） |\n",
    "| `torch.autograd.backward()` | 通用接口（支持多个输出） |\n",
    "| `torch.autograd.grad()` | 返回梯度，不存到 `.grad` |\n",
    "| `torch.no_grad()` | 临时禁用梯度（推理用） |\n",
    "| `torch.inference_mode()` | 更快的推理模式 |\n",
    "| `torch.autograd.set_detect_anomaly(True)` | 开启异常检测 |\n",
    "\n",
    "---\n",
    "## 常见坑 ⚠️\n",
    "1. `.grad` 会累加 → 训练时要 `zero_grad()`\n",
    "2. 非标量 `.backward()` 需传 `grad`\n",
    "3. inplace 操作可能破坏计算图\n",
    "4. 只有浮点/复数张量能 `requires_grad=True`\n",
    "5. 中间节点默认不保存 `.grad`（用 `autograd.grad()` 获取）\n",
    "\n",
    "---\n",
    "\n",
    "# torch.autograd.backward() \n",
    "是 PyTorch 自动微分引擎 autograd 的核心接口之一，很多人只知道 loss.backward()，但其实它背后调用的就是 torch.autograd.backward()。\n",
    "\n",
    "## 1. 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8dc2f7-2916-45cd-8619-a23b656412c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.backward(\n",
    "    tensors,\n",
    "    grad_tensors=None,\n",
    "    retain_graph=None,\n",
    "    create_graph=False,\n",
    "    grad_variables=None,   # 旧版本别名\n",
    "    inputs=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6156cdb-4f62-4d75-a894-5ab6466fc4e9",
   "metadata": {},
   "source": [
    "### 它的作用是：\n",
    "给定一个或多个张量 tensors（通常是 loss 或输出），计算它们对叶子节点（如模型参数）的梯度，并把结果累加到 .grad 属性里。\n",
    "\n",
    "等价于在张量上调用 .backward()，但更灵活，可以同时对多个目标做反向传播。\n",
    "\n",
    "---\n",
    "    \n",
    "## 2. 参数详解\n",
    "| 参数                | 类型                     | 说明                                                              |\n",
    "| ----------------- | ---------------------- | --------------------------------------------------------------- |\n",
    "| **tensors**       | Tensor 或 list\\[Tensor] | 需要反向传播的“终点”（通常是 loss，或者是你指定的函数输出）                               |\n",
    "| **grad\\_tensors** | Tensor 或 list\\[Tensor] | 与 `tensors` 形状相同，作为每个输出的“外部梯度” (∂L/∂y)。若标量 loss，可省略；若非标量，必须指定。  |\n",
    "| **retain\\_graph** | bool                   | 是否保留计算图。默认 `False`，一次反向传播后释放；设为 `True` 可多次 backward（如 RNN 训练时）。 |\n",
    "| **create\\_graph** | bool                   | 是否创建更高阶的计算图（允许对梯度再次求导，即高阶导数）。默认 `False`。                        |\n",
    "| **inputs**        | list\\[Tensor]          | 指定只对哪些 Tensor 求梯度。若为 `None`，默认对所有叶子节点计算。                        |\n",
    "\n",
    "---\n",
    "## 3. 工作机制\n",
    "\n",
    "autograd 会把 前向计算过程中构建的计算图 反向遍历。\n",
    "\n",
    "从 tensors 出发，把 grad_tensors 作为初始梯度（链式法则的起点），一路传播到需要梯度的叶子节点。\n",
    "\n",
    "最终结果写入各张量的 .grad 属性（注意是 累加 而不是覆盖）。\n",
    "\n",
    "---\n",
    "## 4. 常见用法\n",
    "### (1) 最基本：标量 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4b646-f769-4bb9-9aaa-cb256ad93628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = (x * 3).sum()   # 标量\n",
    "torch.autograd.backward(y)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3fa11-cda9-476c-a143-c1888acc869f",
   "metadata": {},
   "source": [
    "#### 相当于 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb1964-b053-4ff8-a37c-1f64cb0bbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05faa5ac-1cb3-4d9a-ac1e-602531f396a7",
   "metadata": {},
   "source": [
    "### (2) 非标量张量\n",
    "\n",
    "必须指定 grad_tensors，否则 PyTorch 不知道“往下传播什么梯度”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecdc63-99e4-409a-8f7e-af4b40824b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "y = x * 2           # y.shape = (2,2)，不是标量\n",
    "\n",
    "grad_outputs = torch.ones_like(y)   # 每个元素权重=1\n",
    "torch.autograd.backward(y, grad_tensors=grad_outputs)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71903e-15ff-475a-9aeb-6ae44c684cc7",
   "metadata": {},
   "source": [
    "### (3) 多个目标一起反传"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d14d66-c333-4a1b-91e6-ec0d2c68f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y1 = (x**2).sum()     # ∑ x^2\n",
    "y2 = (x**3).sum()     # ∑ x^3\n",
    "\n",
    "torch.autograd.backward([y1, y2], grad_tensors=[torch.tensor(1.), torch.tensor(0.1)])\n",
    "print(x.grad)\n",
    "\n",
    "👉 意思是：总梯度 = 1·∂y1/∂x + 0.1·∂y2/∂x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b08dfb-d567-4bcd-95aa-8bf13eb4ffd9",
   "metadata": {},
   "source": [
    "### (4) 高阶导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b7bab-e590-47a5-91db-b3a65bbfbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "\n",
    "# 一阶梯度\n",
    "torch.autograd.backward(y, create_graph=True)\n",
    "dy_dx = x.grad        # 3*x^2 = 12\n",
    "print(dy_dx)\n",
    "\n",
    "# 二阶梯度\n",
    "dy_dx.backward()\n",
    "print(x.grad)         # 注意 grad 会累加，所以结果是 12 + 12 = 24\n",
    "\n",
    "👉要避免累加错误，记得 x.grad.zero_()。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b2fca-0d23-4329-b83f-2543f1995d0b",
   "metadata": {},
   "source": [
    "### (5) 限制计算某些 inputs 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe4685-961d-41f7-ba7e-2b33d83be774",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = a * b\n",
    "\n",
    "torch.autograd.backward(c, inputs=[a])   # 只对 a 求梯度\n",
    "print(a.grad)   # 3\n",
    "print(b.grad)   # None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798f99f-47d1-4b27-ad69-92864ff04ee5",
   "metadata": {},
   "source": [
    "## 5. 常见坑点 ⚠️\n",
    "\n",
    "1.非标量必须传 grad_tensors\n",
    "\n",
    "- `y = model(x)  # y.shape=(batch, num_classes)`\n",
    "- `y.backward()  # ❌ 报错`\n",
    "\n",
    "✅ 做法：\n",
    "\n",
    "- `y.backward(torch.ones_like(y))`\n",
    "\n",
    "2.梯度会累加：\n",
    "- `每次调用 backward()，结果会累加到 .grad，不是覆盖。训练循环里要 optimizer.zero_grad()。`\n",
    "\n",
    "3.retain_graph ：\n",
    "- `如果你在同一个计算图上多次调用 backward()，必须 retain_graph=True，否则第二次会报错。`\n",
    "\n",
    "4.整数/布尔张量不能反传：\n",
    "- `只有浮点/复数张量能 requires_grad=True，否则报错。`\n",
    "\n",
    "5.in-place 操作可能破坏计算图：\n",
    "- `在 requires_grad=True 的 Tensor 上做 inplace 修改，可能导致梯度计算错误。`\n",
    "\n",
    "---\n",
    "## 6. 总结\n",
    "\n",
    "- `loss.backward() = torch.autograd.backward(loss)（更常用）。`\n",
    "\n",
    "- `非标量输出要传 grad_tensors。`\n",
    "\n",
    "- `多个目标 → backward([y1, y2], grad_tensors=[g1, g2])。`\n",
    "\n",
    "- `高阶导数 → create_graph=True。`\n",
    "\n",
    "- `梯度累加 → 记得 zero_grad()。`\n",
    "\n",
    "- `inputs 参数可以只对部分张量求梯度。`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da476507-625b-4218-82a4-46bd57168027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a8272-aff3-4617-9a22-ae0e00255337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
