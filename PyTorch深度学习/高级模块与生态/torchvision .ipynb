{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c090412-74d5-4b78-9c93-e901c6d0e5de",
   "metadata": {},
   "source": [
    "# torchvision \n",
    "â€”â€”è¿™æ˜¯ PyTorch ä¸“é—¨ä¸º è®¡ç®—æœºè§†è§‰ (CV) æä¾›çš„å·¥å…·åŒ…ã€‚\n",
    "\n",
    "å®ƒä¸»è¦åŒ…æ‹¬ä¸‰å¤§ç±»åŠŸèƒ½ï¼š\n",
    "\n",
    "- æ•°æ®é›† (torchvision.datasets)\n",
    "\n",
    "- å›¾åƒå˜æ¢ (torchvision.transforms)\n",
    "\n",
    "- æ¨¡å‹ (torchvision.models)\n",
    "\n",
    "å¦å¤–è¿˜æœ‰ä¸€äº›å›¾åƒå¤„ç†å·¥å…·å’Œå¯è§†åŒ–æ–¹æ³•ã€‚\n",
    "\n",
    "---\n",
    "## 1. torchvision.datasets\n",
    "\n",
    "æä¾›äº†å¾ˆå¤šå¸¸ç”¨çš„` å›¾åƒæ•°æ®é›†æ¥å£`ï¼Œå¯ä»¥ç›´æ¥ä¸‹è½½ã€åŠ è½½å¹¶è¿”å› PyTorch çš„` Dataset `å¯¹è±¡ï¼Œå’Œ` DataLoader `é…åˆä½¿ç”¨ã€‚\n",
    "\n",
    "å¸¸è§çš„æ•°æ®é›†ï¼š\n",
    "| æ•°æ®é›†                    | ä»»åŠ¡      | æè¿°               |\n",
    "| ---------------------- | ------- | ---------------- |\n",
    "| **MNIST**              | å›¾åƒåˆ†ç±»    | æ‰‹å†™æ•°å­— (28Ã—28 ç°åº¦å›¾) |\n",
    "| **CIFAR10 / CIFAR100** | å›¾åƒåˆ†ç±»    | å°å›¾ç‰‡ (32Ã—32ï¼ŒRGB)  |\n",
    "| **FashionMNIST**       | å›¾åƒåˆ†ç±»    | ç±»ä¼¼ MNISTï¼Œä½†å†…å®¹æ˜¯è¡£ç‰©  |\n",
    "| **ImageNet**           | å›¾åƒåˆ†ç±»    | å¤§è§„æ¨¡æ•°æ®é›† (1000 ç±»)  |\n",
    "| **COCO**               | ç›®æ ‡æ£€æµ‹/åˆ†å‰² | å¸¸ç”¨çš„æ£€æµ‹ & åˆ†å‰²æ•°æ®é›†    |\n",
    "| **VOC**                | ç›®æ ‡æ£€æµ‹/åˆ†å‰² | PASCAL VOC æ•°æ®é›†   |\n",
    "\n",
    "ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526107d-f53e-4d6c-82d0-0b691206f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# ä¸‹è½½ & åŠ è½½ MNIST\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ac5d1-86d7-4071-9a6c-58d190787c3c",
   "metadata": {},
   "source": [
    "## 2. torchvision.transforms\n",
    "\n",
    "ç”¨äº` æ•°æ®å¢å¼º `/` é¢„å¤„ç†`ï¼ŒæŠŠ` PIL.Image `æˆ–` Tensor `è½¬æ¢ä¸ºé€‚åˆç¥ç»ç½‘ç»œçš„è¾“å…¥ã€‚\n",
    "\n",
    "å¸¸è§æ“ä½œï¼š\n",
    "| æ–¹æ³•                                       | ä½œç”¨                     |\n",
    "| ---------------------------------------- | ---------------------- |\n",
    "| `transforms.Resize(size)`                | è°ƒæ•´å›¾ç‰‡å¤§å°                 |\n",
    "| `transforms.CenterCrop(size)`            | è£å‰ªä¸­å¿ƒåŒºåŸŸ                 |\n",
    "| `transforms.RandomCrop(size)`            | éšæœºè£å‰ª                   |\n",
    "| `transforms.RandomHorizontalFlip(p=0.5)` | éšæœºæ°´å¹³ç¿»è½¬                 |\n",
    "| `transforms.RandomRotation(degrees)`     | éšæœºæ—‹è½¬                   |\n",
    "| `transforms.ColorJitter()`               | éšæœºæ”¹å˜äº®åº¦/å¯¹æ¯”åº¦/é¥±å’Œåº¦         |\n",
    "| `transforms.ToTensor()`                  | è½¬æ¢ä¸º Tensorï¼Œå½’ä¸€åŒ–åˆ° \\[0,1] |\n",
    "| `transforms.Normalize(mean, std)`        | æ ‡å‡†åŒ– `(x-mean)/std`     |\n",
    "| `transforms.Compose([...])`              | ä¸²è”å¤šä¸ªå˜æ¢                 |\n",
    "\n",
    "ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdb88f-a374-4f03-9329-fd2ed119589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # ç»Ÿä¸€å¤§å°\n",
    "    transforms.RandomHorizontalFlip(),  # éšæœºç¿»è½¬\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # æ ‡å‡†åŒ–\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c812f-e0c5-49ac-9825-a5e47df1c136",
   "metadata": {},
   "source": [
    "## 3. torchvision.models\n",
    "\n",
    "æä¾›äº†å¾ˆå¤š` é¢„è®­ç»ƒæ¨¡å‹`ï¼ˆåœ¨ ImageNet ä¸Šè®­ç»ƒè¿‡ï¼‰ï¼Œå¯ä»¥ç›´æ¥æ‹¿æ¥` è¿ç§»å­¦ä¹  `æˆ–` ç‰¹å¾æå–`ã€‚\n",
    "\n",
    "å¸¸è§æ¨¡å‹ï¼š\n",
    "\n",
    "- åˆ†ç±»æ¨¡å‹ï¼š\n",
    "\n",
    "    - ResNet (18/34/50/101/152)\n",
    "    \n",
    "    - VGG (11/13/16/19)\n",
    "    \n",
    "    - DenseNet\n",
    "    \n",
    "    - AlexNet\n",
    "    \n",
    "    - MobileNet\n",
    "    \n",
    "    - EfficientNet\n",
    "    \n",
    "    - Vision Transformer (ViT)\n",
    "\n",
    "- æ£€æµ‹ / åˆ†å‰²æ¨¡å‹ï¼š\n",
    "\n",
    "    - Faster R-CNN\n",
    "    \n",
    "    - Mask R-CNN\n",
    "    \n",
    "    - RetinaNet\n",
    "    \n",
    "    - SSD\n",
    "    \n",
    "    - DeepLabV3\n",
    "    \n",
    "    - FCN\n",
    "\n",
    "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33623d8-c461-40a2-9ca1-fad67544a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒ ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# ä¿®æ”¹æœ€åä¸€å±‚ï¼ˆé€‚åº” 10 åˆ†ç±»ä»»åŠ¡ï¼‰\n",
    "import torch.nn as nn\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a5d685-68a0-43d4-a202-208f47a23c46",
   "metadata": {},
   "source": [
    "## 4. torchvision.io / utils\n",
    "\n",
    "`torchvision.io`ï¼šå›¾åƒ/è§†é¢‘çš„è¯»å†™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f65b8-d841-4358-9359-239c934bdcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "img = read_image(\"example.jpg\")  # ç›´æ¥è¯»å–ä¸º Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e387a9-3c1b-4337-90d2-890c81098368",
   "metadata": {},
   "source": [
    "`torchvision.utils`ï¼šå¯è§†åŒ–å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823f349-e641-4d6a-bb88-671036d911a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grid = make_grid(img_batch, nrow=8, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0))  # C,H,W -> H,W,C\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb77b5-ce27-4cea-95c0-18f3e660d85c",
   "metadata": {},
   "source": [
    "## 5. æ€»ç»“\n",
    "\n",
    "âœ… torchvision æ˜¯ PyTorch çš„è§†è§‰å·¥å…·åŒ…ï¼Œä¸»è¦ä½œç”¨ï¼š\n",
    "\n",
    "- datasets â†’ å¿«é€ŸåŠ è½½å¸¸è§æ•°æ®é›† (MNIST, CIFAR, COCO, VOC, ImageNet)\n",
    "\n",
    "- transforms â†’ æ•°æ®å¢å¼º & é¢„å¤„ç† (Resize, Flip, Normalize...)\n",
    "\n",
    "- models â†’ æä¾›é¢„è®­ç»ƒæ¨¡å‹ (ResNet, VGG, ViT, Faster-RCNN...)\n",
    "\n",
    "- io / utils â†’ å›¾åƒè¯»å†™ & å¯è§†åŒ–å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c2909-0d2d-4bef-aabd-214a4f915fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaf858c8-2db1-4510-8264-38f7fe9c95b0",
   "metadata": {},
   "source": [
    "# ğŸ”¥ è¿ç§»å­¦ä¹ ç¤ºä¾‹ï¼šResNet18 + CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66642062-2424-45c3-80b6-5782d66cabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ===============================\n",
    "# 1. æ•°æ®å‡†å¤‡\n",
    "# ===============================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),  # ResNet è¾“å…¥è¦æ±‚ 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬\n",
    "    transforms.RandomCrop(224, padding=4),  # éšæœºè£å‰ª\n",
    "    transforms.ToTensor(),  # è½¬ä¸º tensor å¼ é‡æ•°æ®ç±»å‹\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),  # å½’ä¸€åŒ–\n",
    "                         (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),   # é‡æ–°è®¾ç½®å›¾ç‰‡çš„å°ºå¯¸\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "\n",
    "classes = trainset.classes  # CIFAR-10 ç±»åˆ«\n",
    "\n",
    "# ===============================\n",
    "# 2. å®šä¹‰æ¨¡å‹ï¼ˆè¿ç§»å­¦ä¹ ï¼‰\n",
    "# ===============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# å†»ç»“å‰é¢çš„å·ç§¯å±‚å‚æ•°ï¼ˆåªè®­ç»ƒæœ€åå…¨è¿æ¥å±‚ï¼‰\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ä¿®æ”¹æœ€åä¸€å±‚ï¼šCIFAR-10 å…±æœ‰ 10 ç±»\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# ===============================\n",
    "# 3. æŸå¤±å‡½æ•° & ä¼˜åŒ–å™¨ & å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "# ===============================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # åªä¼˜åŒ–æœ€åä¸€å±‚\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # æ¯ 10 epoch å­¦ä¹ ç‡ Ã—0.1\n",
    "\n",
    "# ===============================\n",
    "# 4. è®­ç»ƒ & éªŒè¯\n",
    "# ===============================\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()   # æ¸…ç©ºæ¢¯åº¦\n",
    "        outputs = model(inputs) # é¢„æµ‹ï¼ˆå‰å‘ä¼ æ’­ï¼‰\n",
    "        loss = criterion(outputs, targets)  # è®¡ç®—æŸå¤±\n",
    "        loss.backward()         # åå‘æ¢¯åº¦ä¼ æ’­\n",
    "        optimizer.step()        # æ ¹æ®æ¢¯åº¦æ›´æ–° æƒé‡\n",
    "\n",
    "        running_loss += loss.item()    # ç»Ÿè®¡è®­ç»ƒæŸå¤±\n",
    "        _, predicted = outputs.max(1)  # æå–é¢„æµ‹ç±»åˆ«çš„ç´¢å¼•\n",
    "        total += targets.size(0)       # ç»Ÿè®¡è®­ç»ƒæ€»æ ·æœ¬\n",
    "        correct += predicted.eq(targets).sum().item()  # ç»Ÿè®¡å‡†ç¡®ç‡\n",
    " \n",
    "    epoch_loss = running_loss / len(trainloader)  # è®¡ç®—æ¯ä¸ªepochæŸå¤± \n",
    "    epoch_acc = 100. * correct / total            # è®¡ç®—æ¯ä¸ªepochå‡†ç¡®ç‡\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(testloader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5. ä¸»è®­ç»ƒå¾ªç¯\n",
    "# ===============================\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    val_loss, val_acc = validate(epoch)\n",
    "    scheduler.step()   # æ›´æ–°å­¦ä¹ ç‡\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "          f\"| Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# ===============================\n",
    "# 6. å¯è§†åŒ– Loss & Acc æ›²çº¿\n",
    "# ===============================\n",
    "epochs = range(1, num_epochs+1)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, train_accs, label=\"Train Acc\")\n",
    "plt.plot(epochs, val_accs, label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 7. æµ‹è¯•é›†å¯è§†åŒ–é¢„æµ‹ç»“æœ\n",
    "# ===============================\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # å»å½’ä¸€åŒ–\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# éšæœºè·å–ä¸€äº›æµ‹è¯•æ ·æœ¬\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# æ˜¾ç¤ºå›¾åƒ\n",
    "imshow(torchvision.utils.make_grid(images[:8]))\n",
    "print(\"GroundTruth: \", \" \".join(f\"{classes[labels[j]]}\" for j in range(8)))\n",
    "\n",
    "# æ¨¡å‹é¢„æµ‹\n",
    "images = images.to(device)\n",
    "outputs = model(images)\n",
    "_, predicted = outputs.max(1)\n",
    "\n",
    "print(\"Predicted: \", \" \".join(f\"{classes[predicted[j]]}\" for j in range(8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83132f-dfe2-4eed-8540-efe451e7a70f",
   "metadata": {},
   "source": [
    "## ğŸš€ è¿è¡Œæ•ˆæœ\n",
    "\n",
    "è®­ç»ƒæ—¥å¿—ï¼š\n",
    "\n",
    "- æ¯ä¸ª epoch è¾“å‡º Train Loss/Acc å’Œ Val Loss/Acc\n",
    "\n",
    "- Loss & Accuracy æ›²çº¿ï¼šå¯è§†åŒ–å¯¹æ¯”è®­ç»ƒ & éªŒè¯è¿‡ç¨‹\n",
    "\n",
    "- æµ‹è¯•æ ·æœ¬å¯è§†åŒ–ï¼šå±•ç¤ºçœŸå®æ ‡ç­¾ vs æ¨¡å‹é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17199938-07e4-4460-9eb2-1f10e01a5f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
