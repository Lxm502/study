{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0eebefa-b8f8-4eb9-97a8-18a0d3c35da5",
   "metadata": {},
   "source": [
    "# 🧩 torch.nn 速查表\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 容器\n",
    "- `nn.Module` → 基类\n",
    "- `nn.Sequential` → 顺序容器（按顺序堆叠层）\n",
    "\n",
    "- `nn.ModuleDict` → 模块字典（key-value 存储子模块）\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 常见层\n",
    "| 层名称 | 主要参数 | 作用 | 使用场景推荐 |\n",
    "|--------|----------|------|--------------|\n",
    "| `nn.Linear(in_features, out_features, bias=True)` | `in_features`: 输入维度<br>`out_features`: 输出维度<br>`bias`: 是否加偏置 | 全连接层（线性变换） | 用于 MLP、分类器最后一层、特征映射 |\n",
    "| `nn.Conv1d(in_c, out_c, kernel_size, stride=1, padding=0)` | `in_c`: 输入通道数<br>`out_c`: 输出通道数<br>`kernel_size`: 卷积核大小<br>`stride`: 步长<br>`padding`: 填充 | 一维卷积 | 常用于文本/时间序列特征提取 |\n",
    "| `nn.Conv2d(in_c, out_c, kernel_size, stride=1, padding=0)` | 同上 | 二维卷积 | 常用于图像特征提取 (CNN) |\n",
    "| `nn.Conv3d(in_c, out_c, kernel_size, stride=1, padding=0)` | 同上 | 三维卷积 | 视频、医学图像 (MRI/CT) |\n",
    "| `nn.ConvTranspose1d(in_c, out_c, kernel_size, stride=1, padding=0)` | 参数同 Conv1d | 反卷积 (转置卷积) | 常用于生成模型、序列解码 |\n",
    "| `nn.ConvTranspose2d(in_c, out_c, kernel_size, stride=1, padding=0)` | 参数同 Conv2d | 二维转置卷积 | 图像上采样 (GAN/Segmentation) |\n",
    "| `nn.ConvTranspose3d(in_c, out_c, kernel_size, stride=1, padding=0)` | 参数同 Conv3d | 三维转置卷积 | 视频/3D 重建 |\n",
    "| `nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True)` | `input_size`: 输入特征维度<br>`hidden_size`: 隐藏层大小<br>`num_layers`: 堆叠层数 | 循环神经网络 | 处理短序列，简单 NLP/时间序列 |\n",
    "| `nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)` | 同 RNN，内部有 `cell state` | 长短期记忆网络 | NLP 序列建模，长依赖任务 |\n",
    "| `nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)` | 同 RNN，简化版 LSTM | 门控循环单元 | NLP，推荐比 RNN 更优先 |\n",
    "| `nn.BatchNorm1d(num_features)` | `num_features`: 通道数 | 一维批归一化 | 序列/表格特征 |\n",
    "| `nn.BatchNorm2d(num_features)` | `num_features`: 通道数 | 二维批归一化 | CNN 图像卷积输出 |\n",
    "| `nn.BatchNorm3d(num_features)` | `num_features`: 通道数 | 三维批归一化 | 视频、3D 图像 |\n",
    "| `nn.LayerNorm(normalized_shape)` | `normalized_shape`: 要归一化的维度 | 层归一化 | NLP Transformer，RNN，稳定训练 |\n",
    "| `nn.GroupNorm(num_groups, num_channels)` | `num_groups`: 分组数<br>`num_channels`: 总通道数 | 分组归一化 | 适合小 batch 的图像任务 |\n",
    "| `nn.InstanceNorm(num_features)` | `num_features`: 通道数 | 实例归一化 | 风格迁移，图像生成 |\n",
    "| `nn.Dropout(p=0.5)` | `p`: 丢弃概率 | 随机丢弃神经元 | 全连接层防过拟合 |\n",
    "| `nn.Dropout2d(p=0.5)` | `p`: 丢弃概率 | 丢弃整张 feature map | CNN 模型防过拟合 |\n",
    "| `nn.Dropout3d(p=0.5)` | `p`: 丢弃概率 | 丢弃 3D 特征块 | 视频/3D CNN |\n",
    "| `nn.Embedding(num_embeddings, embedding_dim)` | `num_embeddings`: 词表大小<br>`embedding_dim`: 向量维度 | 嵌入层（索引 → 向量） | NLP 词向量，ID 特征编码 |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af5cf2-45b8-4426-897c-4c61a94be914",
   "metadata": {},
   "source": [
    "## 🔹 1️⃣激活函数\n",
    "| 激活函数             | 数学公式                                      | 输出范围       | 特点           | 优点                            | 缺点              | 常用场景            |\n",
    "| ---------------- | ----------------------------------------- | ---------- | ------------ | ----------------------------- | --------------- | --------------- |\n",
    "| **ReLU**         | $f(x) = \\max(0, x)$                       | \\[0, ∞)    | 稀疏激活，简单高效    | 收敛快，梯度传递好                     | 负值死区（Dead ReLU） | CNN、全连接网络       |\n",
    "| **LeakyReLU**    | $f(x) = x$ if $x>0$, $\\alpha x$ if $x<=0$ | (-∞, ∞)    | 改进 ReLU 死区问题 | 避免梯度消失                        | 需调 α 参数         | CNN、深层网络        |\n",
    "| **PReLU**        | 类似 LeakyReLU，但 α 可训练                      | (-∞, ∞)    | 自适应负区间       | 自动学习最优负斜率                     | 增加参数            | CNN、ResNet 等    |\n",
    "| **ELU**          | $x$ if $x>0$, $\\alpha(e^x-1)$ if $x<=0$   | (-α, ∞)    | 平滑，负值收敛      | 减少偏移，梯度流好                     | 稍慢，需调 α         | 深层网络            |\n",
    "| **SiLU / Swish** | $x \\cdot sigmoid(x)$                      | (-∞, ∞)    | 平滑，可微        | 收敛稳定，Transformer、EfficientNet | 计算略慢            | CNN、Transformer |\n",
    "| **GELU**         | $x \\cdot Φ(x)$                            | (-∞, ∞)    | 平滑，概率保留      | Transformer/NLP 高效            | 计算稍慢            | BERT, GPT, ViT  |\n",
    "| **Sigmoid**      | $1 / (1+e^{-x})$                          | (0, 1)     | 非线性，平滑       | 输出概率，易解释                      | 梯度消失，饱和慢        | 二分类输出层          |\n",
    "| **Tanh**         | $(e^x - e^{-x}) / (e^x + e^{-x})$         | (-1, 1)    | 平滑，对称        | 归一化输入                         | 梯度消失            | RNN, 中间层非线性     |\n",
    "| **Softmax**      | $\\sigma(x_i) = e^{x_i}/\\sum e^{x_j}$      | (0,1), Σ=1 | 多分类概率输出      | 输出可直接概率化                      | 只用于最后分类层        | 多分类任务输出层        |\n",
    "\n",
    "---\n",
    "## 2️⃣ 激活函数选择指南\n",
    "### (1) 隐藏层激活函数\n",
    "| 场景                      | 推荐函数                            | 原因                     |\n",
    "| ----------------------- | ------------------------------- | ---------------------- |\n",
    "| CNN 卷积层                 | ReLU / LeakyReLU / PReLU / GELU | 简单高效，梯度流好              |\n",
    "| 深层网络                    | ELU / GELU / SiLU               | 平滑，可缓解梯度消失，训练稳定        |\n",
    "| RNN / LSTM 中间层          | Tanh / ReLU                     | 保持输入均值居中，稳定训练          |\n",
    "| Transformer / Attention | GELU / SiLU                     | 平滑激活，训练稳定，NLP/ViT 经典选择 |\n",
    "\n",
    "### (2) 输出层激活函数\n",
    "| 任务类型  | 推荐函数               | 原因          |\n",
    "| ----- | ------------------ | ----------- |\n",
    "| 二分类   | Sigmoid            | 输出概率 (0\\~1) |\n",
    "| 多分类   | Softmax            | 输出类别概率，互斥   |\n",
    "| 回归    | None / ReLU / Tanh | 根据目标范围调整    |\n",
    "| 多标签分类 | Sigmoid            | 每个标签独立概率    |\n",
    "\n",
    "---\n",
    "### 3️⃣ 选择建议\n",
    "\n",
    "1、默认选择\n",
    "\n",
    "- 隐藏层：ReLU（简单高效）\n",
    "\n",
    "- Transformer / NLP：GELU\n",
    "\n",
    "- 输出层：根据任务类型选择 Sigmoid / Softmax\n",
    "\n",
    "2、考虑梯度消失问题\n",
    "\n",
    "- 如果网络深层或负值可能过多 → 使用 LeakyReLU, ELU, GELU\n",
    "\n",
    "3、计算效率 vs 平滑性\n",
    "\n",
    "- CNN、移动端可优先 ReLU\n",
    "\n",
    "- NLP / Transformer 更注重训练稳定性 → GELU / SiLU\n",
    "\n",
    "4、实验验证\n",
    "\n",
    "- 对新模型，最好尝试两三种激活函数对比收敛速度和最终性能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3318e3-c6e2-43cd-9e63-f60028abec99",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 📌 **解释**：\n",
    "- **隐藏层推荐**：  \n",
    "  - 默认 → `ReLU`  \n",
    "  - 想避免“神经元死亡” → `LeakyReLU` / `ELU`  \n",
    "  - NLP / Transformer 系列 → `GELU` / `SiLU`  \n",
    "  - 特殊情况（自归一化网络） → `SELU`  \n",
    "\n",
    "- **输出层推荐**：  \n",
    "  - 二分类 → `Sigmoid`  \n",
    "  - 多分类 → `Softmax`（一般直接用 `nn.CrossEntropyLoss()`，内部会处理）  \n",
    "  - 多分类 + 需要 log 概率 → `LogSoftmax` + `NLLLoss`  \n",
    "  - 回归 → **不加激活函数**（线性输出），如果要限制范围，可以用 `Tanh`  \n",
    "\n",
    "---\n",
    "![激活函数输出值示意图](激活函数输出值示意图.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 损失函数\n",
    "| 损失函数 | 解释 |\n",
    "|----------|------|\n",
    "| `nn.MSELoss()` | **均方误差（MSE）**，常用于回归。 |\n",
    "| `nn.L1Loss()` | **平均绝对误差（MAE）**，比 MSE 对异常值更鲁棒|\n",
    "| `nn.CrossEntropyLoss()` | **交叉熵损失**，多分类常用，内部包含 `LogSoftmax + NLLLoss`，输入应为 logits（未过 softmax 的值）。 |\n",
    "| `nn.NLLLoss()` | **负对数似然损失**，输入必须是 log softmax 概率，通常和 `nn.LogSoftmax` 搭配使用。 |\n",
    "| `nn.BCELoss()` | **二分类交叉熵损失**，输入必须是 [0,1] 概率（通常先过 sigmoid）。 |\n",
    "| `nn.BCEWithLogitsLoss()` | **带 Logits 的二分类交叉熵损失**，内部集成 sigmoid，更稳定，推荐替代 `BCELoss`。 |\n",
    "| `nn.HingeEmbeddingLoss()` | **合页嵌入损失**，用于相似度学习，衡量样本对是否相似（+1）或不相似（-1）。 |\n",
    "| `nn.MarginRankingLoss()` | **排序边界损失**，用于排序任务，约束：\\(\\max(0, -y \\cdot (x1 - x2) + margin)\\)。 |\n",
    "| `nn.TripletMarginLoss()` | **三元组损失**，常用于人脸识别/度量学习，约束：\\( d(anchor, positive) + margin < d(anchor, negative)\\)。 |\n",
    "| `nn.KLDivLoss()` | **KL 散度损失**，度量两个概率分布差异，常用于知识蒸馏、概率建模。 |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 池化\n",
    "- `nn.MaxPool1d/2d/3d`\n",
    "- `nn.AvgPool1d/2d/3d`\n",
    "- `nn.AdaptiveAvgPool2d`\n",
    "- `nn.AdaptiveMaxPool2d`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffa3c8-56fb-48fc-bfce-e9faab7ec00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e58f71c-ae45-4073-a2ab-57fd5887230f",
   "metadata": {},
   "source": [
    "# 1. 容器（Containers）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e4122-7789-409c-989a-c22535cd69a4",
   "metadata": {},
   "source": [
    "## 🔹 1. nn.Module 概念\n",
    "\n",
    "nn.Module = 神经网络组件的基类。\n",
    "\n",
    "可以表示 单个层（如线性层、卷积层），也可以表示 整个网络（由很多子层组合）。\n",
    "\n",
    "所有 torch.nn 的层和损失函数，都是 nn.Module 的子类。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. nn.Module 的核心方法 & 属性\n",
    "| 方法/属性                          | 作用                          |\n",
    "| ------------------------------ | --------------------------- |\n",
    "| `__init__()`                   | 构造函数，用于定义层和参数               |\n",
    "| `forward(input)`               | 前向传播逻辑，定义输入如何计算输出           |\n",
    "| `.parameters()`                | 返回所有需要梯度的参数（`nn.Parameter`） |\n",
    "| `.named_parameters()`          | 带名字的参数迭代器                   |\n",
    "| `.children()`                  | 迭代直接子模块                     |\n",
    "| `.modules()`                   | 递归迭代所有子模块（包含自身）             |\n",
    "| `.state_dict()`                | 返回模型参数字典（用于保存/加载）           |\n",
    "| `.load_state_dict(state_dict)` | 加载参数字典                      |\n",
    "| `.to(device)`                  | 把模型转移到 CPU/GPU              |\n",
    "| `.train()`                     | 切换到训练模式（启用 Dropout/BN）      |\n",
    "| `.eval()`                      | 切换到评估模式（关闭 Dropout/BN）      |\n",
    "\n",
    "### ⚠️ 注意：\n",
    "\n",
    "必须实现 __init__() 和 forward()\n",
    "\n",
    "不要手动调用 forward()，而是用 model(input)，内部会自动调用 forward\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 3. nn.Module 工作流程\n",
    "\n",
    "1、定义层（在 __init__ 中）：把需要的层定义成成员变量（如 self.fc1 = nn.Linear(...)）\n",
    "\n",
    "2、定义计算逻辑（在 forward 中）：输入如何流过这些层，返回输出\n",
    "\n",
    "3、实例化模型并调用：\\\n",
    "model = MyNet() \\\n",
    "output = model(x)（内部会自动调用 forward）\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 4. 示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d594d5-8d97-41d1-a4d5-83b19ed64aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 自定义网络\n",
    "class MyNet(nn.Module):\n",
    "    # 构造各个层\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()  # 继承 nn.Module\n",
    "        self.fc1 = nn.Linear(10, 20)   # 第一层，全连接\n",
    "        self.fc2 = nn.Linear(20, 1)    # 第二层，全连接\n",
    "\n",
    "    # 计算过程\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # 经过第一层 + ReLU\n",
    "        x = torch.sigmoid(self.fc2(x)) # 经过第二层 + Sigmoid\n",
    "        return x\n",
    "\n",
    "# 使用模型\n",
    "model = MyNet()\n",
    "x = torch.randn(5, 10)    # batch_size=5, 输入特征=10\n",
    "output = model(x)\n",
    "print(output.shape)       # [5, 1]\n",
    "\n",
    "# 查看参数\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "\"\"\"\n",
    "输出：\n",
    "fc1.weight torch.Size([20, 10])\n",
    "fc1.bias   torch.Size([20])\n",
    "fc2.weight torch.Size([1, 20])\n",
    "fc2.bias   torch.Size([1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd72d1-125f-4432-92d1-01bf68d1f271",
   "metadata": {},
   "source": [
    "## 🔹 5. 常见坑\n",
    "\n",
    "1、不要手动调用 forward\n",
    "\n",
    "- `y = model(x)   # 正确` \n",
    "- `y = model.forward(x)  # ❌ 不推荐  → 直接调用 forward 会绕过 hooks 等机制。`\n",
    "\n",
    "2、parameters() 只返回需要梯度的参数,如果你有一些 buffer（比如统计量），可以用 register_buffer。\n",
    "\n",
    "3、训练 / 评估模式要切换\n",
    "\n",
    "model.train()  # 启用 dropout, BN \\\n",
    "model.eval()   # 关闭 dropout, BN\n",
    "\n",
    "\n",
    "4、保存/加载参数要用 state_dict()\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\\\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结：\n",
    "nn.Module 就像 乐高积木的基类 —— 你定义的每个积木（层/网络）都要继承它，把层在 __init__ 里搭好，把计算过程写在 forward 里，然后 PyTorch 会帮你自动处理参数管理、梯度计算、保存加载等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23451c-4cef-4acc-b7c4-06e968982898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50de1ca-db5f-4c7a-86c4-45c274d9a26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e4e63a-3772-439e-af9b-40dbacbf96ff",
   "metadata": {},
   "source": [
    "# 🔹完整示例：MNIST 分类\n",
    "### 1、MLP形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df85b09-35c6-4770-b81e-bf69d094d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 1. 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化到 [-1, 1]\n",
    "])\n",
    "\n",
    "# 2. 下载 MNIST 数据集\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# 拆分训练集和验证集\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 3. 定义模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # 展平\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # 最后一层不用激活，交给CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "\n",
    "# 4. 定义损失函数 & 优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. 训练 + 验证过程\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        # ----训练----\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # ----验证----\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# 训练模型\n",
    "train(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "\n",
    "# 6. 测试过程\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# 测试模型\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94895c3-54b0-4cad-b1ab-279437f523c6",
   "metadata": {},
   "source": [
    "### 2、CNN卷积格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec535226-7ce2-4dc7-97de-2c0fc732874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 1. 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化到 [-1,1]\n",
    "])\n",
    "\n",
    "# 2. 下载 MNIST 数据集\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# 拆分训练集和验证集\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 3. 定义 CNN 模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 卷积层：输入 1x28x28\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 输出 32x28x28\n",
    "        self.pool = nn.MaxPool2d(2, 2)                           # 输出 32x14x14\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 输出 64x14x14 -> pool -> 64x7x7\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)   # 经过2次的 pool\n",
    "        self.fc2 = nn.Linear(128, 10)       # 输出10个类别\n",
    "        \n",
    "        # 展平层\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv1 + relu + pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # conv2 + relu + pool\n",
    "        x = self.flatten(x)                   # 展平\n",
    "        x = F.relu(self.fc1(x))               # \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "\n",
    "# 4. 定义损失函数 & 优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. 训练 + 验证过程\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        # ----训练----\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()      # 清零梯度\n",
    "            outputs = model(images)    # 前向传播（预测） \n",
    "            loss = criterion(outputs, labels)  # 计算损失\n",
    "            loss.backward()            # 反向传播计算梯度\n",
    "            optimizer.step()           # 优化器根据梯度更新权重\n",
    "\n",
    "            running_loss += loss.item() # 将 PyTorch 中的张量转换为 Python 的标量数值\n",
    "            \n",
    "            # 沿着维度1（类别维度）寻找最大值， predicted是最大值的索引，也就是预测的类别编号\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()  # 预测正确的个数\n",
    "            total += labels.size(0)  # 通过计算每个batch的大小，得到总数\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # ----验证----\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# 训练模型\n",
    "train(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "\n",
    "# 6. 测试过程\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# 测试模型\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb7d95-958d-4495-88f4-cda1845664a1",
   "metadata": {},
   "source": [
    "### 🔎 涵盖内容\n",
    "\n",
    "神经网络层：nn.Linear\n",
    "\n",
    "激活函数：nn.ReLU\n",
    "\n",
    "正则化：nn.Dropout\n",
    "\n",
    "损失函数：nn.CrossEntropyLoss（内部包含 LogSoftmax + NLLLoss）\n",
    "\n",
    "评估指标：准确率（Accuracy）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb933cd-ca1e-4232-84fd-9e43751258ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ac2d5-6aea-4a4d-a43b-4cbd1d223e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4b3dd-badf-467b-9672-ef0cc07e82ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d3791-da09-4893-9563-d6828c60e713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3e86934-0cd6-4009-a96e-004d6c45abc6",
   "metadata": {},
   "source": [
    "# nn.Sequential。\n",
    "\n",
    "## 核心概念：nn.Sequential 是什么？\n",
    "`nn.Sequential` 是一个有序的容器 (Ordered Container)。你可以把它想象成一条**`“神经网络流水线”`**。你将一系列的神经网络层（如卷积层、激活函数、线性层等）按照你希望数据流动的顺序，一个个地放入这个容器中。\n",
    "\n",
    "当你将数据输入到这个 `Sequential` 容器时，数据会自动地、严格地按照你添加的顺序，依次流过每一个层，前一层的输出就是后一层的输入。\n",
    "\n",
    "它本身也是一个 `nn.Module`，这意味着你可以将一个 `Sequential` 容器作为一个单独的“层”，嵌套在另一个更复杂的自定义模型中。\n",
    "\n",
    "---\n",
    "## 为什么需要 nn.Sequential？\n",
    "对于许多神经网络结构，尤其是那些“直上直下”、没有分支或跳跃连接的简单网络（如多层感知机 MLP、VGG网络中的卷积块），`nn.Sequential` 可以极大地简化代码。\n",
    "\n",
    "对比一下你就明白了：\n",
    "\n",
    "不使用 `nn.Sequential` 的写法 \n",
    "- 你需要：\n",
    "\n",
    "    - 1.在` __init__ `中分别定义每一个层。\n",
    "\n",
    "    - 2.在` forward `方法中，手动地、一步步地调用每一个层，将数据“接力”下去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46052a99-c3f5-4abe-bfae-b11570607772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(784, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "model_custom = MyMLP()\n",
    "print(model_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e6bcf-dac1-454e-b013-63554c567f40",
   "metadata": {},
   "source": [
    "### 使用 nn.Sequential 的写法\n",
    "你只需要将所有层按顺序放入 `nn.Sequential` 容器即可，PyTorch 会自动为你处理 `forward` 的逻辑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726f26d-6c54-4329-9389-a83631816776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "print(model_sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5de53b-e7c5-465b-a5c1-814a5409be98",
   "metadata": {},
   "source": [
    "- 优点显而易见：代码更简洁、更紧凑、可读性更高，减少了样板代码。\n",
    "\n",
    "---\n",
    "\n",
    "## 创建 nn.Sequential 的三种方式\n",
    "### 1. 直接传入模块作为参数\n",
    "这是最常用、最直接的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a982a43-a018-4b8c-b3de-4b202b960c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(20, 64, 5),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a321175-919d-4fb7-8798-e6a4ba8fc81f",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. 使用有序字典 (collections.OrderedDict)\n",
    "这种方式的好处是，你可以为容器中的每一个模块自定义一个名字。这在后续访问特定层或调试时非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3445b-268c-490f-a35e-bca9ef00c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('relu2', nn.ReLU())\n",
    "]))\n",
    "\n",
    "print(model)\n",
    "\n",
    "输出会带有你定义的名字：\n",
    "Sequential(\n",
    "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (relu1): ReLU()\n",
    "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (relu2): ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a313f-d32d-494b-a3a8-92fc0e6c78b8",
   "metadata": {},
   "source": [
    "### 3. 使用 add_module() 方法\n",
    "你可以先创建一个空的 `Sequential` 容器，然后动态地向其中添加模块。这在模型结构需要通过循环等编程方式生成时非常方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5de646-317c-4024-8e55-131c94d97dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('conv1', nn.Conv2d(1, 20, 5))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('conv2', nn.Conv2d(20, 64, 5))\n",
    "model.add_module('relu2', nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3fccd-a276-40d6-9370-9efbc024f979",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31ec27-35f0-40bc-96ad-0bd85903a2e0",
   "metadata": {},
   "source": [
    "## 如何访问子模块\n",
    "从 `nn.Sequential` 容器中访问特定的层也很简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2c796-fa30-4ee0-8778-ce6b366d8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 model 是用上面 OrderedDict 创建的\n",
    "# 1. 通过索引访问 (所有创建方式都支持)\n",
    "first_layer = model[0]       # 获取第一个 Conv2d\n",
    "first_relu = model[1]      # 获取第一个 ReLU\n",
    "print(first_layer)\n",
    "\n",
    "# 2. 通过自定义的名称访问 (仅限使用 OrderedDict 或 add_module 创建时)\n",
    "conv1_layer = model.conv1    # 使用点记法\n",
    "# 或者\n",
    "conv2_layer = model['conv2'] # 使用字典键记法\n",
    "\n",
    "print(conv2_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c1ee4-5ad7-4329-b3d0-45eb8cae2151",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beff65c-a789-4732-a9ab-af878a2c269b",
   "metadata": {},
   "source": [
    "## `nn.Sequential` 的局限性（何时不该用它）\n",
    "`nn.Sequential` 虽然方便，但它只适用于**“`一条路走到黑`”的线性结构。如果你的模型结构包含以下任何一种情况，就不能**单独使用 `nn.Sequential`：\n",
    "\n",
    " -  1、`多输入或多输出`: `Sequential` 只能处理单输入、单输出的流程。\n",
    "\n",
    " -  2、`跳跃/残差连接` (`Skip`/`Residual Connections`): 像 `ResNet` 中那样，需要将前面层的输入 x 与后面层的输出相加。\n",
    "\n",
    " -  3、`分支结构`: 数据流需要分叉，经过不同的处理后再合并。\n",
    "\n",
    "### 示例：一个 `nn.Sequential` 无法实现的残差块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f8e3c-1537-4d7e-a0df-59295d5e4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # self.main_path 可以是 Sequential，因为它内部是线性的\n",
    "        self.main_path = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 数据在主路径上流动\n",
    "        main_output = self.main_path(x)\n",
    "        \n",
    "        # 数据在跳跃连接路径上流动\n",
    "        skip_output = self.conv_skip(x)\n",
    "        \n",
    "        # 两条路径的结果需要相加，这是 nn.Sequential 无法自动处理的\n",
    "        return main_output + skip_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fda45b-b1db-4e60-9c17-73fa50191dae",
   "metadata": {},
   "source": [
    "在这个例子中，`forward` 函数中包含了 `main_output` + `skip_output` 这样的非线性操作，因此必须自定义 `nn.Module`。不过，你可以看到 `Sequential` 依然可以用来`构建其中线性的部分`（如 `self.main_path`），这体现了它的组合性。\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    " - `nn.Sequential` 是一个用于快速搭建`线性层叠`模型的便捷容器。\n",
    "\n",
    " - 它通过自动处理 `forward` 传递，极大地简化了代码。\n",
    "\n",
    " - 你可以使用`有序字典`或 `add_module` 为层命名，方便后续访问和调试。\n",
    "\n",
    " - 当模型涉及跳跃连接、分支、多输入/输出等复杂结构时，你必须回归到继承 `nn.Module` 并手动编写 `forward` 方法的传统方式。\n",
    "\n",
    " - 即使在复杂的模型中，`nn.Sequential` 依然是构建模型局部线性块的绝佳工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0dfb3-7d34-45d2-ba84-a9e201c0410a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc33da1-e782-4bdb-afe1-137321f2eead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
