{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317d3233-1be1-4c69-8740-c5eb5496e5e3",
   "metadata": {},
   "source": [
    "# nn.Linear 层，\n",
    "也常被称为`全连接层` (Fully Connected Layer) 或 `密集层` (`Dense Layer`)，是神经网络中最基本也是最常用的一种层。\n",
    "\n",
    "- 核心功能：线性变换与特征组合 \\\n",
    "`nn.Linear` 层对输入数据执行一个线性变换 :  y = xW**T + b\n",
    "\n",
    "- 其中 ：\n",
    "    - x 是输入。\n",
    "    - W 是该层的权重 (`weights`) 矩阵。\n",
    "    - b 是该层的偏置 (`bias`) 向量。\n",
    "    - y 是输出。\n",
    "    - T转置\n",
    "\n",
    "- 权重` W `和偏置` b `都是模型在训练过程中需要学习的参数。\n",
    "\n",
    "- 它的主要作用是：\n",
    "\n",
    "    - 特征组合：`nn.Linear` 层的每个输出神经元都与所有输入神经元相连接。这使得该层能够学习输入特征之间的全局关系，并将它们组合成更高级的表示。\n",
    "    - 维度变换：它可以将输入数据从一个维度（特征空间）映射到另一个维度。你可以用它来增加或减少特征向量的长度。\n",
    "    - 分类/回归头：在网络的末端，nn.Linear 层通常用作“决策头”。例如，在卷积神经网络（CNN）提取出图像的高级特征后，这些特征会被展平 (flatten) 并送入一个或多个 nn.Linear 层，最终输出每个类别的得分（用于分类）或一个连续值（用于回归）。\n",
    "\n",
    "---\n",
    "### 关键参数详解\n",
    "nn.Linear 的构造函数非常简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29444e13-c837-4dfc-ac18-d3f72ba2dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear(\n",
    "    in_features,\n",
    "    out_features,\n",
    "    bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b99b63-a264-4a5c-8ac7-11e2d7ccc24a",
   "metadata": {},
   "source": [
    "### 1. in_features (整数)\n",
    "含义：`输入样本的特征数量`（即输入`向量的长度`）。\n",
    "\n",
    "- 如何设置：这个值必须与输入到该层的数据的最后一个维度的大小完全匹配。\n",
    "\n",
    "例如，如果你的输入数据是一个包含784个像素值的展平图像，那么第一个 `nn.Linear` 层的 `in_features` 就必须是784。\n",
    "\n",
    "如果它位于另一个 `nn.Linear` 层之后，它的 `in_features` 必须等于前一层的` out_features`。\n",
    "\n",
    "### 2. out_features (整数)\n",
    "- 含义：该层输出的特征数量（`即该层中神经元的数量`）。\n",
    "\n",
    "    - 如何设置：这是你作为模型设计者需要定义的超参数。\n",
    "    \n",
    "    - 在隐藏层中，`out_features `的大小决定了模型的“宽度”和容量。\n",
    "\n",
    "- 在输出层中，它由任务目标决定：\n",
    "\n",
    "    - 多类别分类：`out_features` 等于类别总数 K。\n",
    "    \n",
    "    - 二元分类：`out_features` 通常为 1 (配合 sigmoid 激活函数)。\n",
    "    \n",
    "    - 回归：out_features 通常为 1 (如果预测单个值)。\n",
    "\n",
    "### 3. bias (布尔值)\n",
    "- 含义：是否在该层中添加一个可学习的偏置项 b。\n",
    "\n",
    "- 如何设置：默认为 True。在绝大多数情况下，你都应该保持这个默认设置。偏置项增加了模型的灵活性，使其能够更好地拟合数据。\n",
    "\n",
    "---\n",
    "### 输入与输出的形状 (Shape)\n",
    "这是 `nn.Linear` 层的一个非常重要的特性。\n",
    "\n",
    "- 输入形状: (N, *, H_in)\n",
    "\n",
    "    - N: 批次大小 (Batch Size)。\n",
    "    \n",
    "    - *: 表示任意数量的额外维度。\n",
    "    \n",
    "    - H_in: 输入特征数 (in_features)。nn.Linear 只对输入的最后一个维度进行操作。\n",
    "\n",
    "- 输出形状: (N, *, H_out)\n",
    "    \n",
    "    - H_out: 输出特征数 (out_features)。\n",
    "\n",
    "除了最后一个维度从` H_in `变为` H_out `之外，所有其他维度都保持不变。\n",
    "\n",
    "---\n",
    "- 示例：\n",
    "    - 输入张量形状: (64, 784) (一个批次包含64个样本，每个样本有784个特征)\n",
    "    \n",
    "    - 线性层定义: `nn.Linear`(in_features=784, out_features=128)\n",
    "    \n",
    "    - 输出张量形状: (64, 128)\n",
    " \n",
    "---\n",
    "\n",
    "### 代码示例\n",
    "示例1：构建一个简单的多层感知机 (MLP)\\\n",
    "这个例子展示了如何堆叠 `nn.Linear `层来处理表格数据或展平后的图像数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8537c1-39d6-43ec-bda6-4f311ab3bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个用于MNIST分类的简单网络\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # MNIST图像是28x28=784\n",
    "        # 第一个线性层：输入784，输出256\n",
    "        self.flatten = nn.Flatten()  # 将输入维度展平\n",
    "        self.fc1 = nn.Linear(in_features=784, out_features=256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # 第二个线性层：输入256 (必须匹配上一层的输出)，输出128\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # 输出层：输入128，输出10 (对应0-9十个数字)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的初始形状: (N, 1, 28, 28)\n",
    "        # 首先将图像展平\n",
    "        x = self.flatten(x)  # 自动处理维度  -> (N, 784)\n",
    "        \n",
    "        x = self.fc1(x)    # -> (N, 256)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)    # -> (N, 128)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)    # -> (N, 10)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = MLP()\n",
    "\n",
    "# 创建一个假的输入数据 (批次大小为4)\n",
    "dummy_input = torch.randn(4, 1, 28, 28)\n",
    "\n",
    "# 前向传播\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"输入形状:\", dummy_input.shape)\n",
    "print(\"展平后形状:\", dummy_input.flatten(-1, 28*28).shape)\n",
    "print(\"输出形状:\", output.shape)\n",
    "\n",
    "# 输出:\n",
    "# 输入形状: torch.Size([4, 1, 28, 28])\n",
    "# 展平后形状: torch.Size([4, 784])\n",
    "# 输出形状: torch.Size([4, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2bcd0-35b9-4282-a901-04daa6cad421",
   "metadata": {},
   "source": [
    "### 示例2：在CNN的末尾使用\n",
    "这个例子展示了 nn.Linear 如何作为CNN的分类头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b2370-320e-4ad0-88f2-a329e400839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1), # -> (N, 16, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2) # -> (N, 16, 14, 14)\n",
    "        )\n",
    "        \n",
    "        # 经过卷积和池化后，特征图大小为 16x14x14\n",
    "        # 所以展平后的向量长度为 16 * 14 * 14 = 3136\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=16 * 14 * 14, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的形状: (N, 1, 28, 28)\n",
    "        x = self.conv_block(x) # -> (N, 16, 14, 14)\n",
    "        \n",
    "        # 展平特征图以送入Linear层\n",
    "        x = torch.flatten(x, start_dim=1) # -> (N, 3136)\n",
    "        \n",
    "        output = self.classifier(x) # -> (N, 10)\n",
    "        return output\n",
    "\n",
    "# 创建模型和数据\n",
    "model_cnn = CNN()\n",
    "dummy_input = torch.randn(4, 1, 28, 28)\n",
    "output = model_cnn(dummy_input)\n",
    "print(\"\\n--- CNN 示例 ---\")\n",
    "print(\"CNN输出形状:\", output.shape)\n",
    "\n",
    "# 输出:\n",
    "# --- CNN 示例 ---\n",
    "# CNN输出形状: torch.Size([4, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dc68a-60e3-43e6-963a-1ec3ec634b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
