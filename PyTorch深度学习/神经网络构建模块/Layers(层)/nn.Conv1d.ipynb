{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9675039-6c65-47ab-887d-ee413514585f",
   "metadata": {},
   "source": [
    "# nn.Conv1d\n",
    "核心概念：与 Conv2D 的对比\n",
    "`nn.Conv1d` 的核心思想是在一维序列上滑动一个滤波器（卷积核）。\n",
    "| 特性     | nn.Conv2d (二维卷积)            | nn.Conv1d (一维卷积)                     |\n",
    "|----------|----------------------------------|------------------------------------------|\n",
    "| 数据类型   | 2D数据，如图像                   | 1D数据，如时间序列、文本、音频            |\n",
    "| 滤波器形状 | 2D，如 (3, 3)                   | 1D，如 (3,)                             |\n",
    "| 滑动方式   | 在图像的高度和宽度上滑动          | 仅在序列的长度方向上滑动                 |\n",
    "| 检测目标   | 空间模式，如边缘、纹理、形状       | 序列模式 (Motifs)，如特定n-gram、波形成分 |\n",
    "\n",
    "## 核心功能与应用场景\n",
    "`nn.Conv1d` 主要用于从序列数据中提取局部模式。它的滤波器会在序列上滑动，每次只关注一小段连续的片段（由 `kernel_size` 决定），从而识别出有意义的局部特征。\n",
    "\n",
    "主要应用场景：\n",
    "\n",
    "- 自然语言处理 (NLP)：\n",
    "\n",
    "    - 数据：一个句子可以表示为词向量序列。输入形状为 (`批次大小, 词向量维度, 句子长度`)。\n",
    "    \n",
    "    - 作用：`nn.Conv1d` 可以识别 `N-grams`（连续的N个词）。例如，一个大小为`3`的`卷积核`可以学习识别像 \"I love you\" 或 \"New York City\" 这样的短语模式。\n",
    "\n",
    "- 时间序列分析：\n",
    "\n",
    "    - 数据：传感器读数、股票价格、心电图 (ECG) 信号等。输入形状为 (`批次大小, 特征数, 时间步数`)。\n",
    "    \n",
    "    - 作用：可以识别时间序列中的特定形状或模式，如某个特定周期的价格波动、心电图中的异常波形等。\n",
    "\n",
    "- 音频处理：\n",
    "\n",
    "    - 数据：原始音频波形。\n",
    "    \n",
    "    - 作用：可以作为滤波器，提取音频信号中的特定频率成分。\n",
    "\n",
    "- 基因组学：\n",
    "\n",
    "    - 数据：DNA或蛋白质序列。\n",
    "    \n",
    "    - 作用：识别基因序列中的特定模式（Motifs）。\n",
    "\n",
    "---\n",
    "### 关键参数详解\n",
    "`nn.Conv1d` 的参数与 `nn.Conv2d `非常相似，只是没有了“高度”这个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dbd26-6a5c-403b-a249-5e15d51072b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Conv1d(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    "    bias=True,\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc44d7-86ec-47c1-96d0-793d05c1f38d",
   "metadata": {},
   "source": [
    "- `in_channels` (整数): 输入序列中每个时间步的特征数。\n",
    "\n",
    "    - 在NLP中，这通常是词嵌入的维度 (embedding dimension)。\n",
    "    \n",
    "    - 在时间序列中，这可能是多个传感器在同一时刻的读数。\n",
    "\n",
    "- `out_channels` (整数): `滤波器`（卷积核）的`数量`，决定了输出序列的通道数。代表`模型想要学习的模式种类数量`。\n",
    "\n",
    "- `kernel_size` (整数): 卷积核的长度。`kernel_size=3` 意味着滤波器会一次性查看序列中3个连续的时间步。\n",
    "\n",
    "- `stride` (整数): 滤波器滑动的步长。\n",
    "\n",
    "- `padding` (整数): 在序列的两端进行填充。`padding='same'` (需手动计算) 可以让输出序列和输入序列等长。\n",
    "\n",
    "---\n",
    "### 输入与输出的形状 (Shape)\n",
    "这是理解 `nn.Conv1d` 的关键。\n",
    "\n",
    "- 输入形状: (N, C_in, L_in)\n",
    "\n",
    "    - N: 批次大小 (Batch Size)\n",
    "\n",
    "    - C_in: 输入通道数 (`in_channels`)\n",
    "    \n",
    "    - L_in: 序列长度 (`Sequence Length`)\n",
    "\n",
    "- 输出形状: (N, C_out, L_out)\n",
    "\n",
    "    - N: 批次大小 (不变)\n",
    "\n",
    "    - C_out: 输出通道数 (out_channels)\n",
    "    \n",
    "    - L_out: 输出序列长度 (需要计算)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec1cba-c93c-4a13-9813-4542406d01dc",
   "metadata": {},
   "source": [
    "# 输出序列长度计算公式\n",
    "\n",
    "对于序列模型，输出序列长度 $L_{out}$ 的计算公式通常为：\n",
    "\n",
    "$$\n",
    "L_{out} = \\left\\lfloor \\frac{L_{in} + 2p - k}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $L_{in}$：输入序列长度\n",
    "- $p$：填充数（padding）\n",
    "- $k$：卷积核大小/窗口大小（kernel size）\n",
    "- $s$：步长（stride）\n",
    "- $\\left\\lfloor \\cdot \\right\\rfloor$：向下取整函数\n",
    "\n",
    "## 示例计算\n",
    "\n",
    "假设：\n",
    "- $L_{in} = 10$\n",
    "- $p = 1$\n",
    "- $k = 3$\n",
    "- $s = 2$\n",
    "\n",
    "则：\n",
    "$$\n",
    "L_{out} = \\left\\lfloor \\frac{10 + 2 \\times 1 - 3}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{9}{2} \\right\\rfloor + 1 = 4 + 1 = 5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b34a786-6a79-4561-9699-4b719f365c5e",
   "metadata": {},
   "source": [
    "---\n",
    "## 示例计算 (NLP场景):\n",
    "假设我们有一个批次的数据，包含32个句子，每个句子最多50个词，每个词用100维的向量表示。\n",
    "\n",
    "- 输入张量形状: (32, 100, 50)  (N=32, C_in=100, L_in=50)\n",
    "\n",
    "- 卷积层定义: `nn.Conv1d`(`in_channels`=100, `out_channels`=256, `kernel_size`=3, `padding`=1)\n",
    "\n",
    "我们想要学习256种不同的 `\"3-gram\"`模式。\n",
    "\n",
    "- `padding`=1 是为了保持序列长度不变。\n",
    "\n",
    "计算输出长度 `L_out`: 50\n",
    "- 最终输出张量形状: (32, 256, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a05fd-4bb5-4f88-9ed0-c33026167ae5",
   "metadata": {},
   "source": [
    "---\n",
    "### 代码示例 (文本分类)\n",
    "这个例子展示了如何使用 `nn.Conv1d `对文本进行分类，这是一个非常经典的应用（`TextCNN`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0cdd39-1af9-4fb7-ad85-66e5689aa663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        # 1. 嵌入层：将词索引转换为词向量\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 2. 卷积层：输入通道数=词向量维度，输出通道数=滤波器数量\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=embedding_dim, \n",
    "            out_channels=128, # 学习128种模式\n",
    "            kernel_size=3, # 类似 3-grams\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 3. 池化层：在时间步维度上取最大值，捕获最重要的信号\n",
    "        # Global Max Pooling\n",
    "        \n",
    "        # 4. 全连接层：进行分类\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 初始形状: (N, L_in) -> (批次大小, 句子长度)\n",
    "        \n",
    "        # 1. 经过嵌入层\n",
    "        x = self.embedding(x) # -> (N, L_in, embedding_dim)\n",
    "        \n",
    "        # 2. 调整维度以匹配Conv1d的输入 (N, C_in, L_in)\n",
    "        # PyTorch 的卷积层要求通道数在第二个维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, embedding_dim, L_in)\n",
    "        \n",
    "        # 3. 经过卷积层\n",
    "        x = self.conv1d(x) # -> (N, 128, L_in)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # 4. 全局最大池化\n",
    "        # 在序列长度维度上取最大值\n",
    "        x = torch.max(x, dim=2)[0] # -> (N, 128)\n",
    "        \n",
    "        # 5. 经过全连接层分类\n",
    "        output = self.fc(x) # -> (N, num_classes)\n",
    "        return output\n",
    "\n",
    "# --- 模型和数据设置 ---\n",
    "VOCAB_SIZE = 1000  # 词汇表大小\n",
    "EMBEDDING_DIM = 100 # 词向量维度\n",
    "SEQ_LENGTH = 50    # 句子长度\n",
    "NUM_CLASSES = 10   # 类别数\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 创建模型实例\n",
    "model = TextClassifier(VOCAB_SIZE, EMBEDDING_DIM, NUM_CLASSES)\n",
    "\n",
    "# 创建假的输入数据 (词索引)\n",
    "dummy_input = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))\n",
    "\n",
    "# 前向传播\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"输入形状:\", dummy_input.shape)\n",
    "print(\"输出形状:\", output.shape)\n",
    "\n",
    "# 输出:\n",
    "# 输入形状: torch.Size([32, 50])\n",
    "# 输出形状: torch.Size([32, 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
