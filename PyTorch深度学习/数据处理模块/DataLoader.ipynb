{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85967f66-e2e8-41d3-849a-7e8ae27b9ad8",
   "metadata": {},
   "source": [
    "# 2、torch.utils.data.DataLoader。\n",
    "\n",
    "如果说` Dataset `是一个`“数据仓库”`，定义了数据的总量和获取单个数据的方法，那么` DataLoader `就是一个高效的`“数据搬运工”`，负责从仓库中取出数据，经过智能打包和运输，高效地送达“模型”这个工厂。\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 核心概念：DataLoader 是什么？\n",
    "`torch.utils.data.DataLoader` 是一个 Python 可`迭代对象` (iterable)。它将一个 Dataset 对象包装起来，为我们提供了一种简单、高效、可定制的方式来迭代访问数据集。\n",
    "\n",
    "它解决了` Dataset `无法直接处理的几个关键问题，使得大规模数据训练成为可能：\n",
    "\n",
    "- 1.`批量处理 (Batching)`：\n",
    "模型训练通常采用小批量随机梯度下降 (mini-batch SGD)，一次处理一小批数据而不是单个样本。`DataLoader` 自动将从` Dataset`中取出的单个样本打包成一个批次 (batch)。\n",
    "\n",
    "- 2.`数据打乱 (Shuffling)`：\n",
    "为了让模型有更好的泛化能力，避免过拟合，我们需要在每个`训练周期` (epoch) 开始时打乱数据顺序。`DataLoader `可以通过一个简单的参数` shuffle=True `实现这一点。\n",
    "\n",
    "- 3.`并行加载 (Parallel Loading)`：\n",
    "数据加载（从硬盘读取、预处理）通常是 CPU 密集型任务。如果串行加载，GPU 可能会花费大量时间等待 CPU 准备好数据，造成“算力饥饿”。`DataLoader `可以使用多个子进程 (num_workers) 在后台并行加载数据，让数据准备和模型计算同时进行，极大地提升了训练效率。\n",
    "\n",
    "- 4.`数据整合 (Collation)`：\n",
    "将多个独立的样本组合成一个批次张量 (batch tensor) 的过程。`DataLoader `有默认的整合逻辑，也支持用户自定义。\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 核心参数详解\n",
    "`DataLoader` 的构造函数有很多参数，我们来讲解其中最重要、最常用的几个："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083c04d-02bf-4d3a-96da-20f5b5bb9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.DataLoader(\n",
    "    dataset,            # 数据来源 (Dataset对象)，必须实现 __len__() 和 __getitem__()\n",
    "    batch_size=1,       # 每个批次包含的样本数量\n",
    "    shuffle=False,      # 是否打乱顺序 (训练时通常设为True，验证/测试时设为False)\n",
    "    sampler=None,       # 自定义采样器对象，用于控制数据加载顺序 (与shuffle互斥)\n",
    "    batch_sampler=None, # 类似sampler但每次返回一个batch的索引 (与batch_size/shuffle/sampler/drop_last互斥)\n",
    "    num_workers=0,      # 用于数据加载的子进程数量 (0表示在主进程加载)\n",
    "    collate_fn=None,    # 合并样本列表形成batch的函数 (默认是torch.stack)\n",
    "    pin_memory=False,             # 若为True，将数据加载到CUDA固定内存(可加速GPU传输)\n",
    "    drop_last=False,              # 是否丢弃最后一个不完整的batch (当样本数不能被batch_size整除时)\n",
    "    timeout=0,                    # 数据加载的超时时间(秒)，0表示不超时\n",
    "    worker_init_fn=None,          # 每个worker初始化函数\n",
    "    multiprocessing_context=None, # 多进程上下文\n",
    "    generator=None,               # 用于生成随机数的生成器对象\n",
    "    prefetch_factor=2,            # 每个worker预加载的batch数量\n",
    "    persistent_workers=False      # 是否保持workers存活(避免每个epoch重建)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407089b8-62aa-40bd-85ee-b0549a3868a3",
   "metadata": {},
   "source": [
    "## 参数详解\n",
    "---\n",
    "#### `dataset`（必需）\n",
    "- **类型**：`torch.utils.data.Dataset` 对象  \n",
    "- **作用**：指定`数据来源`，是 `DataLoader` 的数据源。必须传入一个继承自 `Dataset` 的实例。\n",
    "\n",
    "---\n",
    "\n",
    "#### `batch_size`\n",
    "- **类型**：`int`，默认为 `1`  \n",
    "- **作用**：定义每个批次包含的样本数量。  \n",
    "- **建议**：根据 GPU 显存和模型复杂度调整，常见值为 `16`, `32`, `64`, `128`。\n",
    "\n",
    "---\n",
    "#### `shuffle`\n",
    "- **类型**：`bool`，默认为 `False`  \n",
    "- **作用**：是否在每个 epoch 开始时打乱数据顺序。  \n",
    "- **建议**：\n",
    "  - ✅ 训练时：`True`（提升模型泛化能力）\n",
    "  - ❌ 验证/测试时：`False`（保证评估一致性）\n",
    "\n",
    "---\n",
    "\n",
    "#### `num_workers`\n",
    "- **类型**：`int`，默认为 `0`  \n",
    "- **作用**：用于数据加载的子进程数量。\n",
    "  - `0`：所有数据在主进程中加载（同步）。\n",
    "  - `> 0`：使用多个子进程异步加载数据，提升速度。\n",
    "- **建议**：\n",
    "  - 一般设为 `4`, `8`, `16`，建议不超过 CPU 核心数。\n",
    "  - Windows 上注意避免 `num_workers > 0` 导致的 `freeze_support` 问题（建议在 `if __name__ == '__main__':` 中运行）。\n",
    "\n",
    "---\n",
    "\n",
    "#### `pin_memory`\n",
    "- **类型**：`bool`，默认为 `False`  \n",
    "- **作用**：若为 `True`，将数据加载到“固定内存”（pinned memory），加快从 CPU 到 GPU 的传输速度。  \n",
    "- **建议**：\n",
    "  - ✅ 使用 GPU 训练时：强烈建议设为 `True`\n",
    "  - ❌ CPU 训练时：无需开启\n",
    "\n",
    "---\n",
    "\n",
    "#### `drop_last`\n",
    "- **类型**：`bool`，默认为 `False`  \n",
    "- **作用**：当样本总数不能被 `batch_size` 整除时，最后一个批次样本数会不足。若设为 `True`，则丢弃这个不完整的批次。  \n",
    "- **应用场景**：\n",
    "  - 某些模型要求输入尺寸固定（如部分 RNN、GAN）\n",
    "  - 批归一化（BatchNorm）在小批次上不稳定时\n",
    "\n",
    "---\n",
    "\n",
    "#### `collate_fn`\n",
    "- **类型**：可调用函数（`callable`），默认为 `None`  \n",
    "- **作用**：自定义如何将多个样本组合成一个批次。默认函数会将张量堆叠（`torch.stack`）。  \n",
    "- **默认行为**：\n",
    "  ```python\n",
    "  # 默认 collate_fn 会做类似操作\n",
    "  batch = {\n",
    "      'images': torch.stack([s['image'] for s in samples]),\n",
    "      'labels': torch.tensor([s['label'] for s in samples])\n",
    "  }\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 使用建议总结\n",
    "\n",
    "| 参数 | 训练模式建议 | 验证/测试建议 |\n",
    "|------|---------------|----------------|\n",
    "| `shuffle` | `True` | `False` |\n",
    "| `num_workers` | `4~16`（根据 CPU） | `4~8` |\n",
    "| `pin_memory` | `True`（GPU） | `True`（GPU） |\n",
    "| `drop_last` | `True`（若模型敏感） | `False` |\n",
    "| `collate_fn` | 按需自定义 | 按需自定义 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570e07f-7b13-4032-99c2-f34517cd55f3",
   "metadata": {},
   "source": [
    "## 🔄 DataLoader 工作流程详解\n",
    "当你开始在一个` DataLoader `上进行迭代时（例如：for batch in data_loader:），其内部会自动执行一系列高效的操作，实现数据加载与模型训练的流水线并行。整个流程如下：\n",
    "\n",
    "### 1️⃣ 生成索引（Index Generation）\n",
    "`DataLoader` 首先通过一个 `Sampler`（采样器） 生成当前批次所需的样本索引列表。\\\n",
    "根据 `shuffle` 参数选择不同的采样策略：\n",
    "- `✅ shuffle=True → 使用 RandomSampler（随机打乱顺序）`\n",
    "- `❌ shuffle=False → 使用 SequentialSampler（按顺序采样）`\n",
    "这些索引决定了本次需要加载哪些样本。\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ 分发任务（Task Distribution）\n",
    "如果设置了` num_workers `> 0，`DataLoader `会将这批索引分发给`多个子进程`（worker processes）。\n",
    "每个子进程负责加载一部分数据，实现任务并行化。\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ 并行加载（Parallel Data Loading）\n",
    "每个子进程独立执行：\n",
    "\n",
    "dataset[index]\n",
    "\n",
    "- `即调用 Dataset 的 __getitem__ 方法。`\n",
    "\n",
    "- 加载过程包括：\n",
    "    - `文件读取（如图像、文本）`\n",
    "    - `数据解码（如 PIL 加载图片）`\n",
    "    - `应用 transform 进行预处理`\n",
    "    - `所有子进程并行运行，显著提升 I/O 效率，避免成为训练瓶颈。`\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ 整合数据（Collation）\n",
    "主进程收集所有子进程返回的单个样本，组成一个列表：\n",
    "\n",
    "batch_list = [sample_1, sample_2, ..., sample_batch_size]\n",
    "\n",
    "然后调用 `collate_fn` 函数，将该列表整合为一个完整的批次：\\\n",
    "🔹 默认行为：使用 `torch.stack()` 将张量堆叠成一个大张量。\\\n",
    "🔹 自定义需求：对于变长数据（如` NLP `句子），需自定义` collate_fn `实现 `padding` 或 `packing`。\\\n",
    "输出通常为 `(inputs, labels)` 元组或`字典形式`。\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ 返回批次（Yield Batch）\n",
    "整合后的批次数据通过` yield `返回给训练循环。\\\n",
    "⚡ 关键优势：在主进程进行前向传播、反向传播等计算的同时，子进程已在后台加载下一个批次的数据，形成“流水线并行（Pipelining）”，最大化 GPU 利用率。\n",
    "\n",
    "---\n",
    "\n",
    "### 代码示例\n",
    "我们继续使用之前定义的`CatsAndDogsDataset`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6fa83-b2a7-42ff-873f-7974c5355cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 假设 CatsAndDogsDataset 类已经在这里定义好了\n",
    "# class CatsAndDogsDataset(Dataset):\n",
    "#     ...\n",
    "\n",
    "# 1. 定义数据变换\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 2. 实例化 Dataset\n",
    "dataset_path = 'data/'\n",
    "train_dataset = CatsAndDogsDataset(root_dir=dataset_path, transform=data_transform)\n",
    "\n",
    "# 3. 实例化 DataLoader，并配置核心参数\n",
    "# 这是训练集加载器，所以 shuffle=True\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,       # 每个批次加载 64 张图片\n",
    "    shuffle=True,        # 每个 epoch 都打乱数据\n",
    "    num_workers=4,       # 使用 4 个子进程来加载数据\n",
    "    pin_memory=True      # 如果使用 GPU，设置为 True\n",
    ")\n",
    "\n",
    "# 4. 在训练循环中使用 DataLoader\n",
    "print(f\"开始遍历 train_loader...\")\n",
    "# DataLoader 是一个迭代器，我们可以像遍历列表一样遍历它\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    # 将数据移动到 GPU (如果可用)\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # images = images.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    \n",
    "    # 打印批次数据的形状\n",
    "    print(f\"批次 {i+1}:\")\n",
    "    print(f\"  - 图像批次的形状: {images.shape}\")  # torch.Size([64, 3, 224, 224])\n",
    "    print(f\"  - 标签批次的形状: {labels.shape}\")  # torch.Size([64])\n",
    "\n",
    "    # 在这里，可以将 images 和 labels 送入模型进行训练\n",
    "    # e.g., outputs = model(images)\n",
    "    #        loss = criterion(outputs, labels)\n",
    "    #        ...\n",
    "\n",
    "    # 为了演示，我们只遍历几个批次就退出\n",
    "    if i >= 2:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
