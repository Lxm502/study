{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cdd689-e31a-4535-96f6-a8c480de7e73",
   "metadata": {},
   "source": [
    "# 🔹 3. 训练与优化\n",
    "\n",
    "## 3.1、优化器 (torch.optim)\n",
    "\n",
    "optim.SGD\n",
    "\n",
    "optim.Adam\n",
    "\n",
    "optim.AdamW\n",
    "\n",
    "optim.RMSprop\n",
    "\n",
    "optim.Adagrad\n",
    "\n",
    "## 3.2、学习率调度器 (torch.optim.lr_scheduler)\n",
    "\n",
    "StepLR\n",
    "\n",
    "ExponentialLR \n",
    "\n",
    "CosineAnnealingLR\n",
    "\n",
    "ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ee62e-27e8-4cdc-85c3-d3ffe0e4a40a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d80e5-3365-470e-83be-5117f3a821dd",
   "metadata": {},
   "source": [
    "# 训练\n",
    "## 搭建训练循环\n",
    "- 核心思想：\n",
    "    - 1.前向传播 (forward)\n",
    "    - 2.计算损失 (loss)\n",
    "    - 3.反向传播 (backward)\n",
    "    - 4.参数更新 (optimizer.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6ea90-85eb-4975-9a9f-0718586aa8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0  # 设置初始参数\n",
    "\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device) # 传递到 GPU上运算\n",
    "\n",
    "        optimizer.zero_grad()         # 梯度清零\n",
    "        output = model(data)          # 前向传播\n",
    "        loss = criterion(output, target)  # 计算损失\n",
    "        loss.backward()               # 反向传播\n",
    "        optimizer.step()              # 更新参数\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (output.argmax(1) == target).sum().item()\n",
    "\n",
    "    acc = total_correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2bb2c-8987-466f-affb-635253fdb4a2",
   "metadata": {},
   "source": [
    "## 通用的模型训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7414a5-281c-474a-b214-6798eacb3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_fn, \n",
    "                optimizer, device, num_epochs=10, scheduler=None, \n",
    "                save_dir='path', early_stopping_patience=None):\n",
    "    \"\"\"\n",
    "    通用的模型训练循环\n",
    "    \n",
    "    参数:\n",
    "        model: 要训练的模型\n",
    "        train_loader: 训练数据加载器\n",
    "        val_loader: 验证数据加载器\n",
    "        loss_fn: 损失函数\n",
    "        optimizer: 优化器\n",
    "        device: 设备 (cpu/cuda)\n",
    "        num_epochs: 训练轮数\n",
    "        scheduler: 学习率调度器 (可选)\n",
    "        save_dir: 检查点保存目录\n",
    "        early_stopping_patience: 早停耐心值 (可选)\n",
    "    \n",
    "    返回:\n",
    "        训练历史记录，包含损失和准确率\n",
    "    \"\"\"\n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 初始化历史记录\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # 早停相关变量\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0 # 初始参数\n",
    "        train_batches = 0\n",
    "        \n",
    "        # 使用tqdm添加进度条\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for data, target in train_pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += (output.argmax(1) == target).sum().item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # 更新进度条\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{train_loss/train_batches:.4f}',\n",
    "                'Acc': f'{100*train_correct/(train_batches * train_loader.batch_size):.2f}%'\n",
    "            })\n",
    "        \n",
    "        # 计算训练指标\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_acc = train_correct / len(train_loader.dataset)\n",
    "        \n",
    "        # 验证阶段\n",
    "        avg_val_loss, avg_val_acc = validate(model, val_loader, loss_fn, device)\n",
    "        \n",
    "        # 更新学习率调度器\n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(avg_val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # 记录历史\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(avg_train_acc)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(avg_val_acc)\n",
    "        \n",
    "        # 打印epoch结果\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Train Acc: {100*avg_train_acc:.2f}% | '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Acc: {100*avg_val_acc:.2f}%')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_val_loss,\n",
    "                'acc': avg_val_acc\n",
    "            }, os.path.join(save_dir, 'best_model.pth'))\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        # 早停检查\n",
    "        if early_stopping_patience and epochs_no_improve >= early_stopping_patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs!')\n",
    "            break\n",
    "    \n",
    "    # 保存最终模型\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, os.path.join(save_dir, 'final_model.pth'))\n",
    "    \n",
    "    return history\n",
    "\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    \"\"\"验证函数\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_correct += (output.argmax(1) == target).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_acc = total_correct / len(loader.dataset)\n",
    "    \n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设已有以下组件\n",
    "    # model = YourModel()\n",
    "    # train_loader = DataLoader(...)\n",
    "    # val_loader = DataLoader(...)\n",
    "    # loss_fn = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 训练模型\n",
    "    # history = train_model(\n",
    "    #     model=model,\n",
    "    #     train_loader=train_loader,\n",
    "    #     val_loader=val_loader,\n",
    "    #     loss_fn=loss_fn,\n",
    "    #     optimizer=optimizer,\n",
    "    #     device=device,\n",
    "    #     num_epochs=50,\n",
    "    #     scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1),\n",
    "    #     save_dir='model_checkpoints',\n",
    "    #     early_stopping_patience=5\n",
    "    # )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c727ec2-b29e-42b8-bd69-a3e29a436d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757db1b-9c2f-4f79-aeba-db7406648afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7dde72-b740-428c-ae58-28c1156d4e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc2befc-86ad-44dd-87d3-2bd5fa88a213",
   "metadata": {},
   "source": [
    "# 3.1、核心概念：torch.optim 是什么？\n",
    "`torch.optim `是一个包含了多种优化算法 (Optimization Algorithms) 的包。\n",
    "\n",
    "在神经网络的训练过程中，我们通过以下步骤进行：\n",
    "- `1、前向传播 (Forward Pass)`：将数据输入模型，得到预测结果。\n",
    "\n",
    "- `2、计算损失 (Compute Loss)`：将预测结果与真实标签进行比较，计算出损失值（即模型犯了多大的错）。\n",
    "\n",
    "- `3、反向传播 (Backward Pass)`：根据损失值，计算出模型中每个参数（权重和偏置）的梯度 (`gradient`)。梯度指明了参数应该朝哪个方向调整才能使损失变小。\n",
    "\n",
    "- `4、更新参数 (Update Parameters)`：这就是 `torch.optim` 发挥作用的地方。优化器根据计算出的梯度，使用特定的优化算法（如 `SGD`, `Adam`）来更新模型的每一个参数，从而使损失函数的值逐渐减小。\n",
    "\n",
    "简单来说，如果说损失函数是“地图”，梯度是“方向盘”，那么 优化器就是“引擎”，它决定了我们如何驱动模型参数这辆“车”，一步步驶向地图上的最低点（损失最小化）。\n",
    "\n",
    "---\n",
    "\n",
    "## torch.optim 的核心使用三步法\n",
    "在任何 PyTorch 训练循环中，使用优化器的步骤都遵循一个固定的“三步法”模式。\n",
    "\n",
    "### 1. 初始化优化器 (Initialize the Optimizer)\n",
    "在训练开始之前，你需要创建一个优化器实例。创建时，你需要告诉它两件事：\n",
    "\n",
    "- 1 `要优化的参数是哪些？` 通常是模型的所有参数，通过 `model.parameters()` 来获取。\n",
    "\n",
    "- 2 `使用什么超参数？` 最重要的超参数是学习率 (learning rate, `lr`)，它控制了每次参数更新的步长。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97db19-0b41-4e16-9d5d-c614b7c9dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "# 假设 model 是一个已经定义好的 nn.Module 实例\n",
    "model = nn.Linear(10, 2) \n",
    "\n",
    "# 创建一个 SGD 优化器\n",
    "# 第一个参数：告诉优化器需要更新的参数\n",
    "# lr：设置学习率\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c299f09-66d0-49fb-b143-f9be18db8858",
   "metadata": {},
   "source": [
    "### 2. 梯度清零 (optimizer.zero_grad())\n",
    "在计算新一轮梯度之前，必须手动将上一步的梯度清零。\n",
    "\n",
    "- `为什么需要清零？` 因为 PyTorch 的设计是，每次调用 `loss.backward()` 时，计算出的梯度会累加到之前的梯度上。如果不清零，梯度会越积越大，导致错误的更新方向。\n",
    "\n",
    "- `调用时机`：通常在每个训练迭代（`batch`）的开始。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 更新参数 (optimizer.step())\n",
    "当梯度计算完毕（即调用 `loss.backward()` 之后），调用 `optimizer.step()`。\n",
    "\n",
    "- `作用`：优化器会遍历所有它管理的参数，并根据存储在这些参数` .grad` 属性中的梯度，使用其内部定义的优化算法来更新参数的值。\n",
    "#### 这三步在训练循环中的位置如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01eaa3-48e5-4af8-b84f-0995b4f97e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 train_loader 是我们的数据加载器\n",
    "for inputs, labels in train_loader:\n",
    "    # 1. 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 2. 前向传播\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 3. 计算损失\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 4. 反向传播，计算梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. 更新参数\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d75e6-e418-4d6f-82c7-963eafba4646",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "## 常见优化器详解\n",
    "torch.optim 提供了多种优化器，我们介绍最常用也是最重要的几个。\n",
    "\n",
    "---\n",
    "### 1. optim.SGD (随机梯度下降)\n",
    "最基础、最经典的优化器。\n",
    "\n",
    "- `核心思想`：参数沿着梯度的反方向前进一步。步子的大小由学习率 `lr` 控制。\n",
    "\n",
    "公式简化版: weight=weight−learning_rate\n",
    "timesgradient\n",
    "\n",
    "- 常用参数:\n",
    "\n",
    "  - **`params`**: 模型参数。\n",
    "\n",
    "  - **`lr`**: 学习率。\n",
    "\n",
    "  - **`momentum`**: 动量。引入动量可以帮助加速梯度下降，并冲出局部最优。它模拟了物理世界中物体的惯性，使得更新方向不仅取决于当前梯度，也受之前更新方向的影响。`通常设为 0.9`。\n",
    "\n",
    "  - **`weight_decay`**: 权重衰减（`L2` 正则化）。用于防止过拟合，通过给损失函数增加一个惩罚项来限制模型权重的大小。\n",
    "\n",
    "---\n",
    "### 2. optim.Adam (Adaptive Moment Estimation)\n",
    "目前最流行、最常用的优化器之一，通常作为各种任务的默认首选。\n",
    "\n",
    "- `核心思想`：它是一种自适应学习率算法。它不满足于所有参数共享同一个学习率，而是为每个参数计算自适应的学习率。\n",
    "\n",
    "- `优点`：结合了 `Momentum` 和 `RMSprop` 两种算法的思想，能够快速收敛，并且对超参数的选择不如 `SGD` 那么敏感。\n",
    "\n",
    "- 常用参数:\n",
    "\n",
    "  - **`lr`**: 初始学习率。\n",
    "\n",
    "  - **`betas`**: 用于计算梯度的一阶和二阶矩估计的系数，通常保持默认值 `(0.9, 0.999)`。\n",
    "\n",
    "  - **`eps`**: 为了增加数值稳定性而加到分母的一个小项，通常保持默认值 `1e-8`。\n",
    "\n",
    "  - **`weight_decay`**: 权重衰减。\n",
    "\n",
    "---\n",
    "### 3. optim.AdamW\n",
    "`AdamW` 是对 `Adam` 中权重衰减实现方式的改进版本。在 `Adam` 中，权重衰减与梯度更新耦合在一起，效果有时并不理想。`AdamW` 将其解耦，在许多任务中（尤其是 `NLP` 领域的 `Transformer` 模型）表现得比 `Adam` 更好，正在成为新的标准。\n",
    "\n",
    "---\n",
    "### 4. optim.Adagrad (Adaptive Gradient Algorithm)\n",
    "Adagrad 是最早期的自适应学习率算法之一。\n",
    "\n",
    "- 名称解读 \\\n",
    "Adaptive Gradient：自适应梯度。\n",
    "\n",
    "- 核心思想 \\\n",
    "`Adagrad` 的核心在于，它会记录下到目前为止，每个参数所有梯度值的平方和。在更新参数时，学习率会除以这个累积值的平方根。\n",
    "\n",
    "- 直观理解：\n",
    "\n",
    "  - 如果一个参数的梯度一直很大（经常被更新），那么它的梯度平方和就会很大，导致分母很大，从而使得它的有效学习率变小。\n",
    "\n",
    "  - 如果一个参数的梯度一直很小或很稀疏（偶尔被更新），那么它的梯度平方和就会很小，导致分母很小，从而使得它的有效学习率变大。\n",
    "\n",
    "`优点`\n",
    "   - 自适应学习率：能够自动为不同参数调整学习率，无需手动调整。\n",
    "\n",
    "   - 特别适合稀疏数据：在处理像词嵌入 (word embeddings) 或推荐系统这类稀疏特征时非常有效。因为稀疏特征对应的参数不常被更新，Adagrad 会给予它们较高的学习率。\n",
    "\n",
    "`致命弱点` \n",
    "   - `Adagrad` 的主要问题在于其分母中的梯度平方和是单调递增的。随着训练的进行，这个累积值会越来越大，永不减小。\n",
    "\n",
    "   - `后果`：训练后期，几乎所有参数的有效学习率都会变得无限接近于 0，导致模型过早地停止学习，无法达到最优解。这就像一辆车，开着开着油门就自动踩到底，再也加不上速了。\n",
    "\n",
    "`适用场景`\n",
    " - 主要用于处理稀疏数据，如自然语言处理中的词嵌入任务。\n",
    "\n",
    " - 在现代的深度学习任务中（如计算机视觉），由于其学习率会过早衰减的问题，现在已经较少使用。\n",
    "\n",
    "\n",
    "---\n",
    "### optim.RMSprop (Root Mean Square Propagation)\n",
    "为了解决 `Adagrad` 学习率急剧下降的问题，Geoff Hinton 提出了 `RMSprop` 算法。它是 `Adam` 优化器的重要组成部分。\n",
    "\n",
    "`名称解读` \\\n",
    "Root Mean Square Propagation：均方根传播。\n",
    "\n",
    "`核心思想` \\\n",
    "`RMSprop` 对 `Adagrad` 做了一个简单而巧妙的修改：它不再累加所有的历史梯度平方，而是计算一个梯度的平方的指数移动平均值 (Exponentially Moving Average)。\n",
    "\n",
    "`直观理解`：\n",
    "\n",
    "   - 指数移动平均值会给予最近的梯度更高的权重，而逐渐“忘记”久远的梯度。\n",
    "\n",
    "   - 这使得分母不会无限地、单调地增长。它会根据最近的梯度情况进行动态调整。如果最近梯度大，分母就大，学习率就小；如果最近梯度小，分母就小，学习率就大。\n",
    "\n",
    "这样一来，RMSprop 解决了 Adagrad 学习率过早消失的问题，让训练可以持续进行。\n",
    "\n",
    "`优点` \\\n",
    "   - 解决了 `Adagrad` 的弊端：通过使用指数移动平均，避免了学习率过早衰减，使得算法在非凸优化问题上表现更好。\n",
    "\n",
    "   - 收敛速度快：作为一种自适应学习率算法，通常比 `SGD` 收敛更快。\n",
    "\n",
    "   - 超参数较少：通常只需要调整学习率 lr 和 alpha (平滑常数)。\n",
    "\n",
    "`缺点`  \\\n",
    "   - 仍然需要设置一个全局的学习率 lr。\n",
    "\n",
    "   - 在某些情况下，Adam 或 AdamW 可能会提供更稳定和更好的性能。\n",
    "\n",
    "`适用场景`  \\\n",
    " - 一个非常通用和强大的优化器，尤其在循环神经网络 (RNNs) 相关的任务上表现出色。\n",
    "\n",
    " - 当你发现 Adam 在你的任务上效果不佳时，RMSprop 是一个非常值得尝试的替代方案。\n",
    "\n",
    "`进化之路`：你可以这样理解优化器的发展： \\\n",
    "`SGD` -> `SGD with Momentum` -> `Adagrad` (引入自适应思想) -> `RMSprop` (修复 Adagrad 缺陷) -> `Adam` (结合 RMSprop 和 Momentum) -> `AdamW` (改进 Adam 的权重衰减)。\n",
    "\n",
    "---\n",
    "## 优化器对比表\n",
    "\n",
    "| 优化器 (Optimizer) | 核心思想 (Core Idea) | 学习率特点 (Learning Rate) | 优点 (Pros) | 缺点 (Cons) | 适用场景 (Best Use Cases) |\n",
    "|--------------------|----------------------|----------------------------|------------|------------|----------------------------|\n",
    "| SGD | 沿着梯度反方向更新参数。可加入动量（Momentum）来增加惯性。 | 全局固定：所有参数共享同一个学习率。 | 简单可靠，经过充分调优后可能找到泛化能力更好的解。 | 对学习率敏感，收敛慢，容易陷入局部最优或鞍点。 | 作为研究基线；当你有足够的时间和计算资源进行精细调参时。 |\n",
    "| Adagrad | 为每个参数累加所有历史梯度的平方，并用它来调整学习率。 | 自适应：每个参数有独立学习率。梯度大的参数，学习率衰减快。 | 对稀疏数据非常有效（如NLP词嵌入）。 | 学习率单调递减，训练后期会变得极小，导致过早停止学习。 | 主要用于稀疏数据任务，在深度学习中已较少使用。 |\n",
    "| RMSprop | 使用梯度平方的指数移动平均来调整学习率。 | 自适应：每个参数有独立学习率，但解决了学习率消失问题。 | 解决了 Adagrad 的弊端，收敛速度快。 | 仍然需要手动设置学习率。 | 在循环神经网络（RNN）上表现出色；可作为 Adam 的替代方案。 |\n",
    "| `Adam` | 结合了 Momentum 和 RMSprop。同时使用梯度的一阶矩（动量）和二阶矩（自适应学习率）。 | 自适应：每个参数有独立学习率，且更新带有惯性。 | 收敛速度极快，对超参数不敏感，`几乎适用于所有任务`。 | 权重衰减的实现方式存在问题；可能收敛到泛化能力较差的“锐利”最小值。 | 曾经的默认首选。适合快速原型设计和大多数深度学习任务。 |\n",
    "| `AdamW` | 修复了 Adam 的权重衰减问题，将其与梯度更新解耦。 | 自适应：与 Adam 相同，但正则化效果更好。 | 拥有 Adam 的所有优点，同时通过修正权重衰减，通常能获得更好的泛化性能和更低的训练损失。 | `几乎没有明显缺点`。 | 当前推荐的`默认首选`。尤其在 Transformer、BERT 等大型模型上表现优异。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7be0d-d2da-4866-8ce1-db410add3709",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e68ce-73c9-44a2-8449-2856bbcedd42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf9087-b5b6-4501-b5a1-e711b0c50c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "243cb312-972b-4474-bbca-a5ed943fc861",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# 3.2、学习率调整策略 (torch.optim.lr_scheduler)\n",
    "torch.optim.lr_scheduler。\n",
    "\n",
    "- `核心概念`：为什么需要学习率调整策略？ \\\n",
    "在之前的 `torch.optim` 讲解中，我们为优化器设置了一个固定的学习率（learning rate, `lr`）。然而，在整个训练过程中使用一个固定的学习率并非最优选择。\n",
    "\n",
    "- 一个理想的学习率策略应该是这样：\n",
    "\n",
    "  - 训练初期: 模型参数是随机初始化的，离最优解很远。此时，我们希望使用一个较大的学习率，让模型能够快速地向最优解的方向收敛，就像下山初期大步流星。\n",
    "\n",
    "  - 训练中期: 当模型接近最优解时，一个大的学习率可能会导致模型在最优解附近“来回震荡”，无法精确收敛。\n",
    "\n",
    "  - 训练后期: 我们需要一个较小的学习率，让模型能够“小心翼翼”地在最优解的“山谷”中进行微调，从而找到那个最低点。\n",
    "\n",
    "`torch.optim.lr_scheduler` 就是为了实现这种动态调整学习率的需求而设计的。它提供了多种预设的策略，可以根据训练的进度（如 epoch 数）或其他指标（如验证集损失）来自动调整优化器中的学习率。\n",
    "\n",
    "---\n",
    "### 使用方法:\n",
    "\n",
    "使用任何 lr_scheduler 都遵循一个简单的三步流程：\n",
    "\n",
    "- 1 、`创建优化器 (Optimizer)`：首先，像往常一样创建一个优化器。\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "- 2 、`创建调度器 (Scheduler)`：然后，用创建好的优化器来实例化一个调度器。\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR  \n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "- 3 、`在训练循环中更新调度器 (scheduler.step())`: 在训练循环的适当位置调用 `scheduler.step()` 来更新学习率。\n",
    "\n",
    "- `step()` 的调用时机是关键：\n",
    "\n",
    "  - 大多数调度器: 都是基于 epoch 进行更新的，所以 `scheduler.step()` 应该在每个 `epoch` 的训练循环之后调用。\n",
    "\n",
    "  - 少数调度器: 如 `CosineAnnealingLR`, `OneCycleLR` 等，可以基于 `batch`/`iteration` 进行更新，此时 `scheduler.step()` 应该在每个 `batch` 的训练步骤之后调用。\n",
    "\n",
    "  - 特殊调度器: `ReduceLROnPlateau` 是基于验证指标更新的，所以它的 `step()` 需要传入该指标的值，例如 scheduler.step(validation_loss)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac752d-8917-47ca-8ac5-6c032ca898ff",
   "metadata": {},
   "source": [
    "---\n",
    "### 常用 Scheduler:\n",
    "下面我们介绍几种最常用、最有代表性的调度器。\n",
    "\n",
    "## 1. StepLR (阶梯式下降)\n",
    "- 一句话解释: 每隔 `step_size` 个 epoch，就将当前学习率乘以一个 `gamma` 因子。\n",
    "\n",
    "- `核心参数`:\n",
    "\n",
    "   - `optimizer`: 关联的优化器。\n",
    "\n",
    "   - `step_size (int)`: 更新学习率的间隔 `epoch` 数。\n",
    "\n",
    "   - `gamma (float)`: 学习率衰减的乘法因子（例如，0.1 代表衰减为原来的1/10）。\n",
    "\n",
    "- 学习率曲线: 呈阶梯状下降。\n",
    "\n",
    "- 适用场景: 简单有效，适合作为入门和基线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2708368e-2b51-4297-85d2-018b4776ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每 10 个 epoch，学习率变为原来的 0.5 倍\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65de94f-8c36-4399-be8d-35a39b0de571",
   "metadata": {},
   "source": [
    "## 2. MultiStepLR (多阶梯式下降)\n",
    "- 一句话解释: 在预先设定好的一系列 `milestones` (里程碑) `epoch` 处，将学习率乘以 `gamma`。\n",
    "\n",
    "- `核心参数`:\n",
    "\n",
    "    - `milestones (list of ints)`: 一个包含 epoch 索引的列表，在这些 epoch 处进行学习率衰减。\n",
    "\n",
    "    - `gamma (float)`: 衰减因子。\n",
    "\n",
    "- 学习率曲线: 不规则的阶梯状，更具灵活性。\n",
    "\n",
    "- 适用场景: 许多经典论文（如 ResNet）中使用的标准策略，当你对训练过程有比较明确的预期时（例如，你知道模型大概在第60和第90个epoch会遇到瓶颈）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd4cbb-1d5d-4cb2-8072-c0d85cf609bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在第 60, 90, 120 个 epoch 时，学习率分别衰减为原来的 0.1 倍\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 90, 120], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6cb454-4a59-4f22-a4b3-e89013e6f065",
   "metadata": {},
   "source": [
    "## 3. CosineAnnealingLR (余弦退火)\n",
    "- 一句话解释: 学习率在一个周期（`T_max` 个 epoch）内，按照余弦函数的形式从初始值平滑地下降到最小值 `eta_min`。\n",
    "\n",
    "- `核心参数`:\n",
    "\n",
    "    - `T_max (int)`: 一个学习率周期的长度（以 `epoch`(周期) 或 `iteration`(迭代次数) 为单位）。\n",
    "\n",
    "    - `eta_min (float)`: 学习率的下限，默认为 0。\n",
    "\n",
    "- 学习率曲线: 平滑的余弦半周期曲线。\n",
    "\n",
    "`特点`: 这是一种非常流行且高效的策略。它在前期保持较高的学习率，然后缓慢下降，在周期末期学习率变得非常小，有助于模型找到更优的解。还可以配合`“热重启”`（Warm Restarts）使用，让模型有机会跳出局部最优。\n",
    "\n",
    "`适用场景`: 当前各种深度学习任务中的 `SOTA` (State-of-the-art) 选择，尤其在大型模型训练中表现优异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1873760-cadd-4473-91b4-3217450d04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 100 个 epoch 内，学习率从初始值退火到 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5826da5-06e2-4e43-8618-5cf6a828fee0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## 4. ReduceLROnPlateau (在平台期降低学习率)\n",
    "- 一句话解释: 监控一个指定的指标（如验证集损失），如果该指标在 `patience` 个 `epoch`abs 内没有改善，就降低学习率。\n",
    "\n",
    "- `核心参数`:\n",
    "\n",
    "    - `mode (str)`: '`mi`' 或 '`max`'。监控指标是越小越好 ('min'，如损失) 还是越大越好 ('max'，如准确率)。\n",
    "\n",
    "    - `factor (float)`: 学习率衰减因子 (`new_lr` = `lr` * `factor`)。\n",
    "\n",
    "    - `patience (int)`: 容忍多少个 `epoch` 指标没有改善。\n",
    "\n",
    "    - `verbose (bool)`: 如果为 `True`，每次更新时会打印一条信息。\n",
    "\n",
    "`特点`: 这是一种`“响应式”`策略，而不是预设的。它根据模型的实际表现来调整学习率，非常智能。\n",
    "\n",
    "- `step()` 调用方式: 特别注意，它需要在验证阶段后调用，并传入监控的指标值：`scheduler.step(validation_loss)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30f079-c1bc-4445-ba56-e12fbee738e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 监控验证集损失，如果连续 5 个 epoch 损失没有下降，则学习率变为原来的 0.2 倍\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f74ac-97bd-4575-804e-7468e2098dce",
   "metadata": {},
   "source": [
    "## 完整代码演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00070e0-62a7-4831-9cc3-36c51502d097",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "\n",
    "# 1. 虚拟的模型、数据和优化器\n",
    "model = nn.Linear(10, 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 2. 创建调度器 (选择一个)\n",
    "# scheduler = StepLR(optimizer, step_size=10, gamma=0.1) # 方案一：阶梯下降\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0.001) # 方案二：余弦退火\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "print(\"开始训练...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # ----- 模拟训练循环 -----\n",
    "    for i in range(10): # 假设有 10 个 batch\n",
    "        # ... your training steps ...\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        pass # 此处省略训练细节\n",
    "    # -----------------------\n",
    "    \n",
    "    # 在每个 epoch 结束后，更新学习率\n",
    "    scheduler.step()\n",
    "    \n",
    "    # 打印当前学习率以观察变化\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, 当前学习率: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8845a87-916c-4b48-a1e4-0cce43f91714",
   "metadata": {},
   "source": [
    "### 如何选择？\n",
    "- `简单基线`: `StepLR` 或 `MultiStepLR` 是非常好的起点，稳定可靠。\n",
    "\n",
    "- `追求更高性能`: `CosineAnnealingLR` 是目前冲击 `SOTA` 性能的首选策略之一。\n",
    "\n",
    "- `需要自适应调整`: 当你不确定训练多少个 epoch 会遇到瓶颈时，`ReduceLROnPlateau` 是一个非常智能和方便的选择，它让模型自己决定何时降低学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e828db-a749-4d28-a0fe-4d3a76267b56",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0462d-f4e7-4908-a308-a770185fc70f",
   "metadata": {},
   "source": [
    "## 总结\n",
    "- `torch.optim` 是 PyTorch 的优化核心，负责根据梯度更新模型参数。\n",
    "\n",
    "- 掌握 `optimizer.zero_grad()` -> `loss.backward()` -> `optimizer.step()` 的`“三步曲”`是编写任何 PyTorch 训练代码的基础。\n",
    "\n",
    "- `SGD` 是基础，`Adam` 和 `AdamW` 是当前最常用、效果最好的优化器之一。\n",
    "\n",
    "配合使用 `torch.optim.lr_scheduler` 动态调整学习率是提升模型最终性能的标准实践。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e83d40-36b1-43f8-a6b5-2d3acebdd71c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
