{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c305c0b6-45b3-47a1-8e90-5ef1bded9128",
   "metadata": {},
   "source": [
    "# 🐍 PyTorch `torch` 模块速查表\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. 张量创建（Tensor creation）\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.tensor(data)` | 从数据创建张量 |\n",
    "| `torch.empty(size)` | 未初始化的张量 |\n",
    "| `torch.zeros(size)` | 全 0 张量 |\n",
    "| `torch.ones(size)` | 全 1 张量 |\n",
    "| `torch.full(size, fill_value)` | 固定值填充，size：张量的形状,fill_value:填充值 |\n",
    "| `torch.arange(start, end, step)` | 范围序列 |\n",
    "| `torch.linspace(start, end, steps)` | 均匀分布序列 |\n",
    "| `torch.logspace(start, end, steps)` | 对数分布序列 |\n",
    "| `torch.eye(n)` | 单位矩阵（对角线元素为1，其余为0），n:指定输出矩阵的行数，m=None:指定输出矩阵的列数 |\n",
    "| `torch.rand(size)` | 均匀分布 [0,1) |\n",
    "| `torch.randn(size)` | 标准正态分布 |\n",
    "| `torch.randint(low, high, size)` | 均匀整数分布，low：随机整数下限(包含)，high：随机整数上限(不包含)，size：张量的形状 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. 张量操作（Tensor ops）\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.reshape(t, shape)` | 改变形状。t：要改变形状的输入张量。shape：目标形状，可以是tuple或list |\n",
    "| `torch.view(t, shape)` | 改变形状（共享内存） |\n",
    "| `torch.transpose(t, dim0, dim1)` | 交换维度(返回新的张量,共享内存)。t:要交换维度的输入张量。 dim0:要交换的第一个维度索引（从 0 开始）,dim1:要交换的第二个维度索引（从 0 开始）|\n",
    "| `torch.permute(t, dims)` | 任意维度换位(返回新的张量,共享内存)。dims: 新维度顺序的元组（必须包含所有维度的排列）|\n",
    "| `torch.cat([t1, t2], dim)` | 拼接（共享内存） t1、t2：要连接的张量(形状必须相同，除了dim维度。dim：沿着哪个维度拼接，默认dim=0)|\n",
    "| `torch.stack([t1, t2], dim)` | 堆叠（创建新维度） t1、t2：要堆叠的张量（形状必须完全相同）。dim：新维度的插入位置（默认dim=0）|\n",
    "| `torch.chunk(t, chunks, dim)` | 均匀分块(返回一个张量列表,共享内存)。chunks：分割的块数（不是每一块的大小）。 |\n",
    "| `torch.split(t, split_size, dim)` | 按大小分块(返回一个张量列表,共享内存)。split_size：每块的大小(int)或各块大小列表(list) |\n",
    "| `torch.tensor_split(t,indices_or_sections,dim)` | 不均匀分割。分割策略：(int:分割的块数，或, list of ints:指定分割位置的索引列表) |\n",
    "| `torch.squeeze(t)` | 去掉长度为 1 的维度 |\n",
    "| `torch.unsqueeze(t, dim)` | 增加维度(返回的是原始张量的视图（共享存储）):用于在指定位置插入长度为1的维度 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 3. 数学运算（Math ops）\n",
    "| 类别 | 方法 |\n",
    "|------|------|\n",
    "| 基本运算 |逐元素：加减乘除 `add`：+ / `sub`：- / `mul`：* / `div`：/ 。原地操作：函数_(),例如：add_()|\n",
    "| 幂与根 | `pow`：幂运算/ `sqrt`： 平方根。/ 原地操作：函数_(),例如：pow_(2)|\n",
    "| 指数对数 | `exp`： 自然指数函数​（以 e为底）/ `log`：自然对数​（以 e为底） / `log10` / `log2` |\n",
    "| 三角函数 | `sin` / `cos` / `tan` / `atan2` |\n",
    "| 统计 | `mean`： 平均值/ `median`：中位数 / `mode`：众数 / `std`： 标准差/ `var`： 方差|\n",
    "| 归约 | `sum`： 求和/ `prod`： 连乘/ `cumsum`： 累积和/ `cumprod`： 累积乘积|\n",
    "| 矩阵运算 | `mm`：严格的二维矩阵乘法 / `matmul`：广义矩阵乘法​（支持广播和高维张量） / `bmm`：批量矩阵乘法(3D张量) |\n",
    "| 线性代数 | `inverse`： 计算方阵的逆矩阵/ `det`：计算方阵的行列式 / `svd`：计算矩阵的奇异值分解 / `linalg.eig`：计算方阵的特征值与特征向量 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 4. 比较 & 逻辑\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.eq` / `torch.ne` | 相等 / 不等 / 举例：torch.eq(x,y) 等价写法：x == y|\n",
    "| `torch./absgt` / `torch.ge` | 大于 / 大于等于 /  举例：torch.gt(x,y) 等价写法：x > y|\n",
    "| `torch.lt` / `torch.le` | 小于 / 小于等于 /  举例：torch.le(x,y) 等价写法：x <= y |\n",
    "| `torch.all` / `torch.any` |all：判断张量中所有元素是否均为 True​（或非零/非空）/any:判断张量中是否存在至少一个 True​（或非零/非空） |\n",
    "| `torch.isfinite`：：检测张量中的有限数值​（非无穷且非NaN） / `torch.isnan` / `torch.isinf` | 判断数值状态 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 5. 随机数\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.manual_seed(seed)` | 固定随机种子 |\n",
    "| `rand` / `randn` / `randint` | 常用分布:均匀分布[0,1)/标准准态分布（0，1）/离散均匀分布[最小值，最大值） |\n",
    "| `bernoulli` | 伯努利分布 |\n",
    "| `normal(mean, std)` | 正态分布 |\n",
    "| `multinomial` | 多项分布 |\n",
    "| `randperm(n)` | 随机排列 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 6. 自动求导（Autograd）\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.autograd.backward(tensors, grad_tensors)` | 反向传播 |\n",
    "| `torch.autograd.grad(outputs, inputs)` | 手动求导 |\n",
    "| `torch.set_grad_enabled(mode)` | 控制梯度开关 |\n",
    "| `torch.no_grad()` | 禁用梯度（推理时用） |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 7. 神经网络 & 优化\n",
    "| 模块 | 说明 |\n",
    "|------|------|\n",
    "| `torch.nn` | 神经网络层、损失函数 |\n",
    "| `torch.nn.functional` | 函数式 API (如 `F.relu`) |\n",
    "| `torch.optim` | 优化器 (SGD, Adam...) |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 8. 数据加载\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.utils.data.Dataset` | 数据集抽象类 |\n",
    "| `torch.utils.data.DataLoader` | 数据批加载 |\n",
    "| `torchvision.datasets` | 常用图像数据集 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 9. 设备与并行\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.device(\"cpu\"/\"cuda\")` | 指定设备 |\n",
    "| `torch.cuda.is_available()` | CUDA 是否可用 |\n",
    "| `torch.cuda.manual_seed(seed)` | CUDA 随机种子 |\n",
    "| `torch.cuda.empty_cache()` | 清理显存 |\n",
    "| `torch.nn.DataParallel` | 多 GPU |\n",
    "| `torch.distributed` | 分布式训练 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 10. 保存与加载\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.save(obj, path)` | 保存模型/张量 |\n",
    "| `torch.load(path)` | 加载 |\n",
    "| `torch.jit` | TorchScript 序列化 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 11. 其他常用\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.clone(t)` | 克隆张量 |\n",
    "| `torch.detach()` | 去掉梯度信息 |\n",
    "| `torch.numel(t)` | 元素个数 |\n",
    "| `torch.size(t)` / `t.shape` | 形状 |\n",
    "| `torch.topk(t, k)` | 取前 k 大值 |\n",
    "| `torch.argmax` / `torch.argmin` | 最大/最小索引 |\n",
    "| `torch.where(cond, x, y)` | 条件选择 |\n",
    "| `torch.nonzero(t)` | 非零索引 |\n",
    "\n",
    "---\n",
    "### ✅ 总结：**torch = 张量创建 + 操作 + 计算 + 自动求导 + 神经网络 + 优化 + 数据加载 + 并行 + 序列化**。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092ab13-e325-4458-9d87-149cc99a17e0",
   "metadata": {},
   "source": [
    "# 1.1、torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485fd58-c695-4a92-a923-b7e8c8d9676e",
   "metadata": {},
   "source": [
    "# 🧭 torch.tensor() \n",
    "\n",
    "## 📌1) 基本签名与作用\n",
    "\n",
    "- torch.tensor(`data`, *, `dtype`=None, `device`=None, `requires_grad`=False, `pin_memory`=False)\n",
    "\n",
    "✅作用：把 `Python` 数据（标量、list/嵌套list、tuple、NumPy 数组、其他张量等）转换成一个新的 `torch.Tensor`。\n",
    "\n",
    "- 核心特性：默认会拷贝数据（与 `torch.as_tensor` 不同），因此`不会`与原始输入`共享内存`。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌2) 主要参数\n",
    "\n",
    "| 参数              | 说明                                                       |\n",
    "| --------------- | -------------------------------------------------------- |\n",
    "| `data`          | Python 数据（标量、list/嵌套 list、tuple、NumPy、Tensor）            |\n",
    "| `dtype`         | 数据类型（默认：整数→`torch.int64`，浮点→`torch.get_default_dtype()`） |\n",
    "| `device`        | 存放设备，如 `'cpu'`、`'cuda:0'`                                |\n",
    "| `requires_grad` | 是否参与自动求导（仅浮点/复数有效）                                       |\n",
    "| `pin_memory`    | 是否固定在页锁内存，加速 CPU→GPU 拷贝（仅对 CPU 张量有效）                     |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌3) dtype 推断与默认行为\n",
    "\n",
    "全是整数 → torch.int64\n",
    "\n",
    "含浮点 → 默认浮点类型（torch.float32，可用 torch.get_default_dtype() 查看/设置）\n",
    "\n",
    "布尔 → torch.bool\n",
    "\n",
    "混合类型 → 类型提升，如 [1, 2.5] → float32\n",
    "\n",
    "---\n",
    "\n",
    "## 📌4) 形状与维度\n",
    "\n",
    "标量 → 0 维张量（shape == ()）。\n",
    "\n",
    "扁平 list → 1 维；嵌套 list → 多维。必须是规则矩阵（每层列表长度一致），否则报错。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌5) 拷贝/共享内存：与其他构造器的区别\n",
    "\n",
    "这块是很多人最容易踩坑的地方：\n",
    "| 构造方式                   | 是否复制           | 与输入共享内存                              | 是否保留梯度图（当输入是 Tensor）                            |\n",
    "| ---------------------- | -------------- | ------------------------------------ | ----------------------------------------------- |\n",
    "| `torch.tensor(x)`      | **总是尽量复制**     | 否                                    | **不保留**（会断开计算图；且默认 `requires_grad=False`）       |\n",
    "| `torch.as_tensor(x)`   | 尽量**不复制**      | 尽可能共享（尤其是来自 `np.ndarray` 或已有 Tensor） | 是（若 `x` 本身是 Tensor，就直接返回它，保留图与 `requires_grad`） |\n",
    "| `torch.from_numpy(nd)` | **不复制**（满足条件时） | 是（与 `nd` 共享内存）                       | 不涉及计算图；对 `nd` 的写会反映到张量，反之亦然                     |\n",
    "\n",
    "| 方法                          | 是否拷贝  | 是否共享内存 | 梯度计算图             |\n",
    "| --------------------------- | ----- | ------ | ----------------- |\n",
    "| `torch.tensor(x)`           | ✅ 拷贝  | ❌ 不共享  | ❌ 不保留（断开）         |\n",
    "| `torch.as_tensor(x)`        | 尽量不拷贝 | 尽量共享   | ✅ 保留              |\n",
    "| `torch.from_numpy(ndarray)` | ❌ 不拷贝 | ✅ 共享   | 无计算图；与 NumPy 共享数据 |\n",
    "| `x.clone()`                 | ✅ 拷贝  | ❌ 不共享  | ✅ 保留梯度属性          |\n",
    "| `x.detach()`                | ❌ 不拷贝 | ✅ 共享   | ❌ 切断梯度            |\n",
    "\n",
    "\n",
    "### 示例要点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50a6fe-315c-4c92-9d10-1e20416176d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array([1, 2, 3], dtype=np.int64)  # a 指向一块内存：[1, 2, 3]\n",
    "\n",
    "t1 = torch.tensor(a)        # ✅ 复制数据（新建内存）\n",
    "t2 = torch.as_tensor(a)     # 🔗 共享内存（不复制）\n",
    "t3 = torch.from_numpy(a)    # 🔗 共享内存（不复制）\n",
    "\n",
    "a[0] = 999  # 修改原始 NumPy 数组的第一个元素\n",
    "\n",
    "结果：\n",
    "t1[0] 仍然是 1 → 因为 t1 是复制的，修改 a 不影响它。\n",
    "t2[0] 变成 999\n",
    "t3[0] 变成 999\n",
    "👉 因为 t2 和 t3 共享 a 的内存，所以 a[0] = 999 修改了那块内存，t2 和 t3 读取时自然看到变化。\n",
    "\n",
    "\n",
    "\n",
    "# 1) 基本创建\n",
    "a = torch.tensor([1, 2, 3])                    # int64\n",
    "b = torch.tensor([1.0, 2, 3])                  # float32\n",
    "c = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
    "\n",
    "# 2) 指定设备 & 梯度\n",
    "g = torch.tensor([0.1, 0.2], device='cuda', requires_grad=True)\n",
    "\n",
    "# 3) NumPy 互操作\n",
    "import numpy as np\n",
    "arr = np.arange(6).reshape(2,3)\n",
    "t1 = torch.tensor(arr)         # 拷贝\n",
    "t2 = torch.from_numpy(arr)     # 共享内存\n",
    "arr[0,0] = 999                 # t2[0,0] 跟着变，t1 不变\n",
    "\n",
    "# 4) 标量 / 0维张量\n",
    "s = torch.tensor(3.14)         # shape: ()\n",
    "float(s)                       # -> 3.14 (Python float)\n",
    "\n",
    "# 5) 修改默认浮点 dtype\n",
    "torch.set_default_dtype(torch.float64)\n",
    "u = torch.tensor([1.0, 2.0])   # float64\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786679ff-bf1b-4bad-9066-baae4be2a5d4",
   "metadata": {},
   "source": [
    "### 对已有 Tensor 的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bd1bc-6a20-48d0-8870-bb31944626e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = torch.tensor(x)         # 新张量，拷贝数据，计算图被断开，默认 requires_grad=False\n",
    "z = torch.as_tensor(x)      # 返回 x 本身（或视图），保留计算图与 requires_grad=True\n",
    "w = x.clone()               # 拷贝数据，保留 dtype/device，仍在同一计算图上（对 autograd 通常更安全）\n",
    "d = x.detach()              # 不拷贝数据的“视图”，但切断梯度（不再参与反向）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c501c-63ed-44de-a6d6-1353922f6445",
   "metadata": {},
   "source": [
    "### 👉小结： \n",
    "需要拷贝且不保留梯度 → torch.tensor(x)；\\\n",
    "能不拷贝就不拷贝 → torch.as_tensor(x) / torch.from_numpy(nd)；\\\n",
    "拷贝但保留梯度属性 → x.clone()（或 x.clone().detach() 视场景）。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌6) device 与 pin_memory\n",
    "\n",
    "直接指定设备：\n",
    "\n",
    "t = torch.tensor([1,2,3], device='cuda')    # 直接在 GPU 上创建\n",
    "\n",
    "若无可用 GPU 会报错。\n",
    "\n",
    "pinned memory（仅 CPU 张量）：\n",
    "\n",
    "t = torch.tensor([1,2,3], pin_memory=True)  # 之后 t.to('cuda') 更快\n",
    "\n",
    "pin_memory=True 不能用于 device='cuda' 的创建。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌7) requires_grad 与 autograd\n",
    "\n",
    "只有浮点或复数张量才能 requires_grad=True。整型/布尔会报错或被忽略。\n",
    "\n",
    "若你打算把这个张量作为可训练参数或参与可微运算，设置：\n",
    "\n",
    "w = torch.tensor([0.1, 0.2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "用 torch.tensor(tensor_input) 会把 requires_grad 复位为 False 且断开图，很多人因此梯度为 None。保持图请用 as_tensor 或 clone()。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌8) 常用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a6beb-dab2-42f8-8c37-e843c9a12b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 基本创建（自动推断）\n",
    "a = torch.tensor([1, 2, 3])                          # int64\n",
    "b = torch.tensor([1.0, 2, 3])                        # float32（默认浮点）\n",
    "c = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
    "\n",
    "# 2) 指定设备 & 梯度\n",
    "g = torch.tensor([0.1, 0.2], device='cuda', requires_grad=True)\n",
    "\n",
    "# 3) 从 NumPy（是否共享内存对比）\n",
    "import numpy as np\n",
    "arr = np.arange(6).reshape(2,3)\n",
    "t_copy = torch.tensor(arr)        # 拷贝\n",
    "t_share = torch.from_numpy(arr)   # 共享\n",
    "arr[0,0] = 999                    # 仅 t_share 受影响\n",
    "\n",
    "# 4) 标量/0维张量\n",
    "s = torch.tensor(3.14)            # shape: ()\n",
    "float(s)                          # 3.14（Python 标量）\n",
    "\n",
    "# 5) 修改默认浮点 dtype（谨慎）\n",
    "torch.set_default_dtype(torch.float64)\n",
    "u = torch.tensor([1.0, 2.0])      # 现在是 float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5c9e3-0b07-4ff7-9030-504b2c829ee3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9) 常见坑（踩雷清单）\n",
    "\n",
    "无意中断开计算图：\n",
    "用 torch.tensor(existing_tensor) 会断开图且 requires_grad=False。想保留 → existing_tensor.clone() 或直接用 as_tensor(existing_tensor)。\n",
    "\n",
    "误以为一定共享内存：\n",
    "torch.tensor(np_array) 是拷贝；要共享用 torch.from_numpy() 或 as_tensor(np_array)。\n",
    "\n",
    "在 GPU 上设 pinned memory：\n",
    "pin_memory=True 只对 CPU 张量有效；GPU 上无意义且可能报错。\n",
    "\n",
    "整数/布尔设 requires_grad=True：\n",
    "梯度只对浮点/复数有效，整型/布尔不行。\n",
    "\n",
    "不规则嵌套 list：\n",
    "形状必须规则，否则报错。\n",
    "\n",
    "默认 dtype 误判：\n",
    "整数默认 int64，浮点默认 get_default_dtype()；跨项目时若有人改过默认浮点为 float64，会导致显存翻倍或数值差异。\n",
    "\n",
    "---\n",
    "\n",
    "## 10) 什么时候不用 torch.tensor()？\n",
    "\n",
    "需要就地初始化（不从现有数据来）→ 用工厂函数更快：\n",
    "\n",
    "torch.zeros/ones/empty/full/rand/randn/arange/linspace 等，并能直接指定 dtype/device。\n",
    "\n",
    "想与 NumPy 高效互操作（共享内存）→ torch.from_numpy() / torch.as_tensor()。\n",
    "\n",
    "想保留梯度信息复制一个已有张量 → x.clone()（必要时再 .detach()）。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56a0b3-22c8-4724-b091-c2031609cdde",
   "metadata": {},
   "source": [
    "# 🚨 常见坑清单\n",
    "\n",
    "### 1、断开计算图\n",
    "\n",
    "y = torch.tensor(x)   # 会拷贝数据 & requires_grad=False\n",
    "\n",
    "→ 想保留梯度：用 x.clone() 或 torch.as_tensor(x)。\n",
    "\n",
    "### 2、误以为总是共享内存\n",
    "\n",
    "torch.tensor(np_array) 是 拷贝，要共享请用 torch.from_numpy() 或 torch.as_tensor()。\n",
    "\n",
    "### 3、在 GPU 上用 pin_memory\n",
    "pin_memory=True 仅对 CPU 张量 有效；GPU 张量会报错。\n",
    "\n",
    "### 4、整数/布尔张量设 requires_grad=True\n",
    "只有浮点/复数支持梯度，整型/布尔不行。\n",
    "\n",
    "### 5、不规则嵌套 list\n",
    "形状必须规则，否则报错。\n",
    "\n",
    "### 6、默认 dtype 被改动\n",
    "项目中若有人 torch.set_default_dtype(torch.float64)，会导致显存翻倍或数值不一致。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 建议：\n",
    "\n",
    "数据初始化 → 用 torch.zeros/ones/rand/... 更高效\n",
    "\n",
    "NumPy 转 Tensor 且需要共享 → torch.from_numpy()\n",
    "\n",
    "需要保留梯度 → x.clone()（或 x.clone().detach()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d835f3-9bbc-4564-bb6f-6a888e51b31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a671074-4549-4b16-a004-694078c17c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
