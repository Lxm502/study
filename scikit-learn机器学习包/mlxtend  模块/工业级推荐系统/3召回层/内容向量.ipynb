{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33747152-74ce-4844-aa48-0612cfb54b8b",
   "metadata": {},
   "source": [
    "# 什么是内容向量召回？\n",
    "内容向量召回是基于物品的内容特征（如文本、图像、属性等）构建向量表示，通过向量相似度计算来实现物品召回的方法。\n",
    "\n",
    "- 核心思想\n",
    "    - 将物品内容转化为数值化向量\n",
    "    - 在向量空间中度量物品相似度\n",
    "    - 基于用户历史偏好寻找相似内容\n",
    "\n",
    "---\n",
    "## 1. 概念与作用（为什么要用内容向量）\n",
    "\n",
    "- 内容向量：把物品（或用户）的内容/属性（文本、分类特征、图像、数值属性等）映射为一个低维实数向量（embedding），用向量表示语义/属性相似性。\n",
    "\n",
    "- 在召回层的作用：通过向量相似度（如余弦/内积）快速召回与用户已交互物品或用户画像相似的一堆候选（Candidate），通常作为召回（Recall）阶段的一种方式（与`协同过滤召回`、`行为共现召回`、`热门`/`规则召回`并列）。\n",
    "\n",
    "- 优点：解决新物品冷启动、易解释（基于内容相似）、能结合文本/图像/结构化属性做混合召回；工程上可离线计算并用向量索引（ANN）实时检索。\n",
    "\n",
    "- 缺点：内容特征质量决定效果；单纯基于内容可能缺乏协同信号（个性化弱），通常与协同信号融合使用。\n",
    "\n",
    "---\n",
    "## 2. 内容向量的来源（常见类型）\n",
    "\n",
    "- 稀疏` TF-IDF `/` BoW `向量（文本标题/描述）\n",
    "\n",
    "- 预训练词向量或句向量（word2vec 平均、Doc2Vec、sentence-transformers / BERT）\n",
    "\n",
    "- 图像特征向量（预训练 CNN 的池化输出）\n",
    "\n",
    "- 结构化 / 类别特征` embedding`（类别 one-hot 后 embedding 表）\n",
    "\n",
    "- 混合多模态向量（文本 + 图像 + 结构化拼接或融合后降维）\n",
    "\n",
    "- 学习得来的向量（Supervised / Metric Learning）：用有监督任务或对比学习训练得到的 embedding（更适合下游召回/排序）\n",
    "\n",
    "---\n",
    "## 3. 向量构建方法（从简单到复杂）\n",
    "\n",
    "- `TF-IDF`（简单、快速）：对 `title`/`description` 建模，适合快速 `baseline`，配合 `MinHash` / `LSH` 做近似检索。\n",
    "\n",
    "- 平均词向量（word2vec avg）：用预训练词向量平均或加权平均（按 TF-IDF 权重）得到句子/描述向量。\n",
    "\n",
    "- Sentence-BERT / Transformer Embeddings（效果好）：直接把句子/长文本映射成 384/768 维向量，语义捕获更强。\n",
    "\n",
    "- 图像 Embedding：用 ResNet、EfficientNet 等预训练网络，取 GAP 后向量。\n",
    "\n",
    "- 融合策略：简单拼接（concatenate）→ 可做 FC 层压缩；或用注意力/自适应融合（learned fusion）。\n",
    "\n",
    "- 训练策略：\n",
    "\n",
    "    - 监督（Classification/CTR）训练：把 item embedding 作为特征，训练目标是点击/转化 —— embedding 会更有任务相关性。\n",
    "    \n",
    "    - 对比学习（Siamese/Triplet/InfoNCE）：把“相似交互对”拉近，不相似对拉远（适用于无标签或弱标签场景）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34114cac-a8aa-46db-b9ce-99d9156ecd8c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 用户画像（User Profile）如何生成\n",
    "\n",
    "- 简单加权平均：用户向量 = 已交互 `item` 向量的加权平均（权重按 recency、行为类型、评分、购买金额等）。\n",
    "\n",
    "- 向量池化：`max pooling` / `attention pooling`（learned attention 权重），可更好地聚焦用户的偏好。\n",
    "\n",
    "- 序列模型：用 `RNN`/ `Transformer` 对用户最近行为序列编码，输出用户向量（session-based）。\n",
    "\n",
    "---\n",
    "## 5. 召回流程（工程级）\n",
    "\n",
    "- 离线阶段：抽取 `item` 元数据 → 生成 `item` 向量（文本/图像/融合）→ 可选降维（`PCA`）→ 构建` ANN `索引（FAISS/HNSW/Milvus）→ 保存索引 + 向量表。\n",
    "\n",
    "- 在线阶段：实时计算/读取用户向量（或根据用户最近行为做加权平均）→ 在` ANN `索引中检索` Top-K`（K=100~200 常见）→ 过滤（已交互）→ 将候选送入排序模型。\n",
    "\n",
    "- 增量更新：新` item `生成向量后可增量插入索引或批量重建索引（根据索引类型支持）。\n",
    "\n",
    "---\n",
    "## 6. 相似度与 ANN 索引（细节）\n",
    "\n",
    "- 相似度度量：余弦相似度（常用），内积（dot）用于未归一化或训练时直接优化内积。\n",
    "\n",
    "- 若用内积但想等价于余弦：对向量做` L2 `归一化。\n",
    "\n",
    "- `ANN`（近似最近邻）建议：\n",
    "\n",
    "    - 中小规模（百万级）可用 `HNSW`（高召回、低延迟）。\n",
    "    \n",
    "    - 大规模（千万/亿）可用` IVF+PQ`（倒排+乘积量化）节省内存。\n",
    "    \n",
    "    - 推荐库：FAISS（GPU/CPU）、Annoy、HNSWlib、Milvus、Weaviate（向量 DB）。\n",
    "\n",
    "- 索引参数常见建议（经验值，需调优）：\n",
    "\n",
    "    - embedding_dim: 128/256/384/768（取决于模型）。\n",
    "    \n",
    "    - topK_recall: 100 ~ 200（一般召回层输出 100~200 个候选交给排序）。\n",
    "    \n",
    "    - HNSW：M=16~64，efSearch=100~500（越大召回越高、延迟越高）。\n",
    "    \n",
    "    - IVF+PQ：nlist=1k~10k, m(subquantizers)=8~16, nbits=8。\n",
    "\n",
    "---\n",
    "## 7. 向量压缩与存储\n",
    "\n",
    "- 量化（PQ）：降低内存，用在大规模库。\n",
    "\n",
    "- 降维（PCA / TruncatedSVD）：把 768D 压到 128D，保留大部分信息同时加速检索。\n",
    "\n",
    "- 数据类型：存 float32 常见，float16 可节省内存但注意数值精度。\n",
    "\n",
    "---\n",
    "## 8. 冷启动 & 稀疏性问题\n",
    "\n",
    "新` item`：只要有 metadata（title、category、image），就能直接生成向量并召回——内容向量是解决新物品冷启动的利器。\n",
    "\n",
    "- 新用户：用热门/策略推荐或用 onboarding 问卷获取初始偏好，也可以基于 session 的短期行为生成临时用户向量。\n",
    "\n",
    "- 长尾 `item`：可能少交互，内容相似性仍能帮助召回长尾，但效果依赖内容质量。\n",
    "\n",
    "---\n",
    "## 9. 评估（召回层相关指标）\n",
    "\n",
    "- `Recall@K`：召回阶段最关键（衡量召回能否覆盖用户将要点击/购买的物品）。\n",
    "\n",
    "- `Coverage`、`Novelty`、`Diversity`：关注候选集的广度与新颖性。\n",
    "\n",
    "- `Latency` / `QPS`：线上召回必须满足延迟要求（通常 10s-100s ms）。\n",
    "\n",
    "---\n",
    "## 10. 实践最佳实践与工程要点（Checklist）\n",
    "\n",
    "- 对向量做 L2 归一化（如果以余弦为准）；统一训练与检索时的度量。\n",
    "\n",
    "- `Top-K `稳定化：召回 K 不宜太小，推荐层和排序层共同决定最终结果。\n",
    "\n",
    "- 索引热更新：选择支持增量插入或实现短周期批量重建。\n",
    "\n",
    "- 时间衰减：用户画像加时间权重以增强时效性。\n",
    "\n",
    "- 多模态融合：如果有图文，先在各自空间做 embedding(特征提取)，再用 FC(全连接层)/Attention(注意力机制) 融合并归一化。\n",
    "\n",
    "- 对比学习 / 熟化训练：如果有交互数据，优先用对比损失训练向量以提高检索质量（InfoNCE、Triplet）。\n",
    "\n",
    "- A/B 测试：线上验证不同向量/索引/检索策略对` CTR`、`转化`的真实影响。\n",
    "\n",
    "- 监控：召回覆盖率、查询延迟、索引错误率、召回质量变化（随时间）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4befb6d-5dfa-402c-9a7b-f1d7b39acd55",
   "metadata": {},
   "source": [
    "---\n",
    "## 内容向量的构建方法\n",
    "文本内容向量化  \n",
    "- TF-IDF向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cae0d6-3e5a-4d6c-84dc-0f9057eb0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TFIDFContentVector:\n",
    "    def __init__(self, max_features=5000):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        self.item_vectors = None\n",
    "    \n",
    "    def fit(self, items_data):\n",
    "        \"\"\"训练TF-IDF向量化器\"\"\"\n",
    "        # items_data: {item_id: \"物品描述文本\"}\n",
    "        texts = list(items_data.values())\n",
    "        self.item_vectors = self.vectorizer.fit_transform(texts)\n",
    "        self.item_ids = list(items_data.keys())\n",
    "        self.id_to_idx = {id_: idx for idx, id_ in enumerate(self.item_ids)}\n",
    "    \n",
    "    def get_similar_items(self, item_id, top_k=10):\n",
    "        \"\"\"获取相似物品\"\"\"\n",
    "        if item_id not in self.id_to_idx:\n",
    "            return []\n",
    "        \n",
    "        idx = self.id_to_idx[item_id]\n",
    "        item_vector = self.item_vectors[idx]\n",
    "        \n",
    "        # 计算余弦相似度\n",
    "        similarities = cosine_similarity(item_vector, self.item_vectors).flatten()\n",
    "        \n",
    "        # 获取最相似的物品\n",
    "        similar_indices = similarities.argsort()[::-1][1:top_k+1]  # 排除自身\n",
    "        \n",
    "        results = []\n",
    "        for sim_idx in similar_indices:\n",
    "            results.append({\n",
    "                'item_id': self.item_ids[sim_idx],\n",
    "                'similarity': similarities[sim_idx]\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ced5ec-dd2c-4659-bb28-8c6c2e48e119",
   "metadata": {},
   "source": [
    "Word2Vec/Doc2Vec向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7217a-6e0d-454d-9e59-d6c92714ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "\n",
    "class Doc2VecContentVector:\n",
    "    def __init__(self, vector_size=100, window=5, min_count=2):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.model = None\n",
    "        self.item_vectors = {}\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"文本预处理\"\"\"\n",
    "        # 简单的分词处理\n",
    "        return text.lower().split()\n",
    "    \n",
    "    def fit(self, items_data):\n",
    "        \"\"\"训练Doc2Vec模型\"\"\"\n",
    "        # 准备训练数据\n",
    "        tagged_docs = []\n",
    "        for item_id, text in items_data.items():\n",
    "            words = self.preprocess_text(text)\n",
    "            tagged_docs.append(TaggedDocument(words, [item_id]))\n",
    "        \n",
    "        # 训练模型\n",
    "        self.model = Doc2Vec(\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=4,\n",
    "            epochs=20\n",
    "        )\n",
    "        \n",
    "        self.model.build_vocab(tagged_docs)\n",
    "        self.model.train(tagged_docs, \n",
    "                        total_examples=self.model.corpus_count,\n",
    "                        epochs=self.model.epochs)\n",
    "        \n",
    "        # 存储物品向量\n",
    "        for item_id in items_data.keys():\n",
    "            self.item_vectors[item_id] = self.model.dv[item_id]\n",
    "    \n",
    "    def get_similar_items(self, item_id, top_k=10):\n",
    "        \"\"\"获取相似物品\"\"\"\n",
    "        if item_id not in self.item_vectors:\n",
    "            return []\n",
    "        \n",
    "        similar_items = self.model.dv.most_similar(item_id, topn=top_k)\n",
    "        \n",
    "        return [{'item_id': item, 'similarity': sim} for item, sim in similar_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad7269-9fec-4463-a9f1-0133ca0d24df",
   "metadata": {},
   "source": [
    "多模态内容向量  --- 结合文本和属性特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e5760-b7bc-4fc9-9f7f-8f8eb207caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class MultiModalContentVector:\n",
    "    def __init__(self, text_dim=100, numeric_dim=10, categorical_dim=20):\n",
    "        self.text_dim = text_dim\n",
    "        self.numeric_dim = numeric_dim\n",
    "        self.categorical_dim = categorical_dim\n",
    "        \n",
    "    def fit(self, items_df):\n",
    "        \"\"\"处理多模态特征\"\"\"\n",
    "        self.items_df = items_df\n",
    "        self.item_vectors = {}\n",
    "        \n",
    "        # 1. 处理文本特征\n",
    "        text_vectors = self._process_text_features(items_df['description'])\n",
    "        \n",
    "        # 2. 处理数值特征\n",
    "        numeric_vectors = self._process_numeric_features(items_df[['price', 'rating', 'sales']])\n",
    "        \n",
    "        # 3. 处理分类特征\n",
    "        categorical_vectors = self._process_categorical_features(items_df[['category', 'brand']])\n",
    "        \n",
    "        # 4. 特征融合\n",
    "        for idx, item_id in enumerate(items_df['item_id']):\n",
    "            combined_vector = np.concatenate([\n",
    "                text_vectors[idx],\n",
    "                numeric_vectors[idx],\n",
    "                categorical_vectors[idx]\n",
    "            ])\n",
    "            self.item_vectors[item_id] = combined_vector\n",
    "    \n",
    "    def _process_text_features(self, texts):\n",
    "        \"\"\"处理文本特征\"\"\"\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(max_features=self.text_dim)\n",
    "        text_vectors = vectorizer.fit_transform(texts).toarray()\n",
    "        \n",
    "        # 降维（如果需要）\n",
    "        if text_vectors.shape[1] > self.text_dim:\n",
    "            pca = PCA(n_components=self.text_dim)\n",
    "            text_vectors = pca.fit_transform(text_vectors)\n",
    "        \n",
    "        return text_vectors\n",
    "    \n",
    "    def _process_numeric_features(self, numeric_df):\n",
    "        \"\"\"处理数值特征\"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(numeric_df)\n",
    "        return scaled_features\n",
    "    \n",
    "    def _process_categorical_features(self, categorical_df):\n",
    "        \"\"\"处理分类特征\"\"\"\n",
    "        encoded_features = []\n",
    "        \n",
    "        for column in categorical_df.columns:\n",
    "            le = LabelEncoder()\n",
    "            encoded_col = le.fit_transform(categorical_df[column])\n",
    "            # One-hot编码\n",
    "            onehot = pd.get_dummies(encoded_col, prefix=column)\n",
    "            encoded_features.append(onehot.values)\n",
    "        \n",
    "        # 合并所有分类特征\n",
    "        categorical_vectors = np.concatenate(encoded_features, axis=1)\n",
    "        \n",
    "        # 降维\n",
    "        if categorical_vectors.shape[1] > self.categorical_dim:\n",
    "            pca = PCA(n_components=self.categorical_dim)\n",
    "            categorical_vectors = pca.fit_transform(categorical_vectors)\n",
    "        \n",
    "        return categorical_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928d593-d33f-4f24-b572-66b3da10478c",
   "metadata": {},
   "source": [
    "## 基于内容向量的召回系统\n",
    " 用户画像构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01de458-56e5-459f-adff-295e5603886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserProfileVector:\n",
    "    def __init__(self, content_vector_model):\n",
    "        self.content_vector_model = content_vector_model\n",
    "        self.user_profiles = {}\n",
    "    \n",
    "    def update_user_profile(self, user_id, interacted_items, weights=None):\n",
    "        \"\"\"更新用户画像\"\"\"\n",
    "        if weights is None:\n",
    "            weights = np.ones(len(interacted_items))\n",
    "        \n",
    "        user_vector = None\n",
    "        valid_items = 0\n",
    "        \n",
    "        for item_id, weight in zip(interacted_items, weights):\n",
    "            if item_id in self.content_vector_model.item_vectors:\n",
    "                item_vector = self.content_vector_model.item_vectors[item_id]\n",
    "                \n",
    "                if user_vector is None:\n",
    "                    user_vector = np.zeros_like(item_vector)\n",
    "                \n",
    "                user_vector += item_vector * weight\n",
    "                valid_items += 1\n",
    "        \n",
    "        if valid_items > 0:\n",
    "            user_vector /= valid_items  # 平均 pooling\n",
    "            self.user_profiles[user_id] = user_vector\n",
    "    \n",
    "    def get_user_recommendations(self, user_id, all_item_ids, top_k=20):\n",
    "        \"\"\"基于用户画像获取推荐\"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            return []\n",
    "        \n",
    "        user_vector = self.user_profiles[user_id]\n",
    "        similarities = []\n",
    "        \n",
    "        for item_id in all_item_ids:\n",
    "            if item_id in self.content_vector_model.item_vectors:\n",
    "                item_vector = self.content_vector_model.item_vectors[item_id]\n",
    "                similarity = cosine_similarity([user_vector], [item_vector])[0][0]\n",
    "                similarities.append((item_id, similarity))\n",
    "        \n",
    "        # 排序并返回Top-K\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e7840-cdb7-4eed-8183-4c65b94eeb23",
   "metadata": {},
   "source": [
    "实时向量召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6aad0c-b02d-4d81-9498-59471e17bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss  # Facebook开源的向量相似度搜索库\n",
    "\n",
    "class FaissVectorRecall:\n",
    "    def __init__(self, dimension):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # 内积相似度\n",
    "        self.item_id_to_index = {}\n",
    "        self.index_to_item_id = {}\n",
    "    \n",
    "    def build_index(self, item_vectors):\n",
    "        \"\"\"构建向量索引\"\"\"\n",
    "        vectors = []\n",
    "        \n",
    "        for idx, (item_id, vector) in enumerate(item_vectors.items()):\n",
    "            vectors.append(vector)\n",
    "            self.item_id_to_index[item_id] = idx\n",
    "            self.index_to_item_id[idx] = item_id\n",
    "        \n",
    "        # 归一化向量（用于余弦相似度）\n",
    "        vectors = np.array(vectors).astype('float32')\n",
    "        faiss.normalize_L2(vectors)\n",
    "        \n",
    "        self.index.add(vectors)\n",
    "        print(f\"索引构建完成，共{self.index.ntotal}个向量\")\n",
    "    \n",
    "    def search_similar_items(self, query_vector, top_k=10):\n",
    "        \"\"\"搜索相似物品\"\"\"\n",
    "        if isinstance(query_vector, list):\n",
    "            query_vector = np.array(query_vector).astype('float32')\n",
    "        \n",
    "        # 归一化查询向量\n",
    "        faiss.normalize_L2(query_vector.reshape(1, -1))\n",
    "        \n",
    "        # 搜索\n",
    "        similarities, indices = self.index.search(query_vector.reshape(1, -1), top_k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (similarity, index) in enumerate(zip(similarities[0], indices[0])):\n",
    "            if index != -1:  # 有效结果\n",
    "                item_id = self.index_to_item_id[index]\n",
    "                results.append({\n",
    "                    'item_id': item_id,\n",
    "                    'similarity': float(similarity),\n",
    "                    'rank': i + 1\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_by_item(self, item_id, top_k=10):\n",
    "        \"\"\"基于物品搜索相似物品\"\"\"\n",
    "        if item_id not in self.item_id_to_index:\n",
    "            return []\n",
    "        \n",
    "        index = self.item_id_to_index[item_id]\n",
    "        # 获取物品向量\n",
    "        item_vector = self.index.reconstruct(index)\n",
    "        \n",
    "        return self.search_similar_items(item_vector, top_k + 1)[1:]  # 排除自身"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d0b48-a973-4d24-b98c-1980f1c3b723",
   "metadata": {},
   "source": [
    "## 完整的内容向量召回系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36295e0c-8e91-4433-9e71-e204ea364c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedRecallSystem:\n",
    "    def __init__(self, vector_dim=300):\n",
    "        self.vector_dim = vector_dim\n",
    "        self.vector_model = None\n",
    "        self.faiss_index = None\n",
    "        self.user_profiles = {}\n",
    "    \n",
    "    def initialize_system(self, items_data):\n",
    "        \"\"\"初始化系统\"\"\"\n",
    "        print(\"开始构建内容向量模型...\")\n",
    "        \n",
    "        # 1. 构建内容向量\n",
    "        self.vector_model = MultiModalContentVector()\n",
    "        self.vector_model.fit(items_data)\n",
    "        \n",
    "        # 2. 构建向量索引\n",
    "        self.faiss_index = FaissVectorRecall(self.vector_dim)\n",
    "        self.faiss_index.build_index(self.vector_model.item_vectors)\n",
    "        \n",
    "        print(\"内容向量召回系统初始化完成\")\n",
    "    \n",
    "    def update_user_behavior(self, user_id, interacted_items, behavior_type='click'):\n",
    "        \"\"\"更新用户行为\"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            self.user_profiles[user_id] = {\n",
    "                'clicked_items': [],\n",
    "                'purchased_items': [],\n",
    "                'last_update': None\n",
    "            }\n",
    "        \n",
    "        # 根据行为类型分配权重\n",
    "        weights = {\n",
    "            'click': 1.0,\n",
    "            'purchase': 3.0,\n",
    "            'favorite': 2.0\n",
    "        }\n",
    "        \n",
    "        weight = weights.get(behavior_type, 1.0)\n",
    "        \n",
    "        # 更新用户画像\n",
    "        user_profile = UserProfileVector(self.vector_model)\n",
    "        \n",
    "        if behavior_type == 'click':\n",
    "            self.user_profiles[user_id]['clicked_items'].extend(\n",
    "                [(item, weight) for item in interacted_items]\n",
    "            )\n",
    "        elif behavior_type == 'purchase':\n",
    "            self.user_profiles[user_id]['purchased_items'].extend(\n",
    "                [(item, weight) for item in interacted_items]\n",
    "            )\n",
    "        \n",
    "        # 重新计算用户向量\n",
    "        all_interactions = (self.user_profiles[user_id]['clicked_items'] + \n",
    "                          self.user_profiles[user_id]['purchased_items'])\n",
    "        \n",
    "        if all_interactions:\n",
    "            items, weights = zip(*all_interactions)\n",
    "            user_profile.update_user_profile(user_id, items, weights)\n",
    "    \n",
    "    def get_recommendations(self, user_id, recall_num=50):\n",
    "        \"\"\"获取推荐结果\"\"\"\n",
    "        if user_id not in self.user_profiles or not self.user_profiles[user_id]['clicked_items']:\n",
    "            # 冷启动用户：返回热门物品或随机物品\n",
    "            return self._get_cold_start_recommendations(recall_num)\n",
    "        \n",
    "        # 基于用户画像的召回\n",
    "        user_profile = UserProfileVector(self.vector_model)\n",
    "        items, weights = zip(*self.user_profiles[user_id]['clicked_items'])\n",
    "        user_profile.update_user_profile(user_id, items, weights)\n",
    "        \n",
    "        user_vector = user_profile.user_profiles[user_id]\n",
    "        recommendations = self.faiss_index.search_similar_items(user_vector, recall_num)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _get_cold_start_recommendations(self, recall_num):\n",
    "        \"\"\"冷启动推荐策略\"\"\"\n",
    "        # 可以基于物品流行度、新鲜度等进行召回\n",
    "        all_item_ids = list(self.vector_model.item_vectors.keys())\n",
    "        np.random.shuffle(all_item_ids)\n",
    "        return [{'item_id': item_id, 'similarity': 0.0} for item_id in all_item_ids[:recall_num]]\n",
    "    \n",
    "    def get_similar_items(self, item_id, top_k=10):\n",
    "        \"\"\"获取相似物品（用于相关推荐）\"\"\"\n",
    "        return self.faiss_index.search_by_item(item_id, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f376b3-6698-45bc-bc0d-a6cb05020c4c",
   "metadata": {},
   "source": [
    "## 实际应用示例\n",
    "电影推荐系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb9200-f18e-4e9f-b429-b1dec9d4d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例数据准备\n",
    "movies_data = {\n",
    "    'item_id': [1, 2, 3, 4, 5],\n",
    "    'title': ['The Godfather', 'The Dark Knight', 'Pulp Fiction', 'Forrest Gump', 'Inception'],\n",
    "    'description': [\n",
    "        'Crime family drama about the Corleone family',\n",
    "        'Batman battles the Joker in Gotham City',\n",
    "        'Interconnected stories of criminals in Los Angeles',\n",
    "        'Life story of a man with low IQ but good heart',\n",
    "        'Dream thieves perform corporate espionage through dreams'\n",
    "    ],\n",
    "    'genre': ['Crime', 'Action', 'Crime', 'Drama', 'Sci-Fi'],\n",
    "    'rating': [9.2, 9.0, 8.9, 8.8, 8.7]\n",
    "}\n",
    "\n",
    "# 创建推荐系统\n",
    "recall_system = ContentBasedRecallSystem()\n",
    "recall_system.initialize_system(pd.DataFrame(movies_data))\n",
    "\n",
    "# 模拟用户行为\n",
    "recall_system.update_user_behavior('user1', [1, 2], 'click')  # 用户点击了教父和黑暗骑士\n",
    "\n",
    "# 获取推荐\n",
    "recommendations = recall_system.get_recommendations('user1', 10)\n",
    "print(\"为用户推荐的电影:\")\n",
    "for rec in recommendations:\n",
    "    print(f\"电影ID: {rec['item_id']}, 相似度: {rec['similarity']:.3f}\")\n",
    "\n",
    "# 获取相似电影\n",
    "similar_movies = recall_system.get_similar_items(1, 5)  # 与教父相似的电影\n",
    "print(\"\\n与《教父》相似的电影:\")\n",
    "for movie in similar_movies:\n",
    "    print(f\"电影ID: {movie['item_id']}, 相似度: {movie['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f166af58-0719-4acf-895c-65d500f9dcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
